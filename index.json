[{"categories":null,"contents":"","date":"2025-08-21T00:00:00Z","permalink":"https://unionpan.github.io/photos/","section":"","tags":null,"title":"Photos"},{"categories":["I am back"],"contents":"This past year I suffered a lot mentally and financially, my funding was paused and my girlfriend Sam (check out her spotify here!) lost her boojee A.P.C. job, so I haven\u0026rsquo;t posted for a while. It\u0026rsquo;s really a leisure and effort to write stuff consistently (because wtf am I supposed to write?!!?). But the coding agents (Claude, Gemini etc..) really revolutionarized my efficiency, and shamelessly I have to admit that I use them a lot to \u0026ldquo;vibe code\u0026rdquo; and even do research, with sufficient human surveillance. So hopefully, I will resume writing for my own sanity.\nMaybe I am being hypocritical here\u0026hellip;a lot of people love giving an existing/trivial phenomenon/methodology a new name, and hence proposing a new \u0026lsquo;\u0026lsquo;framework\u0026rsquo;\u0026rsquo;. The underlying mathematical or engineering contribution is often minimal, or sometimes even worse, wrong. Throughout the years I find this widely exists in academia, or even industry. Don\u0026rsquo;t get me wrong—I totally understand why this happens. Coming up with genuinely novel ideas is hard, especially when you\u0026rsquo;re under constant pressure to publish, get grants, and prove your worth to reviewers who might be having a bad day. Sometimes you gotta work with what you\u0026rsquo;ve got, rebrand it nicely, and hope people buy into your \u0026ldquo;innovative framework.\u0026rdquo;\nI guess the real problem is there\u0026rsquo;s no objective bar for what counts as \u0026ldquo;innovative enough.\u0026rdquo; One reviewer\u0026rsquo;s \u0026ldquo;trivial extension\u0026rdquo; is another\u0026rsquo;s \u0026ldquo;groundbreaking contribution.\u0026rdquo; This ambiguity, combined with the sheer volume of papers flooding the system, has caused troubles. Overworked reviewers sometimes delegate to grad students or (God help us) undergrads who may not catch when the emperor has no clothes.\n\u0026ldquo;Holonic Risk\u0026rdquo; So my advisor has been writing a paper called \u0026ldquo;Holonic Risk\u0026rdquo;, using ChatGPT (the reason I know this is because I tracked his Overleaf editing history and code style, a lot of \u0026lsquo;\u0026rsquo;_\\( _\\)\u0026rsquo;\u0026rsquo; for equations,) the core idea is that he imagined that there exists a networked chunks of players that interact with each other through the specific network structured utility functions. A \u0026ldquo;holons\u0026rdquo; is a \u0026ldquo;chunk of players\u0026rdquo;, and the Nash equilibrium, as you may upgrade it into Bayesian, is a \u0026ldquo;Holonic equilibrium\u0026rdquo;.\nHere\u0026rsquo;s a global game1 example \u0026ldquo;holonic Bayesian game\u0026rdquo;:\nPlayers: Set of voters $N = \\bigcup_{i \\in I} N_i$ where $I$ indexes committees (holons) and $N_i$ are voters in committee $i$.\nTypes: Each voter $k \\in N_i$ has private type $\\xi_k^i \\in \\{0,1\\}$ drawn from Bernoulli$(p)$, representing their preference strength.\nActions: Binary voting decision $x_k^i \\in \\{0,1\\}$.\nInformation Structure: Each voter observes only their own type $\\xi_k^i$ but knows the type distribution $p$ is common across all voters.\nPayoff Structure: For voter $k$ in committee $i$, the utility function is: $$U_k^i(x_k^i, x_{-k}^i, \\omega_{-i}; \\xi_k^i) = \\mathbf{1}_{{ \\omega^i = 0}}(c_0 + c_1 x_k^i) + \\mathbf{1}_{ { \\omega^i = 1 } } c_2 \\cdot \\mathbf{1}_{ { x_k^i \\neq \\xi_k^i }}$$\nwhere $\\omega^i \\in {0,1}$ is the committee outcome and $\\omega_{-i}$ represents other committees\u0026rsquo; outcomes.\nThe \u0026ldquo;holonic\u0026rdquo; aspect reduces to this coupling mechanism:\n$$ \\omega^i = \\mathbf{1}_{\\left\\{ \\sum_{k \\in N_i} x_k^i \\geq \\theta + \\gamma \\sum_{j \\neq i} \\right\\}}$$\nSo committee $i$ succeeds if its vote total exceeds a threshold that increases linearly with the number of failed external committees. The parameter $\\gamma \\geq 0$ controls coupling strength.\nBayesian Equilibrium Analysis In a Bayesian Nash equilibrium, each voter chooses a strategy $\\sigma_k^i: {0,1} \\rightarrow {0,1}$ that maximizes expected utility given their beliefs about others\u0026rsquo; strategies and external outcomes.\nTruth-telling strategy: $\\sigma_k^i(\\xi_k^i) = \\xi_k^i$ (vote your type).\nEquilibrium condition: For truth-telling to be a Bayesian Nash equilibrium, we need: $$\\mathbb{E}[U_k^i(\\xi_k^i, \\sigma_{-k}^i(\\xi_{-k}^i), \\omega_{-i}; \\xi_k^i)] \\geq \\mathbb{E}[U_k^i(1-\\xi_k^i, \\sigma_{-k}^i(\\xi_{-k}^i), \\omega_{-i}; \\xi_k^i)]$$\nfor all $k, i$ and both values of $\\xi_k^i$.\nThis reduces to the incentive compatibility constraint: $$\\mathbb{P}(\\omega^i = 1) \\geq \\frac{c_1}{c_1 + c_2}$$\nThe Fixed-Point Problem Under symmetric truth-telling strategies, all committees have the same success probability $q$. The number of external failures follows $\\text{Binomial}(|I|-1, 1-q)$, leading to the fixed-point equation:\n$$q = \\sum_{z=0}^{|I|-1} \\binom{|I|-1}{z} (1-q)^z q^{|I|-1-z} \\cdot \\mathbb{P}\\left(\\text{Binomial}(n,p) \\geq \\theta + \\gamma z\\right)$$\nFor large $n$, using the normal approximation: $$q = \\sum_{z=0}^{|I|-1} \\binom{|I|-1}{z} (1-q)^z q^{|I|-1-z} \\cdot \\Phi\\left(\\frac{np - \\theta - \\gamma z}{\\sqrt{np(1-p)}}\\right)$$\nIn the limit $|I| \\rightarrow \\infty$ with $\\delta = \\gamma |I|$ fixed, this becomes: $$q = \\Phi\\left(\\frac{np - \\theta - \\delta(1-q)}{\\sqrt{np(1-p)}}\\right)$$\nExistence and Uniqueness of Equilibria The key mathematical questions become:\nExistence: When does a solution $q^* \\in [0,1]$ exist? Uniqueness: When is the solution unique? Stability: When are equilibria robust to perturbations? Uniqueness condition: The mapping $T(q) = \\Phi\\left(\\frac{np - \\theta - \\delta(1-q)}{\\sqrt{np(1-p)}}\\right)$ is a contraction when: $$\\frac{\\delta \\phi\\left(\\frac{np - \\theta - \\delta(1-q)}{\\sqrt{np(1-p)}}\\right)}{\\sqrt{np(1-p)}} \u0026lt; 1$$\nwhere $\\phi$ is the standard normal density. This is guaranteed when $\\delta$ is sufficiently small relative to $\\sqrt{np(1-p)}$.\nMultiple equilibria: For large $\\delta$, the system can exhibit bistability with equilibria near $q = 0$ and $q = 1$, separated by an unstable interior equilibrium.\nWhat\u0026rsquo;s Actually Novel (Spoiler: Not Much) From a game theory perspective, this is:\nA global game with strategic complementarities Threshold models with network effects Coordination games with incomplete information The specific linear coupling $\\theta + \\gamma z$ is new, but the mathematical framework is entirely standard. The \u0026ldquo;holonic equilibrium\u0026rdquo; concepts are just rebranded Bayesian Nash equilibria with particular structure.\nThe Computational Reality Check Here\u0026rsquo;s where theory meets practice. The beautiful fixed-point equation only works when:\n$n$ is large: Need CLT for normal approximation $\\gamma$ is small: Need contraction for uniqueness Truth-telling holds: Need incentive compatibility When any of these fail, you\u0026rsquo;re stuck with a discrete Boolean satisfiability problem: $$\\omega^i = \\mathbf{1}{{X^i \\geq \\theta + \\gamma \\sum{j \\neq i}(1-\\omega^j)}}$$\nfor vote totals $X^i$. This system can have:\nNo solutions (contradictory constraints) Multiple solutions (coordination failures) Exponential complexity (need to check $2^{|I|}$ configurations) So the \u0026ldquo;elegant\u0026rdquo; theory only applies in limiting regimes that wash out the interesting discrete dynamics.\nLook, I get it. Academic survival sometimes requires creative packaging. And there\u0026rsquo;s value in applying standard tools to new domains, even if the mathematical innovation is limited.\nBut let\u0026rsquo;s be honest about what we\u0026rsquo;re doing. This is Bayesian game theory with a specific payoff structure, not a revolutionary new framework. The insights come from computational implementation and parameter exploration, not from theoretical breakthroughs.\nMaybe that\u0026rsquo;s enough. Maybe we don\u0026rsquo;t need every paper to prove new theorems. Sometimes the contribution is showing how existing theory applies to practical problems, or developing computational tools that make complex models tractable.\nBut we should call it what it is.\nMorris, Stephen, and Hyun Song Shin. \u0026ldquo;Global games: Theory and applications.\u0026rdquo; (2001).\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"2025-08-18T00:00:00Z","permalink":"https://unionpan.github.io/post/2025/mechanism_design/","section":"post","tags":null,"title":"Bayesian games, mechanism design, and so on"},{"categories":["Some random math"],"contents":"It sometimes drives me insane to hear about engineering or economics people using the word \u0026ldquo;sufficient statistics\u0026rdquo; in the seminars. The idea of it, as many people will understand it correctly, is that this function summarizes the information from the data, impeccable, alright. But the terminology itself comes with a very rigorous definition. I would very much prefer if you do not plan to give the rigorous description about why it is sufficient in the statistical sense, use some other words like sufficient information or sufficient signaling if necessary.\nAnyhow, in case we are really gonna use it, let\u0026rsquo;s talk about what a sufficient statistic is. Given a probability space $(\\Omega, \\mathbb{P}, \\mathcal{F})$, where the parameter space $\\Omega$ that labels all the possible statistical models ${P_\\theta}$, we call a function, often denoted as $T$, that maps from data $X \\sim P_\\theta(\\cdot) \\in \\Delta (\\mathcal{X})$ to some \u0026lsquo;\u0026lsquo;coonclusion\u0026rsquo;\u0026rsquo; a statistic. E.g., from the Boston housing prices data we get the average price, and that is a statistic, we can also calculate the emprical variance, now that\u0026rsquo;s also a statistic. To be more general, we restrict the data to be in a measurable space $(\\mathcal{X}, \\mathcal{B})$ and the statistical outcome to be in a measurable space $(\\mathcal{T}, \\mathcal{C})$.\nDefinition 1 Let $(\\mathcal{T} , \\mathcal{C})$ be a measurable space such that the $\\sigma$-field $\\mathcal{C}$ contains all singletons. A measurable mapping $T : \\mathcal{X} \\to \\mathcal{T}$ is called a statistic.\nUsually we can think of $\\mathcal{X}/\\mathcal{T}$ as a subspace of $\\mathbb{R}^d$ and $\\mathcal{B}/\\mathcal{C}$ its borel-algebra. Consider the distribution $P_\\Theta$ densities $f_\\Theta$ w.r.t. a measure $\\nu$, so does the distribution of $T = T(X)$, the idea is that $T$ should say all the things about the $\\Theta$-data-generating process: with $t = T(x)$, the conditional probability $$ f_{X|T, \\Theta} (x|t, \\theta) = \\frac{f_{X, T | \\Theta} (x, t | \\theta)}{f_{T| \\Theta} ( t|\\theta)} = \\frac{f_{X | \\Theta} (x | \\theta)}{f_{T| \\Theta} ( t|\\theta)} $$ remains the same for all the $\\Theta = \\theta \\in \\Omega$. In plain words, no matter what the statistical model $P_\\Theta$ is, knowing the likelihood of the data generated ($f_{X | \\Theta} (x | \\theta)$) is equivalent to knowing the likelihood of the statistics calculated $f_{T| \\Theta} ( t|\\theta)$, in which case we don\u0026rsquo;t even care what the data looks like, since the statistics $t$ are sufficient. Simple, right? To say some quantities are sufficient statistics, one does not need to give a rigorous definition like the following, but at least discuss the ratio of the conditional probability above. Because in some cases, it\u0026rsquo;s simply not true.\nDefinition 2. Suppose there exist versions of conditional distributions $\\mu_{X|\\Theta,T} (· | \\theta, t)$ and a function $r : \\mathcal{B} × \\mathcal{T} \\to [0, 1]$ such that\n$r(\\cdot, t)$ is a probability on $\\mathcal{B}$ for each $t \\in \\mathcal{T} $, $r(B, \\cdot)$ is measurable for each $B \\in \\mathcal{B}$, and for each $\\theta \\in \\Omega$ and $B \\in \\mathcal{B}$. $\\mu_{X|\\Theta,T} (B | θ, t) = r(B, t)$, for $\\mu_{T | \\Theta}(\\cdot | \\theta) − a.e. \\ \\ t.$ Then $T$ is called a sufficient statistic for $\\Theta$ (in the classical sense).\nNotice that we haven\u0026rsquo;t really discussed whether our setting is Frequentist or Bayesian yet, in the sense that we don\u0026rsquo;t really know if there is a prior measure on $\\Omega$. But this definition is considered Frequentist version by default.\nNow let\u0026rsquo;s look at the Bayesian setting, where we have a prior $\\mu_\\Theta(\\cdot) \\in \\Delta(\\Omega)$.\nDefinition 3. A statistic $T$ is called a sufficient statistic for the parameter $\\Theta$ (in the Bayesian sense) if, for every prior $\\mu_\\Theta$, there exists versions of posterior distributions $\\mu_{\\Theta|X}$ and $\\mu_{\\Theta|T}$ such that, for every $A \\in \\mathcal{F}$, we have $$ \\mu_{\\Theta|X} (A | x) = \\mu_{\\Theta|T} (A|T(x)) \\quad\\quad \\mu_X-a.s. $$ where $\\mu_X$ is the marginal distribution of $X$.\nWhen there are densities, the equality looks like this $$ \\begin{aligned} \\mu_{\\Theta \\mid X}(A \\mid x) \u0026amp; =\\int_{A} f_{\\Theta \\mid X}(\\theta \\mid x) \\mu_{\\Theta}(d \\theta) , \\\\ \\mu_{\\Theta \\mid T}(A \\mid t) \u0026amp; =\\int_{A} f_{\\Theta \\mid T}(\\theta \\mid t) \\mu_{\\Theta}(d \\theta) . \\end{aligned} $$ Therefore, for any element $x$ in the support of $\\mu_X$, it must holds for $ f_{\\Theta \\mid X}(\\theta \\mid x) = f_{\\Theta \\mid T}(\\theta \\mid t) $, which collapses to the condition in the Frequentist setting.\nNow, suppose someone gives you a parameterized family of statistical models ${P_\\theta : \\theta \\in \\Omega}$, which are all densities w.r.t some measure $\\nu$, how do we find a sufficient statistic? Or, how do you check if a statistic is sufficient? The following theorem will help.\nFactorization Theorem $T$ is a sufficient statistic if.f. there exist $h$ and $g$, such that $$ f_{X|\\Theta} (x| \\theta ) = h(x) g(\\theta , T(x)) $$\nProof Sufficiency: $$ \\begin{aligned} \\frac{d \\mu_{\\Theta \\mid X}}{d \\mu_{\\Theta}}(\\theta \\mid x) \u0026amp; =\\frac{f_{X \\mid \\Theta}(x \\mid \\theta)}{\\int_{\\Omega} f_{X \\mid \\Theta}(x \\mid \\theta) \\mu_{\\Theta}(d \\theta)} \\\\ \u0026amp; =\\frac{h(x) g(\\theta, T(x))}{\\int_{\\Omega} h(x) g(\\theta, T(x)) \\mu_{\\Theta}(d \\theta)} \\\\ \u0026amp; =\\frac{g(\\theta, T(x))}{\\int_{\\Omega} g(\\theta, T(x)) \\mu_{\\Theta}(d \\theta)}\\end{aligned} $$ hence it is a function of $T$;\nNecessity: $$ f_{X | \\Theta}(x | \\theta)= f_{\\Theta \\mid X}(\\theta \\mid x) f_{X}(x)= \\underbrace{f_{X}(x)}_{h(x)} \\underbrace{f_{\\Theta \\mid T}(\\theta | T(x))}_{g(\\theta, T(x))} . $$ $\\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad\\quad \\quad \\quad \\quad \\quad \\quad \\quad\\quad \\quad \\quad \\quad \\quad \\quad \\quad\\quad \\quad \\quad \\quad \\quad \\quad \\square $\nA sufficient statistic always exists because we can compute the exponential family, i.e., if we put $T(x) = (t_i(x))_{i=1}^d$, we can always put $$ f_{X \\mid \\Theta}(x \\mid \\theta) = \\underbrace{h(x)}_{h(x)} \\underbrace{c(\\theta) \\exp \\bigg( { \\sum_{i=1}^{d} \\theta_{i} t_{i}(x) \\bigg) }}_{g(\\theta, T(x))}. $$\nTo summarize, a statistic is a function, when we say it is sufficient we have to tell people what (statistical) model, and what parameters it is sufficient to, even though sometimes it is intuitive and does not need all the fuss.\n","date":"2024-04-28T00:00:00Z","permalink":"https://unionpan.github.io/post/2024/what_are_sufficient_statistics/","section":"post","tags":null,"title":"What (on earth) is a Sufficient Statistic?"},{"categories":["Some random math"],"contents":"Chaotic behavior can emerge even in simple dynamics such as replicator, and Rock-Paper-Scissor oscillators, depending on the game settings. To begin with, we first define what is chaos qualitatively: \u0026ldquo;Chaos can be described as long term, aperiodic behaviour that exhibits sensitive dependence on initial conditions. Sensitive dependence on initial conditions implies that nearby trajectories diverge exponentially fast over time.\u0026rdquo; Or, by Edward Lorenz, when the present determines the future but the approximate present does not approximately determine the future.\nEdward Norton Lorenz (1918-2008) was an American mathematician and meteorologist who really laid the groundwork for how we understand weather and climate predictability today. He studied math at both Dartmouth and Harvard, and then took a break from his academic pursuits to serve as a weather forecaster for the Air Force during World War II. After the war wrapped up, he went to MIT, where he completed his doctoral degree in meteorology.\nIn the mid-1900s, the field of meteorology was still very much in its infancy. Lorenz programmed an existing computer, the Royal McBee, to aide in his research of atmosphere equations and forecasting. His program allowed him control the initial conditions of a weather system based on 12 differential equations. And the idea naturally came to him in 1961, when he was running a program with data rounded off from previous experiment, he found \u0026ldquo;chaos\u0026rdquo;: the fact that two weather conditions, which differ by less than $0.1%$, can produce significantly different results. So he wrote the paper \u0026ldquo;ordinary differential equations whose solutions afford the simplest example of deterministic non periodic flow and finite amplitude convection\u0026rdquo;, where he found that when applying Fourier series to one of Rayleigh\u0026rsquo;s convection equations, all except three variables tended to zero, which were used to construct a simple model based on the 2-dimensional representation of the earth\u0026rsquo;s atmosphere, the Lorenz Equation: $$ \\begin{aligned}\\frac{d x}{d t}=\\dot{x}=\\sigma(y-x) \\\\ \\frac{d y}{d t}=\\dot{y}=\\rho x-y-x z \\\\ \\frac{d z}{d t}=\\dot{z}=x y-\\beta z . \\end{aligned} $$\nHere, $x$ represents the convective overturning on the plane, $y$ and $z$ represent horizontal and vertical temperature variation respectively.\nThe parameters in the Lorenz system—sigma ($\\sigma$), rho ($\\rho$), and beta ($\\beta$)—have specific physical interpretations in the context of atmospheric convection, but they also have broader implications for the system\u0026rsquo;s behavior in mathematical and physical systems modeling. Here\u0026rsquo;s what each parameter represents:\n$\\sigma$ is the Prandtl number, which is a dimensionless number expressing the ratio of momentum diffusivity (viscosity) to thermal diffusivity. In the context of the Lorenz attractor, it represents the rate of heat transfer (convection) versus the rate of temperature change. A higher sigma increases the rate at which the system diverges along the $x$-axis relative to the $y$-axis, influencing the system\u0026rsquo;s tendency toward chaotic behavior. $\\rho$ is the Rayleigh number divided by its critical value for the onset of convection. It is a measure of the buoyancy-driven flow (thermal instability within the fluid), which is a result of temperature differences. In the Lorenz equations, it directly influences the nonlinearity of the system and is critical for the emergence of chaos. When $\\rho$ is larger than a certain threshold (in the standard Lorenz system, $\\rho$ \u0026gt; 24.74), the system exhibits chaotic behavior. $\\beta$ is a dimensionless parameter related to the geometry of the problem, specifically the aspect ratio of the convective cells. It can be thought of as influencing the vertical temperature profile within the system. Beta affects the dissipation rate of the vertical velocity component, influencing how the system\u0026rsquo;s trajectories contract towards the $z$-axis. Specific parameter settings will lead to chaos. By setting $\\dot{x}, \\dot{y},$ and $\\dot{z}$ to $0$ we get three equilibrium points: $$ \\begin{aligned} \u0026amp; k_0 = (0, 0, 0) \\\\ \u0026amp; k_1 = (-\\sqrt{\\beta ( \\rho - 1 )}, \\sqrt{\\beta ( \\rho - 1 )}, \\rho - 1 ) \\\\ \u0026amp; k_2 = (\\sqrt{\\beta ( \\rho - 1 )}, - \\sqrt{\\beta ( \\rho - 1 )}, \\rho - 1 ) \\end{aligned} $$\nLinearizing the ODE around these equilibrium points to check the stability we get: $$ \\left[\\begin{array}{c}\\dot{x} \\ \\dot{y} \\ \\dot{z}\\end{array}\\right]=\\left[\\begin{array}{ccc}-\\sigma \u0026amp; \\sigma \u0026amp; 0 \\ \\rho-\\bar{z} \u0026amp; -1 \u0026amp; \\bar{x} \\ \\bar{y} \u0026amp; \\bar{x} \u0026amp; -\\beta\\end{array}\\right]\\left[\\begin{array}{l}x \\ y \\ z\\end{array}\\right] $$\nif we take $(\\bar{x}, \\bar{y}, \\bar{z})$ to be $k_0$ we get eigenvalue equation: $$ \\lambda^{3}+(\\beta+\\sigma+1) \\lambda^{2}+(\\beta+\\beta \\sigma+\\sigma-\\rho \\sigma) \\lambda+\\beta \\sigma(1-\\rho)=0 $$ and $-\\beta$ is one solution, so we get $$ (\\lambda+\\beta)\\left(\\lambda^{2}+(\\sigma+1) \\lambda+\\sigma(1-\\rho)\\right)=0 $$ and the eigenvalues: $$ \\lambda_{1}, \\lambda_{2}=\\frac{-\\sigma-1 \\pm \\sqrt{(\\sigma+1)^{2}+4 \\sigma(\\rho-1)}}{2}, \\lambda_{3}=-\\beta . $$\nIf we take $k_1$ or $k_2$, we end up with eigenvalues satisfying: $$ \\mu^{3}+(\\beta+\\sigma+1) \\mu^{2}+(\\sigma+\\rho) \\beta \\mu+(1-\\rho) 2 \\sigma \\beta=0 $$ where all the three eigenvalues are negative when $$ \\rho\u0026lt;\\frac{\\sigma(\\sigma+\\beta+3)}{\\sigma-\\beta-1}=\\rho_{c} $$\n$\\rho_c \\approx 24.74$ when $\\sigma = 10$ and $\\beta = 8/3$.\nTherefore, at $\\rho \u0026gt; \\rho_c$, there are no fixed points. The flow will enter an invariance region around the origin where we see chaotic behavior. $$ \\begin{array}{|l|l|} \\hline \\rho \u0026amp; \\text { Fixed Points } \\\\ \\hline [0, 1] \u0026amp; (0,0,0) \\\\ \\hline (1,24.74) \u0026amp; k_{1}, k_{2} \\\\ \\hline [24.74-30.1) \u0026amp; \\text { None, chaos occurs } \\\\ \\hline [30.1-\\infty) \u0026amp; \\text { intermittency (not proven) } \\\\ \\hline\\end{array} $$\nThe Butterfly Effect Picking initial conditions $(0,1,0)$ $(1,0,1)$ we will get two distinct paths, shown in the following picture. It sort of resembles a butterfly, hence the chaotic effects in general are referred to as the Butterfly Effect. This draws upon Lorenz\u0026rsquo;s findings that two seemingly identical weather systems could produce two very different weather systems in the near future. Thus, a butterfly flapping its wings could alter the atmosphere ever so slightly, so as to deviate from the initial conditions, and accordingly alter the course of weather forever. Lorenz first used the example of a seagull\u0026rsquo;s wings, though the analogy has morphed into using a butterfly.\nThe buutterfly effect can be helpful to explain a lot of phenomenons in engineering, geography, and particularly stock market, where the linear financial models have failed many times. Just like Lorenz\u0026rsquo;s conclusion about weather conditions, the long run economic forecast is not feasible beyond a short time frame.\n","date":"2024-04-08T00:00:00Z","permalink":"https://unionpan.github.io/post/2024/butterfly_effect/","section":"post","tags":null,"title":"The Lorenz dynamics and Butterfly Effect"},{"categories":["Some random math"],"contents":"This post is in celebration of Michel Talagrand winning Abel prize. Not to overly romanticize this but this is pretty much a come back story because back in the days of last century, “The type of mathematics I do was not fashionable at all when I started. It was considered inferior mathematics.” \u0026ndash;Michel Talagrand.\nNow we\u0026rsquo;ve seen significance of his contribution to the concentration of measure, suprema of stochastic processes and spin glass, all partially owing to his celebrated isoperimetry inequality in product probability space. What is an isoperimetry inequality? It is a concept in mathematics, particularly in the field of geometry and geometric analysis, that compares the length (or perimeter) of a closed curve to the area of the region it encloses, establishing that among all shapes with the same perimeter, the circle has the maximum area. For example: $$ 4 \\pi A \\leq L^2 $$ where the equality holds if.f. the curve is a circle. In a more general sense, the isoperimetric inequality relates the volume of an $n$-dimensional domain to the surface area of its boundary, with the sphere in $n$-dimensional space providing the optimal (maximum volume for a given surface area) ratio. Applying this concept to probability space, Talagrand was able to prove the following:\nTheorem For a product probability space $\\Omega = \\prod_{i=1}^n \\Omega_i$ endowed with a produdct measure and $A \\subseteq \\Omega$ $$ \\mathbb{P}[A] \\cdot \\mathbb{P}\\left[A_{t}^{c}\\right] \\leq e^{-t^{2} / 4} $$ for any $t \u0026gt; 0$, where $A^c_t = { w \\in \\Omega, d(A, \\omega) \\leq t }$ is an event defined by convex distance: $$ d(A, \\omega) = \\max_{\\alpha, |\\alpha|_2 \\leq 1} \\min_{y \\in A} \\sum_{ i: \\omega_i \\neq y_i} \\alpha_i. $$\nTo prove the inequality we need a little bit of preparation. Given set $A$, $x \\in \\Omega$: $\\mathcal{D}_{A}^{c}(x)=\\sup_{a \\in \\mathcal{R}_{+}^{n}} \\left( d_{a}(x, A)=\\inf_{y \\in A} d_{a}(x, y) \\right)$. Let $$ V_{A}(x) = \\text{ Convex-hull }\\left( U_{A}(x) \\right) = \\left\\{\\sum_{s \\in U_{A} (x) } \\alpha_{s} S: \\sum \\alpha_{s}=1, \\alpha_{s} \\geq 0 \\text{ for all } s \\in U_{A}(x) \\right\\}. $$ Thus, $$ x \\in A \\Leftrightarrow \\mathbf{1}(x \\neq x)=0 \\in U_{A}(x) \\Leftrightarrow 0 \\in V_{A}(x). $$\nLemma We have the following $$ \\mathcal{D}_{A}^{c}(x)=d \\left(0, V_{A}(x) \\right) \\equiv \\inf_{y \\in V_{A}(x)}|y| $$\nProof We proceed by proving (i) $\\mathcal{D}_{A}^{c}(x) \\leq \\inf_{y \\in V_{A}(x)}|y|$ and (ii) $\\mathcal{D}_{A}^{c}(x) \\geq \\inf_{y \\in V_{A}(x)}|y|$.\n(i): since $\\inf_{y \\in V_{A}(x)}|y|$ is achieved, let $Z$ be such that $|Z| = \\inf_{y \\in V_{A}(x)}|y| $. For any $a \\in \\mathbb{R}^n_+$ $|a| = 1$: $$ \\inf_{y \\in V_{A}(x)} a \\cdot y \\leq a \\cdot z \\leq|a||z|=|z|. $$ Since $ \\inf_{y \\in V_{A}(x)} a \\cdot y$ is linear programming, the minimum is achieved at an extreme point. That is, there exists $s \\in U_A(x)$ such that $$ \\inf_{y \\in V_{A}(x)} a \\cdot y=\\inf_{s \\in U_{A}(x)} a \\cdot s=\\inf_{y \\in A} d_{a}(x, y) \\text { for some } y \\in A . $$which is true for all $$ \\sup_{|a|=1, a \\in \\mathbb{R}_{+}^{n}} \\inf_{y \\in A} d_{a}(x, y) \\leq|z| \\equiv \\inf_{y \\in V_{A}(x)}|y|. $$\n(ii): Let $z$ be the one achieving minimum in $V_A(x)$. Then due to convexity of the objective (equivalently $|y|^2 = \\sum y^2_i = f(y)$) and of the domain, we have for any $ y \\in V_A (x), \\langle \\nabla f(z), y -z \\rangle \\geq 0$ for any $y \\in V_A(x)$. $\\nabla f(z) = \\nabla (z \\dot z) = 2 z$. Therefore the condition implies: $$ (y-z) z \\geq 0 \\Leftrightarrow y \\cdot z \\geq z \\cdot z=|z|^{2} \\Rightarrow y \\cdot \\frac{z}{|z|} \\geq|z|. $$\nThus, for $a = \\frac{z}{|z|} \\in \\mathbb{R}^n_+ $, $|a| = 1$, we have that $$ \\inf_{y \\in V_A(x)} a \\dot y \\geq |z| . $$ But for any given $a$, $\\inf_{y \\in V_A(x)} a \\cdot y = \\inf_{s \\in U_A (x)} a \\cdot s = d_a (x,A)$ as explained before. That is, $\\sup_{a: |a| = 1} d_a (x, A) \\geq |z| = \\inf_{ y \\in V_A(x)} |y|$.\n$\\quad\\quad\\quad \\quad\\quad\\quad \\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad \\quad\\quad\\quad \\quad\\quad\\quad \\quad\\quad\\quad \\quad\\quad\\quad \\quad\\quad\\quad \\square$\nProof of Isoperimetry inequality We prove by induction. Consider $n=1$, given $A$, $$ \\mathcal{D}_{A}^{c}(x)=\\sup_{a \\in \\mathbb{R}_{+}^{n},|a|=1} \\inf_{y \\in A} d_{a}(x, y)=\\inf_{y \\in A} \\mathbf{1}(x \\neq y)=\\left\\{ \\begin{array}{ll}0, \u0026amp; \\text{ for } x \\in A \\\\ 1, \u0026amp; \\text { for } x \\notin A \\end{array} \\right. $$\nThen $$ \\begin{aligned} \\int \\exp \\left (D^{2} / 4\\right ) d P \u0026amp; =\\int_{A} \\exp (0) d P+ \\int_{A^{c}} \\exp (1 / 4) d P \\\\ \u0026amp; =P(A)+e^{1 / 4}(1-P(A)) \\\\ \u0026amp; =e^{1 / 4}-\\left(e^{1 / 4}-1 \\right ) P(A) \\\\ \u0026amp; \\leq \\frac{1}{P(A)}. \\end{aligned} $$\nLet $f(x) = e^{1/4} - (e^{1/4} - 1) x$ and $g(x)= \\frac{1}{x}$. Because $f(x)$ is a decreasing function of $x$, $g(x)$ is a decreasing convex function, hence the result holds for $n=1$.\nNow let\u0026rsquo;s assume it holds for $n$, let $A \\subset \\Omega^{n+1}$, and $B$ be its projections on $\\Omega^n$. Let $A(\\omega)$ be section of $A$ along $\\omega$: if $x \\in \\Omega^n$, $\\omega \\in \\Omega$, then $z = (x, \\omega) \\in \\Omega^{n+1}$. The key observation is that:\nif $s \\in U_{A(\\omega)} (x)$, then $(s, 0) \\in U_{A}(z)$ if $t \\in U_{B}(x)$, then $(t, 1) \\in U_{A}(z)$. if $\\xi \\in V_{A(\\omega)}(x), \\zeta \\in V_{B}(x)$, and $\\theta \\in [0,1]$, then $((\\theta \\xi+(1-\\theta) \\zeta), 1-\\theta) \\in V_{A}(z)$. Recall $$ \\begin{aligned} \\mathcal{D}_{A}^{c}(z)^{2} \u0026amp; =\\inf_{y \\in V_{A}(z)}|y|^{2} \\leq(1-\\theta)^{2}+|\\theta \\xi+(1-\\theta) \\zeta|^{2} \\\\ \u0026amp; \\leq(1-\\theta)^{2}+\\theta|\\xi|^{2}+(1-\\theta)|\\zeta|^{2} \\\\ \u0026amp; \\leq(1-\\theta)^{2}+\\theta \\inf_{\\xi \\in V_{A(\\omega)}(x)}|\\xi|^{2}+(1-\\theta) \\inf_{\\zeta \\in V_{B}(x)}|\\zeta|^{2} \\\\ \u0026amp; =(1-\\theta)^{2}+\\theta \\mathcal{D}_{A(\\omega)}^{c}(x)^{2}+(1-\\theta) \\mathcal{D}_{B}^{c}(x)^{2} \\end{aligned} $$ By Holder\u0026rsquo;s inequality, and the induction hypothesis, $\\forall \\omega \\in \\Omega$, $$ \\begin{aligned} \u0026amp; \\int_{\\Omega^{n}} e^{\\mathcal{D}_{A}^{c}(x, \\omega)^{2} / 4} d P(x) \\\\ \u0026amp; \\leq \\int_{\\Omega^{n}} \\exp \\left(\\frac{(1-\\theta)^{2}+\\theta \\mathcal{D}_{A(\\omega)}^{c}(x)^{2}+(1-\\theta) \\mathcal{D}_{B}^{c}(x)}{4} \\right) d P(x) \\\\ \u0026amp; \\leq \\exp \\left ( \\frac{(1-\\theta)^{2}}{4} \\right ) \\int_{\\Omega^{n}} \\underbrace{\\exp \\left(\\frac{\\theta \\mathcal{D}_{A(\\omega)}^{c}(x)^{2}}{4}\\right)}_{X} \\underbrace{\\exp \\left(\\frac{(1-\\theta) \\mathcal{D}_{B}^{c}(x)^{2}}{4}\\right)}_{Y} d P(x) \\\\ \u0026amp; =\\exp \\left(\\frac{(1-\\theta)^{2}}{4}\\right) \\mathbb{E}[X \\cdot Y] \\\\ \u0026amp; \\leq \\exp \\left(\\frac{(1-\\theta)^{2}}{4}\\right) \\mathbb{E}\\left[X^{p}\\right]^{1 / p} \\mathbb{E}\\left[Y^{q}\\right]^{1 / q},\\left(\\text{ for } p=\\frac{1}{\\theta}, q=\\frac{1}{1-\\theta}: \\theta \\in[0,1]\\right) \\\\ \u0026amp; =\\exp \\left ( \\frac{(1-\\theta)^{2}}{4} \\right )\\left(\\int_{\\Omega^{n}} \\exp \\left(\\mathcal{D}_{A(\\omega)}^{c}(x)^{2} / 4\\right) d P(x)\\right)^{\\theta}\\left(\\int_{\\Omega^{n}} \\exp \\left(\\mathcal{D}_{B}^{c}(x)^{2} / 4\\right) d P(x)\\right)^{1-\\theta} \\\\ \u0026amp; \\leq \\exp \\left(\\frac{(1-\\theta)^{2}}{4}\\right)\\left(\\frac{1}{P(A(\\omega))}\\right)^{\\theta}\\left(\\frac{1}{P(B)}\\right)^{1-\\theta} \\text{ by induction hypothesis. } \\\\ \u0026amp; =\\exp \\left ( \\frac{(1-\\theta)^{2}}{4} \\right ) \\frac{1}{P(B)}\\left(\\frac{P(A(\\omega))}{P(B)}\\right)^{-\\theta} . \\end{aligned} $$\nNow we optimize $\\theta$, simply by construction: for any $ u \\in [0,1]$ $\\inf_{\\theta \\in[0,1]} \\exp \\left(\\frac{(1-\\theta)^{2}}{4}\\right) u^{-\\theta} \\leq 2-u$. Therefore, the R.H.S. $$ \\leq \\frac{1}{P(B)}\\left(2-\\frac{P(A(\\omega))}{P(B)}\\right) , $$ and thus $$ \\begin{aligned} \u0026amp; \\int_{\\Omega^{n+1}} \\exp \\left(\\frac{\\mathcal{D}_{A}^{c}(x, \\omega)^{2}}{4}\\right) d P(x) d \\mu(\\omega) \\\\ \u0026amp; \\leq \\frac{1}{\\mathbb{P}(B)} \\int_{\\Omega}\\left(2-\\frac{\\mathbb{P}(A(\\omega))}{\\mathbb{P}(B)}\\right) d \\mu(\\omega) \\\\ \u0026amp; \\leq \\frac{1}{\\mathbb{P}(B)}\\left(2-\\frac{(P \\bigotimes \\mu)(A)}{\\mathbb{P}(B)}\\right) \\\\ \u0026amp; \\leq \\frac{1}{(\\mathbb{P} \\bigotimes \\mu)(A)},(\\text{ since } u(2-u) \\leq 1 \\text{ for all } u \\in \\mathbb{R}) . \\end{aligned} $$\n$ \\quad \\quad \\quad\\quad \\quad \\quad\\quad \\quad \\quad\\quad \\quad \\quad\\quad \\quad \\quad\\quad \\quad \\quad\\quad \\quad \\quad\\quad \\quad \\quad \\quad \\quad \\quad\\quad \\quad \\quad\\quad \\quad \\quad\\quad \\quad \\quad\\quad \\square$\n","date":"2024-03-26T00:00:00Z","permalink":"https://unionpan.github.io/post/2024/talagrand/","section":"post","tags":null,"title":"Talagrand's Isoperimetry inequality"},{"categories":["Some random math"],"contents":"In this post I just want to share a simple and elegant idea from a Control System Letter paper written by Maxim Raginsky et al.1. This idea largely inspired my recent work on multi-agent learning in (monotone) games2, which I might devote another post to talk about if I happen to get some interesting results out of it.\nThe idea starts from here: we all know that the archetype of solving convex optimization problem is through gradient descent: $$ x_{k+1} = x_k - \\nabla f(x_k), $$ which, in continuous time, corresponds to an autonomous dynamical system: $$ \\dot{x} = - \\nabla f (x) , \\quad x(0) = x_0. $$ A natural question to ask is what are the hidden objectives being achieved along the gradient flow. This question was approached in a \u0026ldquo;inverse optimal control\u0026rdquo; fashion, i.e., given an autonomous dynamical system, identify the close-loop control and its corresponding optimal control problem. In this approach, the Fenchel-Young inequality played a crucial role to formulate the optimal control problem, as was extensively used in the variational principles introduced by Brezis and Ekeland3.\nFor any function \\(f\\) defined over primal space \\( \\mathcal{X}\\), denote its Fenchel conjugate as \\(f^* (y) = \\sup_{x \\in \\mathcal{X}} \\{ \\langle x,y \\rangle - f(x)\\} \\), we have the Fenchel coupling: $$ \\mathcal{FC}_{f} (x, y) = f(x ) + f^*(y) - \\langle x, y \\rangle \\geq 0 $$\nwith the equality holds if.f. $ y \\in \\partial f(x)$, or $x \\in \\partial f^* (y)$ if $f$ is convex.\nNow we can try constructing the optimal control problem, we have: $$ \\begin{align*} \\inf_{ u \\in \\mathcal{U} } J(x_0) \u0026amp; \\triangleq \\int_{t=0}^{\\infty} f(x(t)) + f^* (-u(t)) + \\langle u(t), \\bar{x} \\rangle \\mathrm{d}t \\\\ \\text{s.t. } \\quad \\quad \\quad \u0026amp; \\dot{x} = u , \\quad x(0) = x_0 \\end{align*} $$ where $\\bar{x} \\in \\arg\\min_{x\\in\\mathcal{X} }f(x) $ is an optimal solution. Intuitively, solving this optimal control problem should also lead to an optimal solution to the original problem $\\min_{x \\in \\mathcal{X}} f$. This can be actually verified by, let\u0026rsquo;s say, put $u^* (t)$ to be a close-loop control $-\\nabla f(x(t))$, we know it eventually leads to $\\bar{x}$, and by Fenchel-young inequality, $f(\\bar{x}(t)) + f^* (-u^* (t)) + \\langle u^* (t), \\bar{x} \\rangle = 0$, meaning that the stage cost of the optimal control problem also goes to $0$, we have enough reason to believe that $u^*(t)$ is actually the optimal control.\nThis result is nearly trivial as we just need to construct a Lyapunov function $V(x) = \\frac{1}{2} \\|x - \\bar{x}\\|^2$. We have, by Fenchel-Young inequality, \\begin{equation} \\dot{V} (x) + f(x) + f^* ( - u) + \\langle \\bar{x}, u \\rangle \\geq 0 , \\end{equation} which simply imply that $V(x)$ is actually a value function and the optimal control should be whatever makes the equality holds, i.e., the close-loop control $ - \\nabla f(x(t)) $. The benefit we can gain from this is the availablity of an entire analytical toolbag for differential equations, even stochastic differential equations if we consider the stochastic case.\nTzen, B., Raj, A., Raginsky, M. and Bach, F., 2023. Variational principles for mirror descent and mirror langevin dynamics. IEEE Control Systems Letters.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nPan, Y., Li, T. and Zhu, Q., 2024. On the Variational Interpretation of Mirror Play in Monotone Games. arXiv preprint arXiv:2403.15636.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nBrézis, H. and Ekeland, I., 1976. Un principe variationnel associéa certaines equations paraboliques. Le cas independant du temps. CR Acad. Sci. Paris Sér. AB, 282(17), pp.971-974.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"2024-02-11T00:00:00Z","permalink":"https://unionpan.github.io/post/2024/variational_perspective/","section":"post","tags":null,"title":"A Variational Perspective On Gradient Descent"},{"categories":["Some random math"],"contents":"The dueling bandit problem natrually fits the description of a variety of recommendation systems that require \u0026lsquo;\u0026rsquo;learning on the fly\u0026rsquo;\u0026rsquo;, yet have no explicit access to a \u0026lsquo;\u0026lsquo;reward\u0026rsquo;\u0026rsquo; model. Instead, the \u0026lsquo;\u0026lsquo;human\u0026rsquo;\u0026rsquo; feedback part takes the form of \u0026lsquo;\u0026lsquo;choices\u0026rsquo;\u0026rsquo;, \u0026lsquo;\u0026lsquo;votes\u0026rsquo;\u0026rsquo;, or some discrete forms. Hence, oftentimes a learning protocol proceeds at time steps \\(t = 1, \\ldots, T\\):\nThe algorithm chooses a pair of arms \\( a_i, a_j \\) from \\(K\\) available ones; The oracle/human feedback/nature reveals the winner arm \\( a_i \\), with probability \\( P(a_i \\succ a_j)\\), \\( P(a_i \\prec a_j) = 1 - P(a_i \\succ a_j) \\) So the feedback is either \\(a_i\\) or \\( a_j \\), the preferences forms a matrix \\(P \\in \\mathbb{R}^{K \\times K}\\) such that \\( P + P^{\\top} = I\\) which defines the hidden information of the dueling bandit problem. The cumulative regret in the stochastic dueling bandit setting is: $$ \\mathcal{R}_T = \\sum_{t=1}^T P( a^* \\succ a^t_i ) + P( a^* \\succ a^t_j ) $$ where \\(a^* \\) is usually the Condorcet winner, i.e., \\( P( a^* \\succ a_j) \u0026gt; \\frac{1}{2} \\ \\ \\forall j \\in [K], a_j \\neq a^*\\). Condorcet winner is a pretty straightforward idea, which might not exist in general cases. To see that, suppose there are three candidates: A, B, and C, and three voters with the following preferences:\nVoter 1: A \u0026gt; B \u0026gt; C Voter 2: B \u0026gt; C \u0026gt; A Voter 3: C \u0026gt; A \u0026gt; B Voter 1 Voter 2 Voter 3 A \u0026gt; B X X B \u0026gt; C X X C \u0026gt; A X X In this example, let\u0026rsquo;s check the pairwise comparisons:\nA vs. B: B is preferred by Voter 2, but A is preferred by Voter 1 and 3. B vs. C: B is preferred by Voter 1 and 2, but C is preferred by Voter 3. C vs. A: C is preferred by Voter 2 and 3, but A is preferred by Voter 1. Since no candidate consistently beats all others in pairwise comparisons, there is no Condorcet winner in this example. Now we transform this example to fit in the dueling bandits context, we can have, for instance, a cyclic relation \\(A \\succ B \\succ C \\succ A \\).\nA B C A 0.5 0.7 0.2 B 0.3 0.5 0.6 C 0.8 0.4 0.5 Apparently the existence of a Condorcet winner requires a row that is greater than \\(0\\). While in a lot of cases such a winner/solution concept might not be suitable, let\u0026rsquo;s first dive into the algorithmic design of trying to find it when it exists. There are basically two styles of algorithms, asymmetric or symmetric:\nThe asymmetric style conceptually separates two choices into choosing a reference arm and an exploration arm. The reference arm acts as a summary of historical pulls. Typical algorithms of this type includes IF, BtM, SAVAGE, Doubler, RUCB, MergeRUCB, RCS, and DTS. The exploration strategy is to maximize the efficiency of identifying the best arm.\nInterleaved Filter (IF) and Beat the Mean (BtM) The very first two methods proposed are Interleaved Filter (IF) 1 and Beat the Mean (BtM) 2. They all assume there is a total ordering of the arms, i.e., we can relabel the arms as \\(a_1, \\ldots, a_K\\), such that \\( p_{i,j} \u0026gt; 0.5 \\) for all \\( i \u0026lt; j\\). Under this assumption the Condorcet winner is \\(a_1\\).\nIF method basically, is a type of \u0026ldquo;hill climbing\u0026rdquo; method that iteratively updates the reference arm \\(\\hat{a}\\) by comparing it with other arms until it becomes the most possible Condorcet winner. The term \u0026ldquo;interleaved\u0026rdquo; is kind of originating from a kind of Netflix ranking acceleration technique, which uses a blend of ranker to recommend videos to the same group of users, then compare the share of viewing hours coming from different rankers. IF picks a reference arm \\(\\hat{a}\\) randomly and preserves a set of \u0026ldquo;Condorcet candidates\u0026rdquo; to compare with the reference arm, as well as a set of confidence intervals \\( \\hat{C}_t := [ P_{\\hat{a}, a} - c_t, P_{\\hat{a}, a} + c_t] \\), where \\(c_{t}=\\sqrt{\\log (1 / \\delta) / t}\\), then it iteratively compares all of them and gradually filters out dominated arms which are out of the confidence intervals, i.e., \\( P_{\\hat{a}, a} - c_t \u0026gt; \\frac{1}{2} \\) and also updates reference arms, i.e., \\(P_{\\hat{a}, a} + c_t \u0026lt; \\frac{1}{2}\\). The corner stone idea is to prove after logarithmic comparisons, the winner between any two pairs is identified as the winner \u0026ldquo;correctly\u0026rdquo; with probability \\(1 - \\delta \\). To see this, let\u0026rsquo;s say \\(n\\) is the number of comparisons, for \\(t \\in \\mathbb{N}_+\\), the event \\( \\mathcal{E}_t\\) is when \\( \\hat{P} - c_t \u0026lt; \\frac{1}{2} \\), which is the condition for the match to continue after \\(t\\) comparisons, therefore $$ Pr( n \\geq t) \\leq P( \\mathcal{E}_t ), $$\nthe confidence interval boundaries \\( p_{i,j} \\notin \\hat{C}_t \\) $$ \\begin{align*} Pr(\\mathcal{E}_t) \u0026amp; = Pr( \\hat{P}_t - p_{i,j} \\leq c_t - \\Delta_{i,j} ) \\\\ \u0026amp; = Pr( \\mathbb{E}[\\hat{P}_t] - \\hat{P}_t \u0026gt; \\Delta_{i,j} - c_t) \\\\ \u0026amp; \\leq Pr( |\\mathbb{E}[\\hat{P}_t] - \\hat{P}_t| \u0026gt; \\Delta_{i,j}/2 ) \\\\ \u0026amp;\\leq 2 \\exp \\left(-t \\epsilon_{i, j}^{2} / 2\\right) \\\\ \u0026amp;\\leq 2 \\exp \\left(-m \\log \\left(T K^{2}\\right)\\right) \\\\ \u0026amp; =2 /\\left(T K^{2}\\right)^{m} , \\end{align*} $$ taking \\( m = \\max{ 4, d }\\), we have \\( Pr( n \\geq t) \\leq K^{-d }\\) since we have \\( c_t \\leq \\Delta_{i,j}/2\\) when \\(t = \\left\\lceil m \\log \\left(T K^{2}\\right) / \\epsilon_{i, j}^{2}\\right\\rceil\\) and \\( m \u0026gt; 4\\).\nBtM leverages the concept of Borda score: $$ b(a_i) = \\frac{1}{K}\\sum_{j} p_{i, j} $$ and two facts:\nthe Condorcet winner cannot be a Borda loser, in the sense that the average Borda score must be greater than \\(0.5\\); the Condorcet winner stays if some other arms are \u0026ldquo;removed\u0026rdquo;. Therefore, as long as we keep eliminate Borda losers, nothing will be left except for the Condorcet winner.\nThe problem with IF is that the theoretical guarantee requires some sorts of Strong Stochastic Transitivity (SST) property: for any triple \\( (i, j, k)\\), \\( \\Delta_{i,k } \\geq \\max \\{ \\Delta_{1, j}, \\Delta_{1,k} \\}\\). BtM only requires a relaxed SST property: there exists \\(\\gamma \\geq 1\\) such that for all pairs \\( (j,k)\\) with \\( 1 \u0026lt; j \u0026lt; k\\), we have \\( \\gamma \\Delta_{1,k} \\geq \\max\\{ \\Delta_{1, j}, \\Delta_{1,k} \\}\\). \\(\\gamma\\) measures the hardness of the problem, as the smaller the gap becomes, the harder it is to identify the better arm while dueling.\nThe following regret bounds have been proven already, given \\(\\Delta_{\\min}\\) and \\( \\gamma\\), $$ \\begin{align*} \\mathbb{E}[ \\mathcal{R}_T^{IF} ] \u0026amp; \\leq \\mathcal{O}\\left( \\frac{K \\log T}{\\Delta_{\\min} }\\right), \\\\ \\mathcal{R}_T^{BtM} \u0026amp; \\leq \\mathcal{O}\\left( \\frac{\\gamma^7 K \\log T}{\\Delta_{\\min }}\\right) \\quad \\text{ with high probability}. \\end{align*} $$\nSensitivity Analysis of VAriables of Generic Exploration (SAVAGE) and Doubler SAVAGE works in a way similar to how BtM works: if we know there is a Condorcet winner, any arms that lose with high probability can be safely eliminated from further consideration. So we can compare arms in a round robin (all-to-all) fashion and drop the pairs of arms as long as it\u0026rsquo;s \u0026ldquo;safe\u0026rdquo; to do so. Its regret bound is of order \\( \\mathcal{O}( K^2 \\log T)\\), which is not tight, however, it empirically outperforms IF and BtM by a wide margin when the arm size is moderate.\nDoubler converts the dueling bandits into conventional multi-armed bandit problems, under the assumptions that the preferences are linear choice functions of underlying utilities associated with the arms. In other words, \\( \\Delta_{A,B} = (\\mu_A - \\mu_B)/2\\), \\(\\mu_A\\) is the mean utility of arm \\(A\\), for example. Doubler proceeds in epochs of exponentially increasing size, (called \u0026ldquo;doubling trick\u0026rdquo;). In each epoch, the left arm is sampled from a fixed distribution, the right arm is chosen from a ordinary bandit algorithm, minimizing the regret against the left arm.\nThere is also a bunch of algorithms based on UCB (Upper Confidence Bound) variants. The fundamental principle for UCB type of algorithms is one can always create a confidence interval for the interesetd statistic estimates depending on how the sampling procedure is going on. (People write tons of tons of paper about different procedures while essentially they are the variants for the same thing, and then there will be someone trying to unifying the different frameworks. Always interesting to keep this thread going on.)\nRMED and Sparring The Relative Minimum Empirical Divergence (RMED) algorithm has been proved to have optimal asymptotic regret. First of all, the authors from 3 constructed some nuanced lower bound example, (I browsed their paper, a little bit hard to comprehend\u0026hellip;) to claim that the lower bound for dueling bandits problem is characterized by $$ \\liminf_{T \\rightarrow \\infty} \\frac{\\mathbb{E}[R(T)]}{\\log T} \\geq \\sum_{i \\in[K] \\backslash{1}} \\min_{j \\in \\mathcal{O}_{i}} \\frac{\\Delta_{1, i}+\\Delta_{1, j}}{2 d\\left(\\mu_{i, j}, 1 / 2\\right)} $$ where the notation \\( d(\\mu_{i,j}, \\frac{1}{2})\\) is kind of characterzing how \u0026ldquo;easy\u0026rdquo; it is to tell which one is better, \\(i\\) or \\(j\\). The trick to match this lower bound is sort of like the plugin principle, you design an algorithm that directly uses the empirical divergence as a criteria to pick the Condorcet winner.\nThe Sparring series belongs to the symmetric algorithms, it is inspired by that the dueling bandits problem is just an example of a symmetric game. symmetric game is one where the payoffs for playing a particular strategy depend only on the other strategies employed, not on who is playing them. That is, if any two players were to switch their strategies with one another, their payoffs would switch as well, leaving the overall outcome of the game unchanged. This is intuitively understandable because you can always view the dueling recommendations as actions played by two players and the eventual goal is to come up with a \u0026ldquo;draw\u0026rdquo; that no one wants to deviate.\nTherefore, any no-regret dynamics would lead to the empirical convergence to the equilibrium of such a game, oftentimes the regret has the form of \\( \\mathcal{O}(\\sqrt{T}) \\), I\u0026rsquo;m going to write a post to discuss the adversarial bandit problems later to talk about my understandings. What was surprising is that empirical experiments show that adversarial algorithms seem to have a logaritmic regret rate despite the proven bound.\nIn general, given an algorithm \\( \\mathcal{A} \\) that solves the adversarial bandit problem, we can use it to sovle the dueling bandits problem by placing a row player and a column player, both sparring with each other by giving out his bet of best action, one gets reward \\(1\\) if he wins, otherwise \\(0\\). As the comparisons have been carried out, the player uses \\(\\mathcal{A}\\) to update their strategies. In this fashion, we are sort of obtaining a sampling estimate of the preference matrix, or at least getting some information of this preference matrix through sparring, it is just this sampling procedure is symmetric.\nA bit thoughts Dueling bandits, as a variant of the multi-armed bandit problem has its relevance in that the preference feedback nowadays are way more easier to obtain. It sort of gave rises to the concept of Reinforcement Learning with Human Feedback and I believe chatgpt has been significantly benefiting from that. I personally think the whole RLHF thing don\u0026rsquo;t really quite need a theoretical fundation unless it is really instructive. Entertainment-wise, I love the advancement of dueling bandits in that it gives a theoretical formalism for how we can deal with discrete type of data. We\u0026rsquo;ll see where it leads us to.\nYue, Y., Broder, J., Kleinberg, R., \u0026amp; Joachims, T. (2012). The k-armed dueling bandits problem. Journal of Computer and System Sciences, 78(5), 1538-1556.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nYue, Y., \u0026amp; Joachims, T. (2011). Beat the mean bandit. In Proceedings of the 28th international conference on machine learning (ICML-11) (pp. 241-248).\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nKomiyama, J., Honda, J., Kashima, H. and Nakagawa, H., 2015, June. Regret lower bound and optimal algorithm in dueling bandit problem. In Conference on learning theory (pp. 1141-1154). PMLR.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"2023-10-11T00:00:00Z","permalink":"https://unionpan.github.io/post/2023/dueling_bandits/","section":"post","tags":null,"title":" Some Notes on Dueling Bandits"},{"categories":null,"contents":"When you write an About Me, you want something permanent. I don\u0026rsquo;t have much that\u0026rsquo;s permanent, but I go by \u0026lsquo;Union\u0026rsquo; \u0026mdash; a homophone of my Chinese name that I find cute and hope will stick. What won\u0026rsquo;t last much longer is that I\u0026rsquo;m finishing my PhD at NYU Tandon, and here goes the elevator pitch for my research:\nMy dissertation focuses on the transient, macroscopic, and microscopic behaviors of multi-agent learning and interactions, which emerge from the complex systems accross various disciplines. My work primarily centers on the intersection of statistical reinforcement learning and applied game theory, where I try to quantify the uncertainties arise in decision-making processes and understand their efficiency.\nYou might think to yourself \u0026ldquo;sounds like some pedantic academic buzzword bingo\u0026rdquo; and you\u0026rsquo;d be right. Well then, maybe:\nI designed a decentralized multi-radar communication algorithm that mitigates the mutual interference during the target detection.\nI can assure you no one has ever been stupid enough to have done this.\nAnyways, the most permenant thing may be my upbringing. I was born in Taoyuan, Hunan province of China during the 90s. My parents were both middle school teachers during that time, before their shifting to politics. It seemed to be the only path that\u0026rsquo;s ultimately justified: （修身，齐家，治国，平天下）\nPerhaps this is the epitome of deep-seated Ruism in East Asian culture, where individual values are measured by their bureaucratic or political success. The irony is they barely advanced down this path apparently, despite being incredibly hard-working people. I have only flagments of memories caught in their chats, but maybe somewhere in those hilly villages, some of the roads and dams owe something to their efforts.\nGrowing up with generational gaps has a lot of causes, it\u0026rsquo;s never irreconcilable\u0026mdash;reconciliation just comes before comprehension. in many ways, I admit they are more like pilgrims than I could ever be. Now I suppose we all acknowledge that regardless of the direction life pulls us, there is profound merit in simply \u0026ldquo;keep doing something\u0026rdquo;. Que sera sera.\n","date":"2023-08-26T20:18:54+03:00","permalink":"https://unionpan.github.io/post/about_me/about/","section":"post","tags":null,"title":"About Me"},{"categories":["Some random math"],"contents":"This post is dedicated to Erdos-Szekeres Theorem, which says:\nES Thm. (Monotone sequence) For any positive integer $n$, any sequence of $n^2 + 1$ distinct real numbers contains a monotone subsequence of length at least $n+1$.\nThis means that within any sufficiently long sequence, there is either an increasing subsequence or a decreasing subsequence of a certain minimum length.\nThere\u0026rsquo;s another version of the theorem statement coming from the 1935 paper by Paul Erdos and George Szekeres1, which concerns combinatorial geometry. It says:\nES Thm. (combinatorial geometry) For every positive integer $n$ among every $N=\\binom{2 n-4}{n-2}+1 \\sim 4^{n} / \\sqrt{n}$ points $p_1, \\ldots, p_N \\in \\mathbb{R}^2$, where $p_i = (x_i , y_i)$ and $x_1 \u0026lt; x_2 \u0026lt; \\ldots \u0026lt; x_N$, there is a convex configuration or a concave configuation of at least $n$ points.\nThis means that there are indices $i_1 \u0026lt; \\ldots \u0026lt; i_n$ such that the slopes of the segments $p_{i_j} p_{i_{j+1}}, j = 1, \\ldots, n-1$ are either all nondecreasing or all nonincreasing.\nThese two statements can be traced back to Ramsey\u0026rsquo;s theory.\nRamsey Thm. For every $k,r,n \\in \\mathbb{N}$ there exists a number $N \\in \\mathbb{N}$ such that for every coloring of the $k$-tuples of the $[N]$ by $r$ colors there is a subset $T \\subseteq [N]$ of size $n$ such that all $k$-tuples of elements of $T$ have the same color. The Ramsey number, $R^r_n (k)$ is the smallest such $N$.\nI might have lost you here already. The connections between these three theorems are pretty vague to me at first. But let me make an analogy to clarify that ES theorem actually makes a special case of Ramsey theorem.\nFirst, Ramsey\u0026rsquo;s theorem can be translated using graph theory language. Let\u0026rsquo;s focus on the case where we are coloring pairs of points, so $k=2$. The theorem then says that if you take a complete graph $K_N$ with a large enough number of vertices $N$, and you color every edge with one of $r$ colors, you are guaranteed to find a complete subgraph $K_n$ (a clique of size $n$) where all edges have the same color.\nLet\u0026rsquo;s see how this applies to the Monotone Sequence Theorem.\nImagine there is a graph that connects the elements of our sequence of $N = n^2+1$ distinct numbers, ${x_1, x_2, \\ldots, x_N}$.\nVertices: Let the vertices of a complete graph $K_N$ be the numbers $x_1, \\ldots, x_N$. Edges: The edges are all the pairs $(x_i, x_j)$ where $i \u0026lt; j$. Colors: We will use two colors ($r=2$), let\u0026rsquo;s call them \u0026ldquo;blue\u0026rdquo; and \u0026ldquo;red\u0026rdquo;. Color the edge $(x_i, x_j)$ blue if $x_i \u0026lt; x_j$ (the sequence is \u0026ldquo;increasing\u0026rdquo; at this step). Color the edge $(x_i, x_j)$ red if $x_i \u0026gt; x_j$ (the sequence is \u0026ldquo;decreasing\u0026rdquo; at this step). Since every pair of numbers is either increasing or decreasing, every edge in our complete graph $K_N$ is colored. Now, what does Ramsey\u0026rsquo;s Theorem promise us? It guarantees that there exists a monochromatic clique of a certain size. What is that in our context?\nA blue clique of size $n+1$ is a subset of vertices ${x_{i_1}, x_{i_2}, \\ldots, x_{i_{n+1}}}$ with $i_1 \u0026lt; i_2 \u0026lt; \\ldots \u0026lt; i_{n+1}$ such that every edge between them is blue. This means for any pair $j \u0026lt; k$, the edge $(x_{i_j}, x_{i_k})$ is blue, which implies $x_{i_j} \u0026lt; x_{i_k}$. This is precisely an increasing subsequence of length $n+1$.\nA red clique of size $n+1$ is a subset of vertices ${x_{i_1}, x_{i_2}, \\ldots, x_{i_{n+1}}}$ with $i_1 \u0026lt; i_2 \u0026lt; \\ldots \u0026lt; i_{n+1}$ such that every edge between them is red. This means for any pair $j \u0026lt; k$, the edge $(x_{i_j}, x_{i_k})$ is red, which implies $x_{i_j} \u0026gt; x_{i_k}$. This is precisely a decreasing subsequence of length $n+1$.\nRamsey\u0026rsquo;s Theorem for two colors, $R(n+1, n+1)$, gives us an upper bound on the sequence length $N$ needed to guarantee one of these structures. The Erdős-Szekeres Theorem provides a much tighter bound, $N = n^2 + 1$, but the underlying principle is the same: in a sufficiently large system, order must emerge.\nA Simple Proof While the connection to Ramsey Theory is fundamental, there is a wonderfully simple proof of the Monotone Sequence Theorem that feels almost like magic. It\u0026rsquo;s one of my all-time favorites.\nLet\u0026rsquo;s take our sequence $X = (x_1, x_2, \\ldots, x_{n^2+1})$. For each number $x_k$ in the sequence, let\u0026rsquo;s assign it a special label—a pair of integers $(a_k, b_k)$. We define them like this:\n$a_k$ = the length of the longest increasing subsequence that ends with $x_k$. $b_k$ = the length of the longest decreasing subsequence that ends with $x_k$. Now, here’s the crucial insight: every single one of these labels is unique.\nLet\u0026rsquo;s think about why this has to be true. Pick any two numbers from our sequence, say $x_i$ and $x_j$, where $x_i$ comes before $x_j$ (so $i \u0026lt; j$). Since all the numbers are distinct, there are only two possibilities:\nIf $x_i$ is smaller than $x_j$: We can take the longest increasing subsequence ending at $x_i$ (which we know has length $a_i$) and just tack $x_j$ onto the end. This gives us a new, longer increasing subsequence that ends at $x_j$. Its length is $a_i + 1$. This means the longest possible increasing subsequence ending at $x_j$ must be at least this long, so $a_j \\ge a_i + 1$. Right away, we know $a_i$ and $a_j$ can\u0026rsquo;t be the same.\nIf $x_i$ is greater than $x_j$: We can do the same trick with the decreasing subsequences. Take the longest decreasing subsequence ending at $x_i$ (length $b_i$) and add $x_j$ to it. This creates a decreasing subsequence ending at $x_j$ of length $b_i + 1$. So, we must have $b_j \\ge b_i + 1$, which means $b_i$ and $b_j$ can\u0026rsquo;t be the same.\nIn either scenario, the label $(a_i, b_i)$ is different from the label $(a_j, b_j)$. Since this applies to any pair of numbers in our sequence, it means all $n^2+1$ labels are unique.\nThis is where the argument comes together. Let\u0026rsquo;s assume, just for a moment, that the theorem is wrong and there is no monotone subsequence of length $n+1$. What would that mean for our labels?\nIt would mean that the length of any increasing subsequence is at most $n$, and the length of any decreasing subsequence is also at most $n$. This puts a very tight limit on the possible values in our labels: $$1 \\le a_k \\le n$$ $$1 \\le b_k \\le n$$\nSo, if our assumption is true, how many different labels could we possibly create? The first number, $a_k$, can be anything from 1 to $n$. The second number, $b_k$, can also be anything from 1 to $n$. This gives us a total of $n \\times n = n^2$ possible unique labels.\nBut wait. We have $n^2+1$ numbers in our sequence, and we just proved that every single one of them generates a completely unique label. This leads to a logical contradiction: we have $n^2+1$ unique items, but we only have $n^2$ available slots for them to fit into. That\u0026rsquo;s simply not possible.\nBy contradiction, there must be at least one monotone subsequence of length $n+1$ or greater, hence the proof.\nErdös, P. and Szekeres, G., 1935. A combinatorial problem in geometry. Compositio mathematica, 2, pp.463-470.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"2023-07-24T00:00:00Z","permalink":"https://unionpan.github.io/post/2023/erdos-szekeres/","section":"post","tags":null,"title":"Erdos-Szekeres Theorem"},{"categories":["Some random math"],"contents":"I remember having those unpleasant lines in our high school canteen, flooded by the starving students queeing for their lunch, I would always pick a window with fewer people waiting, compromising myself with awful food. Another similar thought that always stricked me was the odds that tourists always pick the same time to travel, there\u0026rsquo;s almost always traffic congestion everywhere during holiday seasons. Yeah, lives have been always so hard.\nOn the first level of thinking, when there\u0026rsquo;s a lack of resource you attempt to find alternatives, e.g., picking another time for traveling, another food window, etc.. The level two thinking is maybe \u0026lsquo;\u0026lsquo;since other people are avoiding holidays, what if I insist on going out on holidays\u0026rsquo;\u0026rsquo;? Or maybe there\u0026rsquo;s a twisted level three thinking, \u0026lsquo;\u0026lsquo;what if everybody thinks like the level two thinker, \u0026hellip;.\u0026rsquo;\u0026rsquo; It\u0026rsquo;s somewhat intimidating to follow this infinite hierachy, but definitely rewarding, as there are certainly moments when people are regretting their travel decisions, thinking to themselves \u0026lsquo;\u0026lsquo;I probably should not have gone the high way.\u0026rsquo;\u0026rsquo;\nThe aforementioned phenomenons significantly resembled the first idea proposed by Wardrop1 in 1952. Wardrop conducted some \u0026ldquo;before-and-after\u0026rdquo; analysis over some traffic data, and came into the two alternative criteria to determine the flow distribution on the routes,\nThe journey times on all the routes actually used are equal, and less than those which would be experienced by a single vehicle on any unused route; The average journey time is a minimum. It was not until Beckmann2, 1956 that the two simple yet powerful ideas were mathematically formulated, and was considered an example of Rosenthal games3. The first principle basically states that nobody should be happy to deviate from their own routes, which is essentially the behavior of Nash equilibrium.\nTo elaborate, we consider a transporation network represented by a directed, finite, and connected graph $ \\mathcal{G} = (\\mathcal{V}, \\mathcal{E})$, where \\(\\mathcal{V}\\) represent road junctions, the edges \\(\\mathcal{E}\\) represent road segments. The set of Origin-Destination (OD) pairs is \\(\\mathcal{W} \\subseteq \\mathcal{V} \\times \\mathcal{V}\\), indexed by \\(w\\), \\(|\\mathcal{W}| = :W\\). Let \\(\\mathcal{P}: = \\bigcup_{w\\in\\mathcal{W}}\\mathcal{P}_w\\) be the set of directed paths between OD pairs, each path set \\(\\mathcal{P}_w\\) is indexed by \\(w\\).\nThe individual vehicles traveling through \\(\\mathcal{G}\\) are infinitesimal players over \\(\\mathcal{G}\\), denoted by a measurable space \\((\\mathcal{X}, \\mathcal{M}, m)\\). The players are non-atomic, i.e., \\( m(x) = 0 \\ \\ \\forall x \\in \\mathcal{X}\\); they are split into distinct populations indexed by the OD pairs, i.e., \\(\\mathcal{X} = \\bigcup_{w\\in\\mathcal{W}} \\mathcal{X}_w\\) and \\(\\mathcal{X}_w \\bigcap\\) \\(\\mathcal{X}_w\u0026rsquo; = \\emptyset,\\ \\forall w, w^{\u0026rsquo;} \\in \\mathcal{W}\\). For each OD pair \\(w \\in \\mathcal{W}\\), let \\(m_w = m(\\mathcal{X}_w) \\) represent the traffic demand. For each player \\(x \\in \\mathcal{X}_w\\), we assume that their travel path \\(a \\in \\mathcal{P}_w\\) is fixed right after the path selection.\nThe action profile of all the players \\(\\mathcal{X}\\) induces an edge flow vector \\(q \\in \\mathbb{R}^{|\\mathcal{E}|}_+\\), where \\(q_e := \\int_{\\mathcal{X}} \\mathbb{I}_{{e \\in a}} m(dx), e \\in \\mathcal{E}\\), and a path flow vector \\(\\mu \\in \\Delta :=\\{(\\mu_p)_{p\\in \\cup_{w\\in \\mathcal{W}}\\mathcal{P}_w} | \\mu_p := {\\int_{\\mathcal{X}_w} \\mathbb{I}_{{a = p}} m(dx)}\\}\\). The edge-path incident matrix is \\(\\Lambda = [\\Lambda^1 \\vert , \\ldots, \\vert \\Lambda^{|\\mathcal{W}|}] \\in \\mathbb{R}^{|\\mathcal{E}| \\times |\\mathcal{P}|}\\), \\(\\Lambda^w_{e, p} = \\mathbb{I}_{{e\\in p}}, \\forall e\\in \\mathcal{E}, w \\in \\mathcal{W}, p \\in \\mathcal{P}_w\\). Easy to verify the compact form of edge-path flow relation is \\(q = \\Lambda \\mu\\).\nLet \\((\\Omega, \\mathcal{F}, \\mathbb{P})\\) be the probability space, \\(l_e: \\mathbb{R}_+ \\times \\Omega \\to \\mathbb{R}_+\\) be the cost/latency functions, measuring the travel delay of the edge \\(e \\in \\mathcal{E}\\) determined by its edge flow \\(q_e\\) and a state variable \\(\\omega \\in \\Omega\\) that is universal for the entire traffic network, e.g., \\(\\omega\\) can represent the weather condition, road incidents or anything that affects the congestion level. Let \\(l : \\mathbb{R}^{|\\mathcal{E}|}_{\\geq 0} \\times \\Omega \\mapsto \\mathbb{R}^{|\\mathcal{E}|}_+\\) denote the vector-valued latency function.For an instance \\(\\omega \\in \\Omega\\), the latency of path \\(p\\) is defined as \\(\\ell_p : = \\sum_{e \\in p} l_e (q_e, \\omega ) = \\Lambda^{\\top}_p l (\\Lambda \\mu, \\omega )\\), which can be seen as a function of \\(\\mu\\) and \\(\\omega\\), written as \\(\\ell_p = \\ell_p(\\mu, \\omega)\\). We write the vector-valued path latency function as \\(\\ell : \\Delta \\times \\Omega \\mapsto \\mathbb{R}^{|\\mathcal{P}|}_+\\). Each instance \\(\\omega\\) determines a congestion game, captured by the tuple \\(\\mathcal{G}_c^{\\omega} = ( \\mathcal{G}, \\mathcal{W}, \\mathcal{X} , \\mathcal{P}, \\mathcal{\\ell}(\\cdot, \\omega) )\\). Each path flow profile \\(\\mu \\in \\Delta\\) induces a probability measure associated with the positive random vector \\(\\ell(\\mu, \\cdot): \\Omega \\mapsto \\mathbb{R}^{|\\mathcal{P}|}_+\\). The following assumption gives some realistic properties for the latency.\nStanding Assumption For all \\(e \\in \\mathcal{E}\\), the latency functions \\(l_e\\) are \\(\\omega\\)-measurable, for all \\(\\omega \\in \\Omega\\), \\(l_e\\) are \\(L_0\\)-Lipschitz continuous and differentiable in \\(q_e\\) with \\(\\cfrac{ \\partial l_e (q_e, \\omega)}{\\partial q_e} \u0026gt; 0 \\) for all \\(q_e \\geq 0\\).\nNow let\u0026rsquo;s define and find the equilibria satisfying of a deterministic game, i.e., when \\(|\\Omega|\\) is a singleton. The first, as it turns out, coincides with the Nash Equilibrium.\nDefinition (Wardrop Equilibrium) A path flow \\(\\mu \\in \\Delta\\) is said to be a Wardrop Equilibrium (WE) if \\(\\forall w \\in \\mathcal{W}\\), \\( \\mu_p \u0026gt; 0\\) indicates \\( \\ell_p \\leq \\ell_{p^{\\prime}}\\) for all \\(p^{\\prime} \\in \\mathcal{P}_w\\).\nThe second, while not satisfying the incentive conditions, somehow concides with the Nash equilibrium in a \u0026ldquo;regularized\u0026rdquo; version of the game, where the utility for each individual player is a sum of the latency and a \u0026ldquo;toll price\u0026rdquo;.\nDefinition (System Equilibrium) A path flow \\(\\mu \\in \\Delta\\) is said to be a system optimum if the aggregated latency \\(S(\\mu) := \\sum_{e \\in \\mathcal{E}} q_e l_e \\) is minimized.\nWardrop equilibrium can be characterized by the minimization of an objective function called Beckmann potential, we end this post by proving it.\nTheorem A path flow \\(\\mu^* \\in \\Delta\\) is a WE if and only if it minimizes the Beckmann potential: $$ \\min_{\\mu \\in \\Delta} \\Phi (\\mu) := \\sum_{e\\in \\mathcal{E}}\\int_{0}^{(\\Lambda \\mu)_e} l_e (z) dz. $$\nProof We first write down the constraint \\(\\Delta\\) as a set of inequalities and equalities, $$ \\begin{align*} -\\mu \u0026amp; \\preceq 0 \\\\ M \\mu - \\bf{m} \u0026amp; = 0 \\end{align*} $$ where \\(M \\in \\mathbb{R}^{W \\times |\\mathcal{P}|}\\), \\(M_{w,p} = 1\\) if \\(p \\in \\mathcal{P}_w\\) otherwise \\(0\\), \\(\\boldsymbol{m} = (m_1, \\ldots, m_W)\\) is the measure vector.\nWriting down the Lagrangian, by defining multipliers \\(\\lambda \\in \\mathbb{R}^{|\\mathcal{P}|}, \\nu \\in \\mathbb{R}^{W}\\), let \\(t_e = (0, \\ldots, \\underbrace{1}_{e^{th} edge}, \\ldots, 0)\\) be the basis vecotrs, $$ \\mathcal{L} (\\mu , \\lambda, \\nu ) = \\sum_{e\\in \\mathcal{E}} \\int_{0}^{(t_e \\Lambda) \\mu} l_e (z) dz - \\lambda^{\\top} \\mu - \\nu^{\\top} (M\\mu - \\boldsymbol{m}). $$\nThe KKT condition says, $$ \\begin{align*} \\nabla_{\\mu} \\mathcal{L} \u0026amp; = \\sum_{e \\in \\mathcal{E}} \\Lambda^{\\top} t_e^{\\top} l_e(q_e ) - \\lambda - M^{\\top}\\nu \\\\ \u0026amp; = \\ell - \\lambda - M^{\\top}\\nu = 0 \\\\ \\lambda^{\\top} \\mu \u0026amp; = 0 \\\\ \\lambda \u0026amp; \\succeq 0 \\end{align*} $$ we get that \\(\\ell \\succeq M^{\\top} \\nu\\), \\(\\ell^{\\top }\\mu = \\nu^{\\top} M \\mu\\), what this essentially means is that whenever \\(\\mu_p \u0026gt; 0\\), \\(\\ell_p\\) are identical in that path set \\(\\mathcal{P}_w\\). To see this, note that \\(\\ell_p \\geq (M^{\\top} \\nu)_p = \\sum_{w \\in \\mathcal{W}} \\nu_w \\mathbb{I}_{ p \\in \\mathcal{P}_w }\\), which is a constant lower bound for \\(p \\in \\mathcal{P}_w\\), fixing a \\(w\\); and \\(\\sum_{w \\in \\mathcal{W}}\\sum_{p \\in \\mathcal{P}_w}\\ell_p \\mu_p = \\sum_{w \\in \\mathcal{W}}\\sum_{p \\in \\mathcal{P}_w} (M^{\\top} \\nu)_p \\mu_p \\), this implies that for any \\(w \\in \\mathcal{W}\\), if \\(\\mu_p \u0026gt; 0\\) for some of the \\(p \\in \\mathcal{P}_w\\), the only possibility is that \\(\\ell_p = (M^{\\top}\\nu)_p\\). The uniqueness of the edge flow solution \\(q = \\Lambda \\mu\\) can be established through calculating the Hessian of \\(\\Phi\\), however, \\(\\mu\\) is generally non-unique, as it should be in the solution set to a linear equation.\nWardrop, J.G., 1952. Road paper. some theoretical aspects of road traffic research. Proceedings of the institution of civil engineers, 1(3), pp.325-362.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nBeckmann, M., McGuire, C.B. and Winsten, C.B., 1956. Studies in the Economics of Transportation (No. 226 pp).\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nRosenthal, R.W., 1973. A class of games possessing pure-strategy Nash equilibria. International Journal of Game Theory, 2(1), pp.65-67.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"2023-01-30T00:00:00Z","permalink":"https://unionpan.github.io/post/2023/wardrop_equilibirum/","section":"post","tags":null,"title":"Wardrop Equilibrium"},{"categories":["Some random math"],"contents":"I did a presentation in a group meeting to briefly review the lower complexity bound of first-order convex optimization; and how Nesterov proceed to match the lower bound using the estimation sequence, the slides are here.\nConsider an unconstrained optimization problem: $$ \\min_{x \\in \\mathbb{R}^n} f(x) . $$ Here, \\(f \\in \\mathcal{C}^1\\) is a convex, $L$-Lipschitz smooth function. Obviously we can solve this problem by using first-order methods, using iterations:\n\\[ x_{k} \\in x_{0} + \\operatorname{Span} \\left \\{f^{\\prime} \\left (x_{0} \\right ), \\ldots, f^{\\prime} \\left (x_{k-1} \\right )\\right \\}. \\]\nThe question is what their fundamental limit is, and how to achieve it.\nFor fundamental limits, Nesterov constructed a quadratic function whose one-step gradients give very little geometry information about the other dimensions, so that once the dimensionality of the problem becomes large, it gets really hard to solve it. (As the error is lower bounded by $\\mathcal{O} (1/k^2)$, with $k$ being the dimension.)\nHe then came up with an algorithm to match the lower bound. Despite the complexity of the algorithm itself, the idea is not that complicated:\nWe use a series of weighted quadratic functions $\\{\\lambda_k, \\phi_k \\}$ to estimate the function itself at each point $x_k$, which has two properties\nThey provide a lower bound to the true function at each step. $$ \\phi_{k}(x) \\leq (1-\\lambda_{k} ) f(x)+\\lambda_{k} \\phi_{0}(x) $$ They converge to the true function as the algorithm progresses. $$ \\lambda_k \\to 0 $$ So intead of minimizing the function itself, we minimize $\\{\\phi_k \\}$ at each step, which is easier but also matches the lower bound because it is quadratic!\nThere are of course a bunch of alternative interpretations about this method, one of which I found quite intriguing was the dual perspective1, which views every accelerated gradient step as an optimal coupling of the primal and dual step.\nAllen-Zhu, Z. and Orecchia, L., 2014. Linear coupling: An ultimate unification of gradient and mirror descent. arXiv preprint arXiv:1407.1537.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"2022-12-31T00:00:00Z","permalink":"https://unionpan.github.io/post/2022/nesterov/","section":"post","tags":null,"title":"Nesterov"},{"categories":["Some random math"],"contents":"This is pretty much scribed from Tor Lattimore\u0026rsquo;s Bandit Algorithms book,Bandit Algorithms. This book has been the most informational item for me in a while. There is a chapter about the foundations of information theory, which is simply entropy, but the story telling reminds me of deception. Entropy, in plain words, is the measure of uncertainty for certain information; to communicate is to eliminate such uncertainty, to deceive, however, is to obscure certain information, hence to enforce such uncertainty. The duality has never been formalized as far as I know, because if you simply name the Shannon limit as the capacity of deception, you are doing anything innovative. But, if you think from Shannon\u0026rsquo;s perspective, wasn\u0026rsquo;t he also just doing the same thing? (Sorry for quoting my advisor here.) Anyway here goes the story:\nSo Alice wants to tell with Bob about a sequence of \\(n\\) independent random outcomes sampled from a known distribution \\(Q\\). To keep things concise, they\u0026rsquo;ve agreed on a secret binary language. Now, we know that the entropy of \\(Q\\) can be interpreted as the expected number of bits necessary per random variable using the optimal code as \\(n\\) goes to \\(\\infty\\). The relative entropy between distributions \\(P\\) and \\(Q\\), we can think of it as the extra bits Alice and Bob have to lug around if they mistakenly believe the random variables are sampled from \\(P\\) instead of \\(Q\\).\nLet $P$ be a measure on $[N]$ with $\\sigma$-algebra $2^{[N]}$ and $X: [N] \\to [N]$ be the identity random variable, i.e., $X(\\omega) =X$. Since binary code is used to convey the message, they might code each $X$ as a binary code function $c: [N] \\to \\{0,1\\}^* $ where $\\{0,1\\}^{*} $ is the set of finite sequence of zeros and ones. $c$ must be injective (so it won\u0026rsquo;t cause amibiguity between different random variables), and prefix free (so it won\u0026rsquo;t cause ambiguity between any two codes). This is simply because Bob needs to know where one symbol starts and ends for multiple samples.\nWe know that the easiest choice is to use $\\lceil \\log (N) \\rceil$ bits no matter what value of $X$ is, but if $X$ is far from uniform, let\u0026rsquo;s say $P(X = 1) = 0.99$, and then no matter what the rest of them look like, it\u0026rsquo;s preferreable to use shorter code for $X = 1$ than $\\lceil \\log (N) \\rceil$. A natural objective formulated is\n$$ c^* = \\arg\\min_c \\mathbb{E}_{i \\sim P } [ length(c(i)) ]. $$\nIt is well known that this optimization problem can be solved by Huffman Coding, thus the optimal value satisfies:\n$$ H_2(P) \\leq \\sum_{i=1}^N p_i length(c^*(i)) \\leq H_2(P) + 1, $$\nwhere $H_2(P)$ is the entropy of $P$\n\\[ H_2(P) = \\sum_{i=1, p_i \u0026gt; 0}^N - p_i \\log(p_i) . \\]\nThe naive idea of using a code of uniform length is only recovered when $P$ is uniformly distributed. Why $p_i \u0026gt; 0$? Think about $\\lim_{x \\to 0^+} x\\log(x) = 0$, or think about $H_2(P)$ as kind of an expectation, which should not change under the perturbation of measure $0$ set.\nThe entropy $H_2(P)$ is a fundamental quantity. It\u0026rsquo;s based on $\\log_2$ since we are talking about binary code, sometimes it\u0026rsquo;s more convenient to scale it with natural logarithm. Shannon Source Coding Theorem tells us (informal) that any $P$ compressed to fewer than $N H_2(P)$ will inevitably result in information lost, so any coding that results average bits cost $H_2(P)$ is unimprovable.\nRelative Entropy Now, imagine Alice and Bob in a parallel universe where they use a code that is optimal for $X$ sampled from distribution $Q$, but actually $X$ is sampled from $P$. Here comes the terminology of related entropy between $P$ and $Q$, it measures how much longer the messages are expected to be using the optimal code for $Q$ than what is obtained from using optimal code for $P$. Let $p_i = P(X= i)$ and $q_i = Q(X= i)$, assuming Shannon\u0026rsquo;s coding, the definition of relative entropy can be written as\n$$ D(P,Q) = \\sum_{i \\in [N]: p_i \u0026gt; 0} - p_i \\log(q_i) - (\\sum_{i \\in [N]: p_i \u0026gt; 0} - p_i \\log(p_i)) = \\sum_{i \\in [N]: p_i \u0026gt; 0} p_i \\log(\\frac{p_i}{q_i}) $$\nFrom Jensen\u0026rsquo;s inequality ($\\log$ is concave so using the fact that $\\mathbb{E}_p \\{\\log(\\frac{q_i}{p_i})\\} \\leq \\log( \\mathbb{E}_p ( \\frac{q_i}{p_i})))$ or the optimality of coding, $D(P,Q) \\geq 0$. Actually this is also called KL divergence just in some other contexts. Question remains that what if $p_i $ and $ q_i = 0$ for some $i \\in [N]$? Well, it means that $i$ is not neccessary for consideration since by both $P$ and $Q$, $i$ is in a measure zero set. Also, the sufficient and necessary condition for $D(P,Q)$ to be finite is that whenver $q_i = 0$, $p_i = 0$. Using measure-theoretic language, this condition means that $P$ is absolutely continuous with respect to $Q$.\nNow we jump out of the story and consider arbitrary measurable space $(\\Omega, \\mathcal{F})$. Support of $P$ might not be finite, or even countable. Defining entropy through the same path is pretty hard as the symbols needed have to be infinite. This fundamental difficulty is automatically resolved if we directly consider relative entropy.\nFormally, if we do a discretization over the sample space $\\Omega$, i.e., find a measurable map $X : \\Omega \\to [N]$. Then, the relative entropy can be defined as $$ D(P,Q) = \\sup_{N \\in \\mathbb{N}^+} \\sup_{X} D(P_X, Q_X), $$ $P_X$ and $Q_X$ are pushforwards, i.e., they measure, say, $\\forall \\mathcal{I} \\in 2^{[N]}$, by $P( X^{-1}(\\mathcal{I}))$. In other words, the relative entropy here actually measures the capacity of Bob distinguishing between $P$ and $Q$ by receiving the \u0026lsquo;\u0026lsquo;codes\u0026rsquo;\u0026rsquo; $\\mathcal{I}$, however the encrpytion is done by Alice. This measurement has profound meanings in a ton of applications.\nTheorem 1 Let $(\\Omega, \\mathcal{F})$ be a measurable space, also let $P$ and $Q$ be measures on this space. Then\n$$ D(P,Q) = \\begin{cases} \\int \\log(\\frac{dP}{dQ}(\\omega)) dP(\\omega) ,\\ \\ \u0026amp;\\text{if} P \\ll Q; \\\\\\ \\infty, \\quad \u0026amp;\\text{otherwise} \\end{cases} \\tag{1} $$\nWhen calculating the relative entropy the densities are always used. If $\\lambda$ is a $\\sigma$-finite measure dominating both $P$ and $Q$, let $p = \\frac{dP}{d\\lambda}$ and $q = \\frac{dQ}{d\\lambda}$, if $P \\ll Q$, by chain rule we write\n$$ D(P,Q) = \\int p \\log(\\frac{p}{q}) d\\lambda \\tag{2} $$\nSuch a $\\lambda$ can always be found, for example $\\lambda = P+Q$ always dominate $P$ and $Q$.\nNote that relative entropy measures the distance from $P$ to $Q$ but it can never be treated as a metric since there are some properties unsatisfied such as triangular inequality and commutability. However, it serves the same purpose.\nExamples Consider two Gaussian variables with means $\\mu_1$ and $\\mu_2$ and variance $\\sigma^2$:\n$$ D(\\mathcal{N}(\\mu_1 , \\sigma^2), \\mathcal{N}(\\mu_2 , \\sigma^2)) = \\frac{(\\mu_1 - \\mu_2)^2}{2\\sigma^2}. $$\nThe quadratic term matches our intuition about such a \u0026lsquo;\u0026lsquo;distance\u0026rsquo;\u0026rsquo;.\nConsider two Bernouli random variables with means $p, q \\in [0,1]$, then:\n$$ D(\\mathcal{B}(p), \\mathcal{B}(q)) = p\\log(\\frac{p}{q}) + (1-p)\\log(\\frac{1-p}{1-q}), $$\nand we have to let $0 \\log(\\cdot) = 0$.\nTo Wrap It Up Now we come to the inequality dictating the capacity of deception, an inequality that Connects the relative entropy to the hardness of hypothesis testing in the following theorem\nTheorem 2 (Bretagnolle-Huber Inequality) Let $P$ and $Q$ be probability measures on the same measurable spcae $(\\Omega, \\mathcal{F})$, and let $A \\in \\mathcal{F}$ be an arbitrary event. Then\n$$ P(A) + Q(A^c) \\geq \\frac{1}{2} \\exp (-D(P,Q)) $$\nwhere $A^c = \\Omega \\backslash A$ is the complement of A.\nProof For reals $a, b$, abbreviate $a \\vee b: = \\max\\{ a, b\\}$, and $a \\wedge b := \\min\\{a,b\\}$. If $D(P,Q) = \\infty$, then the inequality holds trivially true. If it\u0026rsquo;s not, then $P \\ll Q$ by Theorem $1$. Let $\\nu = P+Q$, and the Radon-Nikodym derivatives $p =\\frac{ dP }{d\\nu}$, $q =\\frac{ dQ }{d\\nu}$. By $(2)$, the relative entropy\n$$ D(P,Q) = \\int p \\log(\\frac{p}{q}) d\\nu $$\nSometimes we drop $\\nu$ for brevity, writtin it as $\\int p \\log(\\frac{p}{q})$. It turns out a stronger result is sufficient:\n$$ \\int p\\wedge q \\geq \\frac{1}{2} \\exp(-D(P,Q)). $$\nWhy? Because $\\int p \\wedge q = \\int_A p\\wedge q + \\int_{A^c} p \\wedge q \\leq \\int_A p + \\int_{A^c} q = P(A) + Q(A^c)$. We firstly have to utilize the Cauchy-Schwarz inequality and identity $pq = (p\\wedge q) (p\\vee q)$,\n$$ \\left( \\int \\sqrt{pq}\\right)^2 = \\left( \\sqrt{(p\\wedge q) (p\\wedge q)} \\right) \\leq \\left( \\int p \\wedge q\\right) \\left( \\int p \\vee q\\right). $$\nAlso, using identity $p\\wedge q + p \\vee q = p + q$, we have $\\int p\\wedge q = 2 - \\int p \\vee q \\leq 2$, so for both $p \\vee q$ and $p \\wedge q$ we have them lower bounded by $\\left(\\int \\sqrt{pq}\\right)^2$. Now, using Jensen\u0026rsquo;s inequality we arrive at some elementry manipulation:\n$$\\begin{align} \\left(\\int \\sqrt{pq}\\right)^2 \u0026amp; = \\exp(2 \\log \\int \\sqrt{pq}) = \\exp(2 \\log\\int_{p\u0026gt; 0} p \\sqrt{\\frac{q}{p}}) . \\nonumber \\\\\\ \u0026amp; \\geq \\exp(2\\int_{p \u0026gt; 0} p \\frac{1}{2} \\log(\\frac{q}{p})) = \\exp(-\\int_{pq \u0026gt; 0} p \\log(\\frac{p}{q})) \\nonumber \\\\\\ \u0026amp; = \\exp(-\\int p \\log (\\frac{p}{q})) = \\exp(-D(P,Q)). \\end{align}$$\nSince $P\\ll Q$, $q = 0$ implies $p = 0$, so $p\u0026gt;0$ implies $q \u0026gt; 0$, therefore $pq \u0026gt; 0$. Divide both sides by $2$ concludes the proof.\nThere\u0026rsquo;s a little bit intuition. If $P$ and $Q$ are close, we expect $P(A) + Q(A^c)$ to be large to be close enough to $1$, and how large it is is just quantified by this theorem. Also the result is symmetric and we can always replace $D(P,Q)$ by $D(Q,P)$, yet $D(P,Q)$ is not symmetric, therefore sometimes stronger inequality is obtained.\n","date":"2022-11-16T18:58:11+08:00","permalink":"https://unionpan.github.io/post/2022/the_capacity_of_deception/","section":"post","tags":null,"title":"Capacity of Deception"},{"categories":null,"contents":" Phone +1(646)-404-1857\nEmail pyn950@.gmail.com\n","date":"0001-01-01T00:00:00Z","permalink":"https://unionpan.github.io/contact/","section":"","tags":null,"title":""},{"categories":null,"contents":"I am a P.h.D. candidate at NYU Tandon. My dissertation focuses on the transient, macroscopic, and microscopic behaviors of multi-agent learning and interactions, which emerge from the complex systems accross various disciplines. My work primarily centers on the intersection of statistical reinforcement learning and applied game theory, where I try to quantify the uncertainties arise in decision-making processes and understand their efficiency. An application is the design of decentralized multi-radar communication algorithm that mitigates the mutual interference during the target detection.\nEducation Ph.D., Electrical Engineering (Applied Game Theory) — New York University, 2021–2026\nDissertation focus: Non-equilibrium design in multi-agent learning systems.\nM.Sc., Electrical Engineering (Reinforcement Learning) — New York University, 2018–2020\nProjects:\n• Reproducing TRPO \u0026amp; PPO (code)\n• Urban vaccination-site covering via semi-discrete Optimal Transport (code)\nB.Eng., Communication Engineering (NLP) — Beijing University of Posts and Telecommunications, 2014–2018\nThesis: Text summarization based on Determinantal Point Processes (DPPs) (repo).\nEmployment Graduate Assistant — LARX Lab, 2020–2021\nModel-Agnostic Meta-Reinforcement Learning for LQR\n• Repo: github.com/UnionPan/mamllqr\nTeaching Assistant — NYU ECE\n• Probability and Stochastic Processes (2020)\n• System Optimization Methods (2018–2019)\nPublications Journal Articles Y. Pan, T. Li, Q. Zhu. “Model-agnostic meta-policy optimization via zeroth-order estimation: A linear quadratic regulator perspective,” arXiv:2503.00385, 2025. H. Li, T. Li, Y. Pan, T. Xu, Q. Zhu, Z. Zheng. “Towards universal robust federated learning via meta Stackelberg game,” 2024. OpenReview. Conference Proceedings Y. Pan, J. Li, L. Xu, S. Sun, Q. Zhu. “A game-theoretic approach for high-resolution automotive FMCW radar interference avoidance,” 2025 (arXiv–2503). Y. Pan, Q. Zhu. “Extending no-regret hopping in FMCW radar interference avoidance,” 2025. Y.-T. Yang, Y. Pan, Q. Zhu. “Preference-centric route recommendation: Equilibrium, learning, and provable efficiency,” 2025. Y. Pan, T. Li, Q. Zhu. “On the variational interpretation of mirror play in monotone games,” 2024. arXiv:2403.15636. Y. Pan, T. Li, H. Li, T. Xu, Q. Zhu, Z. Zheng. “A first-order meta Stackelberg method for robust federated learning,” New Frontiers in Adversarial ML Workshop, 2023. OpenReview. Y. Pan, T. Li, Q. Zhu. “Is stochastic mirror descent vulnerable to adversarial delay attacks? A traffic assignment resilience study,” CDC 2023, pp. 8328–8333. DOI. Y. Pan, T. Li, Q. Zhu. “On the resilience of traffic networks under non-equilibrium learning,” ACC 2023, pp. 3484–3489. Y. Pan, Q. Zhu. “On poisoned Wardrop equilibrium in congestion games,” GameSec 2022, pp. 191–211. Y. Pan, Q. Zhu. “Efficient episodic learning of nonstationary and unknown zero-sum games using expert game ensembles,” CDC 2021, pp. 1669–1676. Y. Pan, G. Peng, J. Chen, Q. Zhu. “MASAGE: Model-agnostic sequential and adaptive game estimation,” GameSec 2020, pp. 365–384. Book Chapters \u0026amp; Technical Reports H. Li, T. Xu, T. Li, Y. Pan, Q. Zhu, Z. Zheng. A First-Order Meta Stackelberg Method for Robust Federated Learning (Technical Report), 2023. arXiv:2306.13273. T. Li, Y. Pan, Q. Zhu. Decision-Dominant Strategic Defense Against Lateral Movement for 5G Zero-Trust Multi-Domain Networks, 2023. arXiv:2310.01675. Skills Programming: Python, C++, R, SQL, LaTeX Data/Web: MySQL, HTML, CSS, JavaScript Platforms/Tools: PyTorch, SUMO, MATLAB Awards 2023 — Dante Youla Award for Graduate Research Excellence in Electrical Engineering 2022 — Best Paper Award (GameSec 2022): On Poisoned Wardrop Equilibrium in Congestion Games 2020 — Merit Award (NYU ECE) Invited Sessions INFORMS 2021 — Efficient Episodic Learning of Nonstationary and Unknown Zero-Sum Games Using Expert Game Ensembles ACC 2023 — On the Resilience of Traffic Networks Under Non-Equilibrium Learning ","date":"0001-01-01T00:00:00Z","permalink":"https://unionpan.github.io/statement/","section":"","tags":null,"title":""}]