[{"categories":["I haven't even tried Hugging Face a lot"],"contents":"I would love to think of the ICML 2023 conference in Honolulu as a pivotal event for my P.h.D. research. During that time I was kind of talking to Reagan and having the time of my life. I remember in the conference there was a workshop session named sampling over discrete space. It felt serendipitous, like the universe was pointing me toward somewhere significant, much like how my brilliant colleague Tao pointing to the bright future here haha\nThe session ultimately was about how to use the massive structural discrete data, which has been major interests of many companies such as Google. The reason behind this was the killer success story of ChatGPT, which endorsed a bunch of subroutines such as SFT and RLHF (reinforcement learning with human feedback) Back to school I told my advisor I wanted to write a paper about leveraging preference user data to improve user satisfaction and how significant the theoretical contribution could be. His immediate reaction was \u0026lsquo;\u0026lsquo;you do not have the data, and those companies will do astronomically better than you\u0026rsquo;\u0026rsquo;, no shit bro.\nRLHF fundamentals Why Alignment? Alignment is all about ensuring that LLMs behave the way you want. Think of Asimov’s Three Laws of Robotics, only instead of robots, we’re taming text generators. Sometimes you want the model to obey. Other times, to be creative. And often, to just not hallucinate.\nWhat is RLHF? RLHF happens in three acts:\nSupervised Fine-Tuning (SFT):\nTrain the model on quality prompt–response pairs ${ (x^{(i)}, y^{*(i)}) }_{i=1}^N $. Maximize:\n$$ \\log p_\\theta( y_t^{*} | y_{ \u0026lt;t }^{*}, x ) $$\nReward Model Training:\nBuild a reward model $r_\\phi(x, y) $ that learns what humans prefer by ranking outputs. You use pairwise data like:\n$$ D_{\\text{pref}} = {(x, y_A, y_B, \\text{pref}_{AB})} $$\nPolicy Optimization (usually PPO):\nUse RL (e.g., Proximal Policy Optimization) to update the model:\n$$ \\max_{\\pi_\\theta} \\mathbb{E}[r_\\phi(x, y)] - \\lambda D_{\\text{KL}}(\\pi_\\theta | \\pi_{\\text{ref}}) $$\nBasically, before you use RL as an End-to-End method to obtain a good generation policy, you want to prepare a reward model for it, from all the discrete preference data, you got two options oftentimes:\nBradley–Terry (BT) Model: This models binary preference:\n$$ \\Pr(y_A \\succ y_B | x) = \\frac{\\exp(r_\\phi(x, y_A))}{\\exp(r_\\phi(x, y_A)) + \\exp(r_\\phi(x, y_B))} $$\nPlackett–Luce (PL) Model: This generalizes to full rankings.\nBut here’s the thing—reward models can be wrong. And LLMs are great at gaming the reward (a.k.a. reward hacking).\nReward Model Misalignment:\nYour $ r_\\phi $ may not match your actual preferences.\nCovariate Shift:\nThe policy drifts, and $r_\\phi$ hasn’t seen those new outputs.\nSample Inefficiency:\nPPO takes forever and a lot of annotations.\nYou can skip the reward model. Go straight for the preferences. DPO directly optimizes: $$ L_{\\text{DPO}}(\\theta) = - \\sum_{(x, y^+, y^-)} \\log \\sigma\\left(\\beta \\left[ \\log \\pi_\\theta(y^+|x) - \\log \\pi_\\theta(y^-|x) \\right] \\right) $$\nWhere $\\sigma(z) = \\frac{1}{1 + e^{-z}} $. DPO is not RL, not at all, but you may interpret $ \\log \\pi_\\theta(y | x) $ as a latent reward:\n$$ r_\\theta(x, y) := \\log \\pi_\\theta(y | x), $$ and that is where people started to use title \u0026ldquo;your xxx is a latent xxx\u0026rdquo; for some academic bullshit. Yeah, my PhD life is actually a huge waste of my life but I had no better things to do with it anyways.\nNash Learning from Human Feedback (NLHF) Apparently having a preference moodel you also go game-theoretic. In NLHF, we don’t just want a good policy—we want a Nash equilibrium in the space of preferences. This Nash equilibrium defines a strategy that cannot be beaten by an \u0026lsquo;\u0026lsquo;alternative self\u0026rsquo;\u0026rsquo;.\nWe define:\nA preference model $P(y \\succ y\u0026rsquo; | x) \\in [0, 1] $ Policy preference:\n$$ P(\\pi \\succ \\pi\u0026rsquo;) = \\mathbb{E}_{x,y,y\u0026rsquo;}[P(y \\succ y\u0026rsquo; | x)] $$ Goal: find $ \\pi^* $ such that:\n$$ P(\\pi^* \\succ \\pi\u0026rsquo;) \\geq \\frac{1}{2}, \\quad \\forall \\pi\u0026rsquo; $$ This can be solved with Mirror Descent or Policy Gradients: $$ \\pi_{n+1} = \\arg\\max_\\pi \\langle \\nabla J(\\pi_n, \\pi_n\u0026rsquo;), \\pi - \\pi_n \\rangle - \\frac{1}{\\eta} D_{\\text{KL}}(\\pi | \\pi_n). $$ Games like this is notoriously solvable via No-Regret methods, so you can come up with all kind of statistical bounds that ultimately boilds down to $\\mathcal{O}(\\sqrt{T})$ (or maybe a little different), or play with the reward/training structure a little bit more to come up with more fancy algorithms, and test them on some Hugging Face datasets.\nEvaluations on tasks like TL;DR summarization show that:\nNLHF beats RLHF in win-rate by ~5% Using direct preference models improves ranking accuracy: Gemma-2B: 74.2% → 80.7% LLaMA3-8B: 87.8% → 94.8% And… maybe it converges with fewer samples.\nWe are going to come back to this with a discussion of whether RLHF is actually useful. Now it\u0026rsquo;s the fall of 2025 and I already find myself so naive to wanted to do LLM alignment by myself, the resource and the effort put into this would have overwhelmed me, no doubt. A bigger question is, out of so many papers sprang in the last few years, what values can individual researchers actually create?\n","date":"2025-08-25T00:00:00Z","permalink":"https://yunianpan.com/post/2025/rlhf/","section":"post","tags":null,"title":"Reinforcement Learning with Human Feedback"},{"categories":null,"contents":"","date":"2025-08-21T00:00:00Z","permalink":"https://yunianpan.com/photos/","section":"","tags":null,"title":"Photos"},{"categories":["I am back"],"contents":"This past year I suffered a lot mentally and financially, my funding got paused and my girlfriend Sam (check out her spotify!) lost her boojee A.P.C. job. So I haven\u0026rsquo;t posted for a while. It\u0026rsquo;s really a leisure and effort to write stuff consistently (because wtf am I supposed to write?!!?). But the coding agents (Claude, Gemini etc..) really revolutionarized my efficiency, and shamelessly I have to admit that I use them a lot to \u0026ldquo;vibe code\u0026rdquo; and even do research, with sufficient human surveillance. So hopefully, I will resume writing for my own sanity.\nSo I am a huge hypocrite\u0026ndash;throughout the years I\u0026rsquo;ve witnessed tons of people rebranding existing/trivial phenomenon/methodologies as \u0026lsquo;\u0026rsquo;novel frameworks\u0026rsquo;\u0026rsquo;. I mean, aren\u0026rsquo;t we all? Even Shannon was just applying probability theory, except for it actually opened the pandora box. To some degree I believe being completely innovative is inherently impossible as you\u0026rsquo;ve gotta find some shoulders to stand on. It gets harder when you\u0026rsquo;re under constant pressure to publish, get grants, and do rebuttal with reviewers who might be having a bad day.\nSometimes I wonder where the objective bar, for what counts as \u0026ldquo;innovative enough\u0026rdquo;, actually is. The sheer volume of academic papers flood into the system, how much values do they actually create? Overworked reviewers delegate to grad students or (God help us) undergrads, are they able to catch when the emperor has no clothes?\n\u0026ldquo;Holonic Risk\u0026rdquo; This is the paper my advisor has been writing. He gradually accustomed himself with ChatGPT that it just infiltrated to his everyday editing of stuff (the Overleaf history and code style shows it all,) which I can\u0026rsquo;t say if it\u0026rsquo;s good or bad. The core idea is that he envisioned this networked chunks of players that interact with each other through a set of specifically structured utility functions. A \u0026ldquo;holons\u0026rdquo; is a \u0026ldquo;chunk of players\u0026rdquo;, and the Nash equilibrium, as you may upgrade it into Bayesian, is a \u0026ldquo;Holonic equilibrium\u0026rdquo;.\nHere\u0026rsquo;s a global game1 example \u0026ldquo;holonic Bayesian game\u0026rdquo;:\nPlayers: Set of voters $N = \\bigcup_{i \\in I} N_i$ where $I$ indexes committees (holons) and $N_i$ are voters in committee $i$.\nTypes: Each voter $k \\in N_i$ has private type $\\xi_k^i \\in \\{0,1\\}$ drawn from Bernoulli$(p)$, representing their preference strength.\nActions: Binary voting decision $x_k^i \\in \\{0,1\\}$.\nInformation Structure: Each voter observes only their own type $\\xi_k^i$ but knows the type Bernoulli distribution $p$ is common across all voters.\nPayoff Structure: For voter $k$ in committee $i$, the utility function is: $$ U_k^i(x_k^i, x_{-k}^i, \\omega_{-i}; \\xi_k^i) = \\mathbf{1}_{{ \\omega^i = 0}}(c_0 + c_1 x_k^i) + \\mathbf{1}_{ { \\omega^i = 1 } } c_2 \\cdot \\mathbf{1}_{ { x_k^i \\neq \\xi_k^i }} $$\nwhere $\\omega^i \\in {0,1}$ is the committee outcome and $\\omega_{-i}$ represents other committees\u0026rsquo; outcomes.\nThe \u0026ldquo;holonic\u0026rdquo; aspect reduces to this coupling mechanism:\n$$ \\omega^i = \\mathbf{1}_{\\left\\{ \\sum_{k \\in N_i} x_k^i \\geq \\theta + \\gamma \\sum_{j \\neq i} \\omega^{j}\\right\\}}$$\nSo committee $i$ succeeds if its vote total exceeds a threshold that increases linearly with the number of failed external committees. The parameter $\\gamma \\geq 0$ controls coupling strength.\nTo be fair this cannot be counted as completely a global game because it has atomic players. But the common feature is that you can sort of leverage the global symmetry to analyze the Nash equilibrium.\nBayesian Equilibrium Analysis In a Bayesian Nash equilibrium, each voter chooses a strategy $\\sigma_k^i: \\{0,1} \\rightarrow \\{0,1\\}$ that maximizes expected utility given their beliefs about others\u0026rsquo; strategies and external outcomes.\nEquilibrium condition: For truth-telling to be a Bayesian Nash equilibrium, we need basically: $$ \\mathbb{E}[U_k^i(\\sigma^i_k(\\xi_k^i), \\sigma_{-k}^i(\\xi_{-k}^i), \\omega_{-i}; \\xi_k^i)] \\geq \\mathbb{E}[U_k^i(1- \\sigma^i_k(\\xi_k^i), \\sigma_{-k}^i(\\xi_{-k}^i), \\omega_{-i}; \\xi_k^i)] $$ for all $k, i$ and both values of $\\xi_k^i$. So, you have $|\\mathcal{I}|$ times $n$ set of equations for three types of strategies:\nTruth-telling strategy: $\\sigma_k^i(\\xi_k^i) = \\xi_k^i$ (vote your type).\nTruth-reversing strategy: $\\sigma_k^i(\\xi_k^i) = 1 - \\xi_k^i$ (vote against your type).\nPooling strategies: $\\sigma^i_k(\\xi_k^i) = 1$ or $\\sigma^i_k(\\xi_k^i) = 0$ (vote constantly).\nFor truth-telling strategy, the equilibrium condition reduces to the incentive compatibility constraint: $$\\mathbb{P}(\\omega^i = 1) \\geq \\frac{c_1}{c_1 + c_2}.$$ That is to say, under the strategy, I calculate the success probability within one holon $i$, it has to be above certain threshold for this strategy to be ``okay\u0026rsquo;\u0026rsquo; as an equilibrium, but to calculate the success rate under a type prior, you have to consider other holons as well, this eventaully becomes a tangled \u0026lsquo;\u0026lsquo;chicken-egg\u0026rsquo;\u0026rsquo; problem.\nThe Fixed-Point Problem Most \u0026lsquo;\u0026lsquo;chicken-egg\u0026rsquo;\u0026rsquo; problem can be solved by fixed-point equations. Under symmetric truth-telling strategies, all committees have the same success probability $q$. The number of external failures follows $\\text{Binomial}(|I|-1, 1-q)$, leading to the fixed-point equation:\n$$q = \\sum_{z=0}^{|I|-1} \\binom{|I|-1}{z} (1-q)^z q^{|I|-1-z} \\cdot \\mathbb{P}\\left(\\text{Binomial}(n,p) \\geq \\theta + \\gamma z\\right)$$\nFor large $n \\to \\infty$, using the normal approximation: $$q = \\sum_{z=0}^{|I|-1} \\binom{|I|-1}{z} (1-q)^z q^{|I|-1-z} \\cdot \\Phi\\left(\\frac{np - \\theta - \\gamma z}{\\sqrt{np(1-p)}}\\right)$$\nIn the limit $|I| \\rightarrow \\infty$ with $\\delta = \\gamma |I|$ fixed, this becomes: $$q = \\Phi\\left(\\frac{np - \\theta - \\delta(1-q)}{\\sqrt{np(1-p)}}\\right)$$\nExistence and Uniqueness of Equilibria Since there is no other way around than enumerating if every possible strategy satisfies the incentive compatibility, the key mathematical questions become:\nExistence: When does a solution $q^* \\in [0,1]$ exist? Uniqueness: When is the solution unique? Stability: When are equilibria robust to perturbations? Uniqueness condition: The mapping $T(q) = \\Phi\\left(\\frac{np - \\theta - \\delta(1-q)}{\\sqrt{np(1-p)}}\\right)$ is a contraction when: $$\\frac{\\delta \\phi\\left(\\frac{np - \\theta - \\delta(1-q)}{\\sqrt{np(1-p)}}\\right)}{\\sqrt{np(1-p)}} \u0026lt; 1$$\nwhere $\\phi$ is the standard normal density. This is guaranteed when $\\delta$ is sufficiently small relative to $\\sqrt{np(1-p)}$.\nMultiple equilibria: For large $\\delta$, the system can exhibit bistability with equilibria near $q = 0$ and $q = 1$, separated by an unstable interior equilibrium. So the interpretation is, the more coupling between holons, the more chaotic it becomes. Below is kind of a relationship between $q$ and the prior $p$, the theoretical plot got twisted because at some of the $p$ the fixed point equations can\u0026rsquo;t be solved.\nWith a simple set up like this it\u0026rsquo;s already not very computationally convenient for seeking the equilibrium, so you can imagine how hard it is for general Bayesian Nash equilibrium.\nThe motivation of coming up with a system like this is still something I couldn\u0026rsquo;t figure out, because what do I know. But we should call it what it is. LOL.\nMorris, Stephen, and Hyun Song Shin. \u0026ldquo;Global games: Theory and applications.\u0026rdquo; (2001).\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"2025-07-01T00:00:00Z","permalink":"https://yunianpan.com/post/2025/mechanism_design/","section":"post","tags":null,"title":"Bayesian Holonic (Global) Games"},{"categories":null,"contents":"When you write an About Me, you want something permanent. Well, I go by \u0026lsquo;Union\u0026rsquo; \u0026mdash; a homophone of my Chinese name that I find neat and hope will stick. What won\u0026rsquo;t last much longer is that I\u0026rsquo;m finishing my PhD at NYU Tandon, and here goes the elevator pitch for my research:\nI develop algorithms that optimize the transient processes of multi-agent decision-making in complex systems, leveraging game theory and machine learning across domains like communication, transportation, and finance.\nYou might think to yourself \u0026ldquo;sounds like some pedantic academic buzzword bingo\u0026rdquo; and you\u0026rsquo;d be right. Well then, maybe:\nI designed a decentralized multi-radar communication algorithm that mitigates the mutual interference during target detection.\nI can assure you no one has ever been stupid enough to have done this.\nAnyways, the most permenant thing may be my upbringing. I was born in Taoyuan, Hunan province of China during the 90s. Like many of my peers, I grew up with some generational gap. My parents were both middle school teachers during that time, before their shifting to politics. It seemed to be the only path that\u0026rsquo;s ultimately justified: （修身，齐家，治国，平天下 god I hate these words with my guts.）\nPerhaps this is the epitome of deep-seated Ruism in East Asian culture, where individual values are measured by their bureaucratic or political success. The irony is they barely advanced down this path apparently, despite being incredibly hard-working people. In many ways, though, I guess they are more like pilgrims than I could ever be.\nI have no idea when my parents came to terms with my ordinariness. But as of now, I suppose we all acknowledge that regardless of the direction life pulls us, there is some quiet grace in simply \u0026ldquo;keep doing something\u0026rdquo;. Que sera sera.\n","date":"2024-08-26T20:18:54+03:00","permalink":"https://yunianpan.com/post/about_me/about/","section":"post","tags":null,"title":"About Me"},{"categories":["Thomas Sargent told me to avoid using this word"],"contents":"It certainly bothered me (not any more) to hear people casually dropping the word \u0026ldquo;sufficient statistics\u0026rdquo; in their talks, often describing some of their \u0026lsquo;\u0026lsquo;observations\u0026rsquo;\u0026rsquo; or \u0026lsquo;\u0026lsquo;samples\u0026rsquo;\u0026rsquo; that they feel like are \u0026lsquo;\u0026lsquo;sufficient\u0026rsquo;\u0026rsquo; for them to make certain decisions. The thing is, the word \u0026lsquo;\u0026lsquo;sufficient\u0026rsquo;\u0026rsquo; comes with a very rigorous definition, that may not even hold in those frameworks. For example, a lot of traffic control algorithms use measured vehicle streamed data to learn some embeddings as the input, the downstream tasks could be variable speed limit for certain highway segments for example, and people might call the embeddings \u0026lsquo;\u0026lsquo;sufficient statistics\u0026rsquo;\u0026rsquo;. Umm, \u0026hellip; truth is I don\u0026rsquo;t even where to start with this.\nAnyhow, in case you are wondering, let me give you a pedantic-as-hell sufficient statistic definition. Given a probability space $(\\Omega, \\mathbb{P}, \\mathcal{F})$, where the parameter space $\\Omega$ that labels all the possible statistical models ${P_\\theta}$, we call a function, often denoted as $T$, that maps from data $X \\sim P_\\theta(\\cdot) \\in \\Delta (\\mathcal{X})$ to some \u0026lsquo;\u0026lsquo;coonclusion\u0026rsquo;\u0026rsquo; a statistic. E.g., from the Boston housing prices data we get the average price, and that is a statistic, we can also calculate the emprical variance, now that\u0026rsquo;s also a statistic. To be more general, we restrict the data to be in a measurable space $(\\mathcal{X}, \\mathcal{B})$ and the statistical outcome to be in a measurable space $(\\mathcal{T}, \\mathcal{C})$.\nDefinition 1 Let $(\\mathcal{T} , \\mathcal{C})$ be a measurable space such that the $\\sigma$-field $\\mathcal{C}$ contains all singletons. A measurable mapping $T : \\mathcal{X} \\to \\mathcal{T}$ is called a statistic.\nUsually we can think of $\\mathcal{X}/\\mathcal{T}$ as a subspace of $\\mathbb{R}^d$ and $\\mathcal{B}/\\mathcal{C}$ its borel-algebra. Consider the distribution $P_\\Theta$ densities $f_\\Theta$ w.r.t. a measure $\\nu$, so does the distribution of $T = T(X)$, the idea is that $T$ should say all the things about the $\\Theta$-data-generating process: with $t = T(x)$, the conditional probability $$ f_{X|T, \\Theta} (x|t, \\theta) = \\frac{f_{X, T | \\Theta} (x, t | \\theta)}{f_{T| \\Theta} ( t|\\theta)} = \\frac{f_{X | \\Theta} (x | \\theta)}{f_{T| \\Theta} ( t|\\theta)} $$ remains the same for all the $\\Theta = \\theta \\in \\Omega$. In plain words, no matter what the statistical model $P_\\Theta$ is, knowing the likelihood of the data generated ($f_{X | \\Theta} (x | \\theta)$) is equivalent to knowing the likelihood of the statistics calculated $f_{T| \\Theta} ( t|\\theta)$, in which case we don\u0026rsquo;t even care what the data looks like, since the statistics $t$ are sufficient. Simple, right? To say some quantities are sufficient statistics, one does not need to give a rigorous definition like the following, but at least discuss the ratio of the conditional probability above. Because in some cases, it\u0026rsquo;s simply not true.\nDefinition 2. Suppose there exist versions of conditional distributions $\\mu_{X|\\Theta,T} (· | \\theta, t)$ and a function $r : \\mathcal{B} × \\mathcal{T} \\to [0, 1]$ such that\n$r(\\cdot, t)$ is a probability on $\\mathcal{B}$ for each $t \\in \\mathcal{T} $, $r(B, \\cdot)$ is measurable for each $B \\in \\mathcal{B}$, and for each $\\theta \\in \\Omega$ and $B \\in \\mathcal{B}$. $\\mu_{X|\\Theta,T} (B | θ, t) = r(B, t)$, for $\\mu_{T | \\Theta}(\\cdot | \\theta) − a.e. \\ \\ t.$ Then $T$ is called a sufficient statistic for $\\Theta$ (in the classical sense).\nNotice that we haven\u0026rsquo;t really discussed whether our setting is Frequentist or Bayesian yet, in the sense that we don\u0026rsquo;t really know if there is a prior measure on $\\Omega$. But this definition is considered Frequentist version by default.\nNow let\u0026rsquo;s look at the Bayesian setting, where we have a prior $\\mu_\\Theta(\\cdot) \\in \\Delta(\\Omega)$.\nDefinition 3. A statistic $T$ is called a sufficient statistic for the parameter $\\Theta$ (in the Bayesian sense) if, for every prior $\\mu_\\Theta$, there exists versions of posterior distributions $\\mu_{\\Theta|X}$ and $\\mu_{\\Theta|T}$ such that, for every $A \\in \\mathcal{F}$, we have $$ \\mu_{\\Theta|X} (A | x) = \\mu_{\\Theta|T} (A|T(x)) \\quad\\quad \\mu_X-a.s. $$ where $\\mu_X$ is the marginal distribution of $X$.\nWhen there are densities, the equality looks like this $$ \\begin{aligned} \\mu_{\\Theta \\mid X}(A \\mid x) \u0026amp; =\\int_{A} f_{\\Theta \\mid X}(\\theta \\mid x) \\mu_{\\Theta}(d \\theta) , \\\\ \\mu_{\\Theta \\mid T}(A \\mid t) \u0026amp; =\\int_{A} f_{\\Theta \\mid T}(\\theta \\mid t) \\mu_{\\Theta}(d \\theta) . \\end{aligned} $$ Therefore, for any element $x$ in the support of $\\mu_X$, it must holds for $ f_{\\Theta \\mid X}(\\theta \\mid x) = f_{\\Theta \\mid T}(\\theta \\mid t) $, which collapses to the condition in the Frequentist setting.\nNow, suppose someone gives you a parameterized family of statistical models ${P_\\theta : \\theta \\in \\Omega}$, which are all densities w.r.t some measure $\\nu$, how do we find a sufficient statistic? Or, how do you check if a statistic is sufficient? The following theorem will help.\nFactorization Theorem $T$ is a sufficient statistic if.f. there exist $h$ and $g$, such that $$ f_{X|\\Theta} (x| \\theta ) = h(x) g(\\theta , T(x)) $$\nProof Sufficiency: $$ \\begin{aligned} \\frac{d \\mu_{\\Theta \\mid X}}{d \\mu_{\\Theta}}(\\theta \\mid x) \u0026amp; =\\frac{f_{X \\mid \\Theta}(x \\mid \\theta)}{\\int_{\\Omega} f_{X \\mid \\Theta}(x \\mid \\theta) \\mu_{\\Theta}(d \\theta)} \\\\ \u0026amp; =\\frac{h(x) g(\\theta, T(x))}{\\int_{\\Omega} h(x) g(\\theta, T(x)) \\mu_{\\Theta}(d \\theta)} \\\\ \u0026amp; =\\frac{g(\\theta, T(x))}{\\int_{\\Omega} g(\\theta, T(x)) \\mu_{\\Theta}(d \\theta)}\\end{aligned} $$ hence it is a function of $T$;\nNecessity: $$ f_{X | \\Theta}(x | \\theta)= f_{\\Theta \\mid X}(\\theta \\mid x) f_{X}(x)= \\underbrace{f_{X}(x)}_{h(x)} \\underbrace{f_{\\Theta \\mid T}(\\theta | T(x))}_{g(\\theta, T(x))} . $$ $\\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad\\quad \\quad \\quad \\quad \\quad \\quad \\quad\\quad \\quad \\quad \\quad \\quad \\quad \\quad\\quad \\quad \\quad \\quad \\quad \\quad \\square $\nA sufficient statistic always exists because we can compute the exponential family, i.e., if we put $T(x) = (t_i(x))_{i=1}^d$, we can always put $$ f_{X \\mid \\Theta}(x \\mid \\theta) = \\underbrace{h(x)}_{h(x)} \\underbrace{c(\\theta) \\exp \\bigg( { \\sum_{i=1}^{d} \\theta_{i} t_{i}(x) \\bigg) }}_{g(\\theta, T(x))}. $$\nTo summarize, a statistic is a function, when we say it is sufficient we have to tell people what (statistical) model, and what parameters it is sufficient to, even though sometimes it is intuitive and does not need all the fuss. (Statistics is not pure math anyway.)\n","date":"2024-04-28T00:00:00Z","permalink":"https://yunianpan.com/post/2024/what_are_sufficient_statistics/","section":"post","tags":null,"title":"What (on earth) is a Sufficient Statistic?"},{"categories":["My life is in chaos"],"contents":"Chaotic behavior can emerge even in simple dynamics such as replicator, and Rock-Paper-Scissor oscillators, depending on the game settings. To begin with, we first define what is chaos qualitatively: \u0026ldquo;Chaos can be described as long term, aperiodic behaviour that exhibits sensitive dependence on initial conditions. Sensitive dependence on initial conditions implies that nearby trajectories diverge exponentially fast over time.\u0026rdquo; Or, by Edward Lorenz, when the present determines the future but the approximate present does not approximately determine the future.\nEdward Norton Lorenz (1918-2008) was an American mathematician and meteorologist who really laid the groundwork for how we understand weather and climate predictability today. He studied math at both Dartmouth and Harvard, and then took a break from his academic pursuits to serve as a weather forecaster for the Air Force during World War II. After the war wrapped up, he went to MIT, where he completed his doctoral degree in meteorology.\nIn the mid-1900s, the field of meteorology was still very much in its infancy. Lorenz programmed an existing computer, the Royal McBee, to aide in his research of atmosphere equations and forecasting. His program allowed him control the initial conditions of a weather system based on 12 differential equations. And the idea naturally came to him in 1961, when he was running a program with data rounded off from previous experiment, he found \u0026ldquo;chaos\u0026rdquo;: the fact that two weather conditions, which differ by less than $0.1%$, can produce significantly different results. So he wrote the paper \u0026ldquo;ordinary differential equations whose solutions afford the simplest example of deterministic non periodic flow and finite amplitude convection\u0026rdquo;, where he found that when applying Fourier series to one of Rayleigh\u0026rsquo;s convection equations, all except three variables tended to zero, which were used to construct a simple model based on the 2-dimensional representation of the earth\u0026rsquo;s atmosphere, the Lorenz Equation: $$ \\begin{aligned}\\frac{d x}{d t}=\\dot{x}=\\sigma(y-x) \\\\ \\frac{d y}{d t}=\\dot{y}=\\rho x-y-x z \\\\ \\frac{d z}{d t}=\\dot{z}=x y-\\beta z . \\end{aligned} $$\nHere, $x$ represents the convective overturning on the plane, $y$ and $z$ represent horizontal and vertical temperature variation respectively.\nThe parameters in the Lorenz system—sigma ($\\sigma$), rho ($\\rho$), and beta ($\\beta$)—have specific physical interpretations in the context of atmospheric convection, but they also have broader implications for the system\u0026rsquo;s behavior in mathematical and physical systems modeling. Here\u0026rsquo;s what each parameter represents:\n$\\sigma$ is the Prandtl number, which is a dimensionless number expressing the ratio of momentum diffusivity (viscosity) to thermal diffusivity. In the context of the Lorenz attractor, it represents the rate of heat transfer (convection) versus the rate of temperature change. A higher sigma increases the rate at which the system diverges along the $x$-axis relative to the $y$-axis, influencing the system\u0026rsquo;s tendency toward chaotic behavior. $\\rho$ is the Rayleigh number divided by its critical value for the onset of convection. It is a measure of the buoyancy-driven flow (thermal instability within the fluid), which is a result of temperature differences. In the Lorenz equations, it directly influences the nonlinearity of the system and is critical for the emergence of chaos. When $\\rho$ is larger than a certain threshold (in the standard Lorenz system, $\\rho$ \u0026gt; 24.74), the system exhibits chaotic behavior. $\\beta$ is a dimensionless parameter related to the geometry of the problem, specifically the aspect ratio of the convective cells. It can be thought of as influencing the vertical temperature profile within the system. Beta affects the dissipation rate of the vertical velocity component, influencing how the system\u0026rsquo;s trajectories contract towards the $z$-axis. Specific parameter settings will lead to chaos. By setting $\\dot{x}, \\dot{y},$ and $\\dot{z}$ to $0$ we get three equilibrium points: $$ \\begin{aligned} \u0026amp; k_0 = (0, 0, 0) \\\\ \u0026amp; k_1 = (-\\sqrt{\\beta ( \\rho - 1 )}, \\sqrt{\\beta ( \\rho - 1 )}, \\rho - 1 ) \\\\ \u0026amp; k_2 = (\\sqrt{\\beta ( \\rho - 1 )}, - \\sqrt{\\beta ( \\rho - 1 )}, \\rho - 1 ) \\end{aligned} $$\nLinearizing the ODE around these equilibrium points to check the stability we get: $$ \\left[\\begin{array}{c}\\dot{x} \\ \\dot{y} \\ \\dot{z}\\end{array}\\right]=\\left[\\begin{array}{ccc}-\\sigma \u0026amp; \\sigma \u0026amp; 0 \\ \\rho-\\bar{z} \u0026amp; -1 \u0026amp; \\bar{x} \\ \\bar{y} \u0026amp; \\bar{x} \u0026amp; -\\beta\\end{array}\\right]\\left[\\begin{array}{l}x \\ y \\ z\\end{array}\\right] $$\nif we take $(\\bar{x}, \\bar{y}, \\bar{z})$ to be $k_0$ we get eigenvalue equation: $$ \\lambda^{3}+(\\beta+\\sigma+1) \\lambda^{2}+(\\beta+\\beta \\sigma+\\sigma-\\rho \\sigma) \\lambda+\\beta \\sigma(1-\\rho)=0 $$ and $-\\beta$ is one solution, so we get $$ (\\lambda+\\beta)\\left(\\lambda^{2}+(\\sigma+1) \\lambda+\\sigma(1-\\rho)\\right)=0 $$ and the eigenvalues: $$ \\lambda_{1}, \\lambda_{2}=\\frac{-\\sigma-1 \\pm \\sqrt{(\\sigma+1)^{2}+4 \\sigma(\\rho-1)}}{2}, \\lambda_{3}=-\\beta . $$\nIf we take $k_1$ or $k_2$, we end up with eigenvalues satisfying: $$ \\mu^{3}+(\\beta+\\sigma+1) \\mu^{2}+(\\sigma+\\rho) \\beta \\mu+(1-\\rho) 2 \\sigma \\beta=0 $$ where all the three eigenvalues are negative when $$ \\rho\u0026lt;\\frac{\\sigma(\\sigma+\\beta+3)}{\\sigma-\\beta-1}=\\rho_{c} $$\n$\\rho_c \\approx 24.74$ when $\\sigma = 10$ and $\\beta = 8/3$.\nTherefore, at $\\rho \u0026gt; \\rho_c$, there are no fixed points. The flow will enter an invariance region around the origin where we see chaotic behavior. $$ \\begin{array}{|l|l|} \\hline \\rho \u0026amp; \\text { Fixed Points } \\\\ \\hline [0, 1] \u0026amp; (0,0,0) \\\\ \\hline (1,24.74) \u0026amp; k_{1}, k_{2} \\\\ \\hline [24.74-30.1) \u0026amp; \\text { None, chaos occurs } \\\\ \\hline [30.1-\\infty) \u0026amp; \\text { intermittency (not proven) } \\\\ \\hline\\end{array} $$\nThe Butterfly Effect Picking initial conditions $(0,1,0)$ $(1,0,1)$ we will get two distinct paths, shown in the following picture. It sort of resembles a butterfly, hence the chaotic effects in general are referred to as the Butterfly Effect. This draws upon Lorenz\u0026rsquo;s findings that two seemingly identical weather systems could produce two very different weather systems in the near future. Thus, a butterfly flapping its wings could alter the atmosphere ever so slightly, so as to deviate from the initial conditions, and accordingly alter the course of weather forever. Lorenz first used the example of a seagull\u0026rsquo;s wings, though the analogy has morphed into using a butterfly.\nThe buutterfly effect can be helpful to explain a lot of phenomenons in engineering, geography, and particularly stock market, where the linear financial models have failed many times. Just like Lorenz\u0026rsquo;s conclusion about weather conditions, the long run economic forecast is not feasible beyond a short time frame.\n","date":"2024-04-08T00:00:00Z","permalink":"https://yunianpan.com/post/2024/butterfly_effect/","section":"post","tags":null,"title":"The Lorenz dynamics and Butterfly Effect"},{"categories":["What the hell is a convex distance"],"contents":"This post is in celebration of Michel Talagrand winning Abel prize. Not to overly romanticize this but this is pretty much a come back story because back in the days of last century, “The type of mathematics I do was not fashionable at all when I started. It was considered inferior mathematics.” \u0026ndash;Michel Talagrand.\nNow we\u0026rsquo;ve seen significance of his contribution to the concentration of measure, suprema of stochastic processes and spin glass, all partially owing to his celebrated isoperimetry inequality in product probability space. What is an isoperimetry inequality? It is a concept in mathematics, particularly in the field of geometry and geometric analysis, that compares the length (or perimeter) of a closed curve to the area of the region it encloses, establishing that among all shapes with the same perimeter, the circle has the maximum area. For example: $$ 4 \\pi A \\leq L^2 $$ where the equality holds if.f. the curve is a circle. In a more general sense, the isoperimetric inequality relates the volume of an $n$-dimensional domain to the surface area of its boundary, with the sphere in $n$-dimensional space providing the optimal (maximum volume for a given surface area) ratio. Applying this concept to probability space, Talagrand was able to prove the following:\nTheorem For a product probability space $\\Omega = \\prod_{i=1}^n \\Omega_i$ endowed with a produdct measure and $A \\subseteq \\Omega$ $$ \\mathbb{P}[A] \\cdot \\mathbb{P}\\left[A_{t}^{c}\\right] \\leq e^{-t^{2} / 4} $$ for any $t \u0026gt; 0$, where $A^c_t = { w \\in \\Omega, d(A, \\omega) \\leq t }$ is an event defined by convex distance: $$ d(A, \\omega) = \\max_{\\alpha, |\\alpha|_2 \\leq 1} \\min_{y \\in A} \\sum_{ i: \\omega_i \\neq y_i} \\alpha_i. $$\nTo prove the inequality we need a little bit of preparation. Given set $A$, $x \\in \\Omega$: $\\mathcal{D}_{A}^{c}(x)=\\sup_{a \\in \\mathcal{R}_{+}^{n}} \\left( d_{a}(x, A)=\\inf_{y \\in A} d_{a}(x, y) \\right)$. Let $$ V_{A}(x) = \\text{ Convex-hull }\\left( U_{A}(x) \\right) = \\left\\{\\sum_{s \\in U_{A} (x) } \\alpha_{s} S: \\sum \\alpha_{s}=1, \\alpha_{s} \\geq 0 \\text{ for all } s \\in U_{A}(x) \\right\\}. $$ Thus, $$ x \\in A \\Leftrightarrow \\mathbf{1}(x \\neq x)=0 \\in U_{A}(x) \\Leftrightarrow 0 \\in V_{A}(x). $$\nLemma We have the following $$ \\mathcal{D}_{A}^{c}(x)=d \\left(0, V_{A}(x) \\right) \\equiv \\inf_{y \\in V_{A}(x)}|y| $$\nProof We proceed by proving (i) $\\mathcal{D}_{A}^{c}(x) \\leq \\inf_{y \\in V_{A}(x)}|y|$ and (ii) $\\mathcal{D}_{A}^{c}(x) \\geq \\inf_{y \\in V_{A}(x)}|y|$.\n(i): since $\\inf_{y \\in V_{A}(x)}|y|$ is achieved, let $Z$ be such that $|Z| = \\inf_{y \\in V_{A}(x)}|y| $. For any $a \\in \\mathbb{R}^n_+$ $|a| = 1$: $$ \\inf_{y \\in V_{A}(x)} a \\cdot y \\leq a \\cdot z \\leq|a||z|=|z|. $$ Since $ \\inf_{y \\in V_{A}(x)} a \\cdot y$ is linear programming, the minimum is achieved at an extreme point. That is, there exists $s \\in U_A(x)$ such that $$ \\inf_{y \\in V_{A}(x)} a \\cdot y=\\inf_{s \\in U_{A}(x)} a \\cdot s=\\inf_{y \\in A} d_{a}(x, y) \\text { for some } y \\in A . $$which is true for all $$ \\sup_{|a|=1, a \\in \\mathbb{R}_{+}^{n}} \\inf_{y \\in A} d_{a}(x, y) \\leq|z| \\equiv \\inf_{y \\in V_{A}(x)}|y|. $$\n(ii): Let $z$ be the one achieving minimum in $V_A(x)$. Then due to convexity of the objective (equivalently $|y|^2 = \\sum y^2_i = f(y)$) and of the domain, we have for any $ y \\in V_A (x), \\langle \\nabla f(z), y -z \\rangle \\geq 0$ for any $y \\in V_A(x)$. $\\nabla f(z) = \\nabla (z \\dot z) = 2 z$. Therefore the condition implies: $$ (y-z) z \\geq 0 \\Leftrightarrow y \\cdot z \\geq z \\cdot z=|z|^{2} \\Rightarrow y \\cdot \\frac{z}{|z|} \\geq|z|. $$\nThus, for $a = \\frac{z}{|z|} \\in \\mathbb{R}^n_+ $, $|a| = 1$, we have that $$ \\inf_{y \\in V_A(x)} a \\dot y \\geq |z| . $$ But for any given $a$, $\\inf_{y \\in V_A(x)} a \\cdot y = \\inf_{s \\in U_A (x)} a \\cdot s = d_a (x,A)$ as explained before. That is, $\\sup_{a: |a| = 1} d_a (x, A) \\geq |z| = \\inf_{ y \\in V_A(x)} |y|$.\n$\\quad\\quad\\quad \\quad\\quad\\quad \\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad \\quad\\quad\\quad \\quad\\quad\\quad \\quad\\quad\\quad \\quad\\quad\\quad \\quad\\quad\\quad \\square$\nProof of Isoperimetry inequality We prove by induction. Consider $n=1$, given $A$, $$ \\mathcal{D}_{A}^{c}(x)=\\sup_{a \\in \\mathbb{R}_{+}^{n},|a|=1} \\inf_{y \\in A} d_{a}(x, y)=\\inf_{y \\in A} \\mathbf{1}(x \\neq y)=\\left\\{ \\begin{array}{ll}0, \u0026amp; \\text{ for } x \\in A \\\\ 1, \u0026amp; \\text { for } x \\notin A \\end{array} \\right. $$\nThen $$ \\begin{aligned} \\int \\exp \\left (D^{2} / 4\\right ) d P \u0026amp; =\\int_{A} \\exp (0) d P+ \\int_{A^{c}} \\exp (1 / 4) d P \\\\ \u0026amp; =P(A)+e^{1 / 4}(1-P(A)) \\\\ \u0026amp; =e^{1 / 4}-\\left(e^{1 / 4}-1 \\right ) P(A) \\\\ \u0026amp; \\leq \\frac{1}{P(A)}. \\end{aligned} $$\nLet $f(x) = e^{1/4} - (e^{1/4} - 1) x$ and $g(x)= \\frac{1}{x}$. Because $f(x)$ is a decreasing function of $x$, $g(x)$ is a decreasing convex function, hence the result holds for $n=1$.\nNow let\u0026rsquo;s assume it holds for $n$, let $A \\subset \\Omega^{n+1}$, and $B$ be its projections on $\\Omega^n$. Let $A(\\omega)$ be section of $A$ along $\\omega$: if $x \\in \\Omega^n$, $\\omega \\in \\Omega$, then $z = (x, \\omega) \\in \\Omega^{n+1}$. The key observation is that:\nif $s \\in U_{A(\\omega)} (x)$, then $(s, 0) \\in U_{A}(z)$ if $t \\in U_{B}(x)$, then $(t, 1) \\in U_{A}(z)$. if $\\xi \\in V_{A(\\omega)}(x), \\zeta \\in V_{B}(x)$, and $\\theta \\in [0,1]$, then $((\\theta \\xi+(1-\\theta) \\zeta), 1-\\theta) \\in V_{A}(z)$. Recall $$ \\begin{aligned} \\mathcal{D}_{A}^{c}(z)^{2} \u0026amp; =\\inf_{y \\in V_{A}(z)}|y|^{2} \\leq(1-\\theta)^{2}+|\\theta \\xi+(1-\\theta) \\zeta|^{2} \\\\ \u0026amp; \\leq(1-\\theta)^{2}+\\theta|\\xi|^{2}+(1-\\theta)|\\zeta|^{2} \\\\ \u0026amp; \\leq(1-\\theta)^{2}+\\theta \\inf_{\\xi \\in V_{A(\\omega)}(x)}|\\xi|^{2}+(1-\\theta) \\inf_{\\zeta \\in V_{B}(x)}|\\zeta|^{2} \\\\ \u0026amp; =(1-\\theta)^{2}+\\theta \\mathcal{D}_{A(\\omega)}^{c}(x)^{2}+(1-\\theta) \\mathcal{D}_{B}^{c}(x)^{2} \\end{aligned} $$ By Holder\u0026rsquo;s inequality, and the induction hypothesis, $\\forall \\omega \\in \\Omega$, $$ \\begin{aligned} \u0026amp; \\int_{\\Omega^{n}} e^{\\mathcal{D}_{A}^{c}(x, \\omega)^{2} / 4} d P(x) \\\\ \u0026amp; \\leq \\int_{\\Omega^{n}} \\exp \\left(\\frac{(1-\\theta)^{2}+\\theta \\mathcal{D}_{A(\\omega)}^{c}(x)^{2}+(1-\\theta) \\mathcal{D}_{B}^{c}(x)}{4} \\right) d P(x) \\\\ \u0026amp; \\leq \\exp \\left ( \\frac{(1-\\theta)^{2}}{4} \\right ) \\int_{\\Omega^{n}} \\underbrace{\\exp \\left(\\frac{\\theta \\mathcal{D}_{A(\\omega)}^{c}(x)^{2}}{4}\\right)}_{X} \\underbrace{\\exp \\left(\\frac{(1-\\theta) \\mathcal{D}_{B}^{c}(x)^{2}}{4}\\right)}_{Y} d P(x) \\\\ \u0026amp; =\\exp \\left(\\frac{(1-\\theta)^{2}}{4}\\right) \\mathbb{E}[X \\cdot Y] \\\\ \u0026amp; \\leq \\exp \\left(\\frac{(1-\\theta)^{2}}{4}\\right) \\mathbb{E}\\left[X^{p}\\right]^{1 / p} \\mathbb{E}\\left[Y^{q}\\right]^{1 / q},\\left(\\text{ for } p=\\frac{1}{\\theta}, q=\\frac{1}{1-\\theta}: \\theta \\in[0,1]\\right) \\\\ \u0026amp; =\\exp \\left ( \\frac{(1-\\theta)^{2}}{4} \\right )\\left(\\int_{\\Omega^{n}} \\exp \\left(\\mathcal{D}_{A(\\omega)}^{c}(x)^{2} / 4\\right) d P(x)\\right)^{\\theta}\\left(\\int_{\\Omega^{n}} \\exp \\left(\\mathcal{D}_{B}^{c}(x)^{2} / 4\\right) d P(x)\\right)^{1-\\theta} \\\\ \u0026amp; \\leq \\exp \\left(\\frac{(1-\\theta)^{2}}{4}\\right)\\left(\\frac{1}{P(A(\\omega))}\\right)^{\\theta}\\left(\\frac{1}{P(B)}\\right)^{1-\\theta} \\text{ by induction hypothesis. } \\\\ \u0026amp; =\\exp \\left ( \\frac{(1-\\theta)^{2}}{4} \\right ) \\frac{1}{P(B)}\\left(\\frac{P(A(\\omega))}{P(B)}\\right)^{-\\theta} . \\end{aligned} $$\nNow we optimize $\\theta$, simply by construction: for any $ u \\in [0,1]$ $\\inf_{\\theta \\in[0,1]} \\exp \\left(\\frac{(1-\\theta)^{2}}{4}\\right) u^{-\\theta} \\leq 2-u$. Therefore, the R.H.S. $$ \\leq \\frac{1}{P(B)}\\left(2-\\frac{P(A(\\omega))}{P(B)}\\right) , $$ and thus $$ \\begin{aligned} \u0026amp; \\int_{\\Omega^{n+1}} \\exp \\left(\\frac{\\mathcal{D}_{A}^{c}(x, \\omega)^{2}}{4}\\right) d P(x) d \\mu(\\omega) \\\\ \u0026amp; \\leq \\frac{1}{\\mathbb{P}(B)} \\int_{\\Omega}\\left(2-\\frac{\\mathbb{P}(A(\\omega))}{\\mathbb{P}(B)}\\right) d \\mu(\\omega) \\\\ \u0026amp; \\leq \\frac{1}{\\mathbb{P}(B)}\\left(2-\\frac{(P \\bigotimes \\mu)(A)}{\\mathbb{P}(B)}\\right) \\\\ \u0026amp; \\leq \\frac{1}{(\\mathbb{P} \\bigotimes \\mu)(A)},(\\text{ since } u(2-u) \\leq 1 \\text{ for all } u \\in \\mathbb{R}) . \\end{aligned} $$\n$ \\quad \\quad \\quad\\quad \\quad \\quad\\quad \\quad \\quad\\quad \\quad \\quad\\quad \\quad \\quad\\quad \\quad \\quad\\quad \\quad \\quad\\quad \\quad \\quad \\quad \\quad \\quad\\quad \\quad \\quad\\quad \\quad \\quad\\quad \\quad \\quad\\quad \\square$\n","date":"2024-03-26T00:00:00Z","permalink":"https://yunianpan.com/post/2024/talagrand/","section":"post","tags":null,"title":"Talagrand's Isoperimetry inequality"},{"categories":["I procrastinated writing this paper"],"contents":"In this post I just want to share a simple and elegant idea from a Control System Letter paper written by Maxim Raginsky et al.1. This idea largely inspired my recent work on multi-agent learning in (monotone) games2, which I might devote another post to talk about if I happen to get some interesting results out of it.\nThe idea starts from here: we all know that the archetype of solving convex optimization problem is through gradient descent: $$ x_{k+1} = x_k - \\nabla f(x_k), $$ which, in continuous time, corresponds to an autonomous dynamical system: $$ \\dot{x} = - \\nabla f (x) , \\quad x(0) = x_0. $$ A natural question to ask is what are the hidden objectives being achieved along the gradient flow. This question was approached in a \u0026ldquo;inverse optimal control\u0026rdquo; fashion, i.e., given an autonomous dynamical system, identify the close-loop control and its corresponding optimal control problem. In this approach, the Fenchel-Young inequality played a crucial role to formulate the optimal control problem, as was extensively used in the variational principles introduced by Brezis and Ekeland3.\nFor any function \\(f\\) defined over primal space \\( \\mathcal{X}\\), denote its Fenchel conjugate as \\(f^* (y) = \\sup_{x \\in \\mathcal{X}} \\{ \\langle x,y \\rangle - f(x)\\} \\), we have the Fenchel coupling: $$ \\mathcal{FC}_{f} (x, y) = f(x ) + f^*(y) - \\langle x, y \\rangle \\geq 0 $$\nwith the equality holds if.f. $ y \\in \\partial f(x)$, or $x \\in \\partial f^* (y)$ if $f$ is convex.\nNow we can try constructing the optimal control problem, we have: $$ \\begin{align*} \\inf_{ u \\in \\mathcal{U} } J(x_0) \u0026amp; \\triangleq \\int_{t=0}^{\\infty} f(x(t)) + f^* (-u(t)) + \\langle u(t), \\bar{x} \\rangle \\mathrm{d}t \\\\ \\text{s.t. } \\quad \\quad \\quad \u0026amp; \\dot{x} = u , \\quad x(0) = x_0 \\end{align*} $$ where $\\bar{x} \\in \\arg\\min_{x\\in\\mathcal{X} }f(x) $ is an optimal solution. Intuitively, solving this optimal control problem should also lead to an optimal solution to the original problem $\\min_{x \\in \\mathcal{X}} f$. This can be actually verified by, let\u0026rsquo;s say, put $u^* (t)$ to be a close-loop control $-\\nabla f(x(t))$, we know it eventually leads to $\\bar{x}$, and by Fenchel-young inequality, $f(\\bar{x}(t)) + f^* (-u^* (t)) + \\langle u^* (t), \\bar{x} \\rangle = 0$, meaning that the stage cost of the optimal control problem also goes to $0$, we have enough reason to believe that $u^*(t)$ is actually the optimal control.\nThis result is nearly trivial as we just need to construct a Lyapunov function $V(x) = \\frac{1}{2} \\|x - \\bar{x}\\|^2$. We have, by Fenchel-Young inequality, \\begin{equation} \\dot{V} (x) + f(x) + f^* ( - u) + \\langle \\bar{x}, u \\rangle \\geq 0 , \\end{equation} which simply imply that $V(x)$ is actually a value function and the optimal control should be whatever makes the equality holds, i.e., the close-loop control $ - \\nabla f(x(t)) $. The benefit we can gain from this is the availablity of an entire analytical toolbag for differential equations, even stochastic differential equations if we consider the stochastic case.\nTzen, B., Raj, A., Raginsky, M. and Bach, F., 2023. Variational principles for mirror descent and mirror langevin dynamics. IEEE Control Systems Letters.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nPan, Y., Li, T. and Zhu, Q., 2024. On the Variational Interpretation of Mirror Play in Monotone Games. arXiv preprint arXiv:2403.15636.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nBrézis, H. and Ekeland, I., 1976. Un principe variationnel associéa certaines equations paraboliques. Le cas independant du temps. CR Acad. Sci. Paris Sér. AB, 282(17), pp.971-974.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"2024-02-11T00:00:00Z","permalink":"https://yunianpan.com/post/2024/variational_perspective/","section":"post","tags":null,"title":"A Variational Perspective On Gradient Descent"},{"categories":["I prefer A \u003e B \u003e C, you love B \u003e C \u003e A"],"contents":"The dueling bandit problem natrually fits the description of a variety of recommendation systems that require \u0026lsquo;\u0026rsquo;learning on the fly\u0026rsquo;\u0026rsquo;, yet have no explicit access to a \u0026lsquo;\u0026lsquo;reward\u0026rsquo;\u0026rsquo; model. Instead, the \u0026lsquo;\u0026lsquo;human\u0026rsquo;\u0026rsquo; feedback part takes the form of \u0026lsquo;\u0026lsquo;choices\u0026rsquo;\u0026rsquo;, \u0026lsquo;\u0026lsquo;votes\u0026rsquo;\u0026rsquo;, or some discrete forms. Hence, oftentimes a learning protocol proceeds at time steps \\(t = 1, \\ldots, T\\):\nThe algorithm chooses a pair of arms \\( a_i, a_j \\) from \\(K\\) available ones; The oracle/human feedback/nature reveals the winner arm \\( a_i \\), with probability \\( P(a_i \\succ a_j)\\), \\( P(a_i \\prec a_j) = 1 - P(a_i \\succ a_j) \\) So the feedback is either \\(a_i\\) or \\( a_j \\), the preferences forms a matrix \\(P \\in \\mathbb{R}^{K \\times K}\\) such that \\( P + P^{\\top} = I\\) which defines the hidden information of the dueling bandit problem. The cumulative regret in the stochastic dueling bandit setting is: $$ \\mathcal{R}_T = \\sum_{t=1}^T P( a^* \\succ a^t_i ) + P( a^* \\succ a^t_j ) $$ where \\(a^* \\) is usually the Condorcet winner, i.e., \\( P( a^* \\succ a_j) \u0026gt; \\frac{1}{2} \\ \\ \\forall j \\in [K], a_j \\neq a^*\\). Condorcet winner is a pretty straightforward idea, which might not exist in general cases. To see that, suppose there are three candidates: A, B, and C, and three voters with the following preferences:\nVoter 1: A \u0026gt; B \u0026gt; C Voter 2: B \u0026gt; C \u0026gt; A Voter 3: C \u0026gt; A \u0026gt; B Voter 1 Voter 2 Voter 3 A \u0026gt; B X X B \u0026gt; C X X C \u0026gt; A X X In this example, let\u0026rsquo;s check the pairwise comparisons:\nA vs. B: B is preferred by Voter 2, but A is preferred by Voter 1 and 3. B vs. C: B is preferred by Voter 1 and 2, but C is preferred by Voter 3. C vs. A: C is preferred by Voter 2 and 3, but A is preferred by Voter 1. Since no candidate consistently beats all others in pairwise comparisons, there is no Condorcet winner in this example. Now we transform this example to fit in the dueling bandits context, we can have, for instance, a cyclic relation \\(A \\succ B \\succ C \\succ A \\).\nA B C A 0.5 0.7 0.2 B 0.3 0.5 0.6 C 0.8 0.4 0.5 Apparently the existence of a Condorcet winner requires a row that is greater than \\(0\\). While in a lot of cases such a winner/solution concept might not be suitable, let\u0026rsquo;s first dive into the algorithmic design of trying to find it when it exists. There are basically two styles of algorithms, asymmetric or symmetric:\nThe asymmetric style conceptually separates two choices into choosing a reference arm and an exploration arm. The reference arm acts as a summary of historical pulls. Typical algorithms of this type includes IF, BtM, SAVAGE, Doubler, RUCB, MergeRUCB, RCS, and DTS. The exploration strategy is to maximize the efficiency of identifying the best arm.\nInterleaved Filter (IF) and Beat the Mean (BtM) The very first two methods proposed are Interleaved Filter (IF) 1 and Beat the Mean (BtM) 2. They all assume there is a total ordering of the arms, i.e., we can relabel the arms as \\(a_1, \\ldots, a_K\\), such that \\( p_{i,j} \u0026gt; 0.5 \\) for all \\( i \u0026lt; j\\). Under this assumption the Condorcet winner is \\(a_1\\).\nIF method basically, is a type of \u0026ldquo;hill climbing\u0026rdquo; method that iteratively updates the reference arm \\(\\hat{a}\\) by comparing it with other arms until it becomes the most possible Condorcet winner. The term \u0026ldquo;interleaved\u0026rdquo; is kind of originating from a kind of Netflix ranking acceleration technique, which uses a blend of ranker to recommend videos to the same group of users, then compare the share of viewing hours coming from different rankers. IF picks a reference arm \\(\\hat{a}\\) randomly and preserves a set of \u0026ldquo;Condorcet candidates\u0026rdquo; to compare with the reference arm, as well as a set of confidence intervals \\( \\hat{C}_t := [ P_{\\hat{a}, a} - c_t, P_{\\hat{a}, a} + c_t] \\), where \\(c_{t}=\\sqrt{\\log (1 / \\delta) / t}\\), then it iteratively compares all of them and gradually filters out dominated arms which are out of the confidence intervals, i.e., \\( P_{\\hat{a}, a} - c_t \u0026gt; \\frac{1}{2} \\) and also updates reference arms, i.e., \\(P_{\\hat{a}, a} + c_t \u0026lt; \\frac{1}{2}\\). The corner stone idea is to prove after logarithmic comparisons, the winner between any two pairs is identified as the winner \u0026ldquo;correctly\u0026rdquo; with probability \\(1 - \\delta \\). To see this, let\u0026rsquo;s say \\(n\\) is the number of comparisons, for \\(t \\in \\mathbb{N}_+\\), the event \\( \\mathcal{E}_t\\) is when \\( \\hat{P} - c_t \u0026lt; \\frac{1}{2} \\), which is the condition for the match to continue after \\(t\\) comparisons, therefore $$ Pr( n \\geq t) \\leq P( \\mathcal{E}_t ), $$\nthe confidence interval boundaries \\( p_{i,j} \\notin \\hat{C}_t \\) $$ \\begin{align*} Pr(\\mathcal{E}_t) \u0026amp; = Pr( \\hat{P}_t - p_{i,j} \\leq c_t - \\Delta_{i,j} ) \\\\ \u0026amp; = Pr( \\mathbb{E}[\\hat{P}_t] - \\hat{P}_t \u0026gt; \\Delta_{i,j} - c_t) \\\\ \u0026amp; \\leq Pr( |\\mathbb{E}[\\hat{P}_t] - \\hat{P}_t| \u0026gt; \\Delta_{i,j}/2 ) \\\\ \u0026amp;\\leq 2 \\exp \\left(-t \\epsilon_{i, j}^{2} / 2\\right) \\\\ \u0026amp;\\leq 2 \\exp \\left(-m \\log \\left(T K^{2}\\right)\\right) \\\\ \u0026amp; =2 /\\left(T K^{2}\\right)^{m} , \\end{align*} $$ taking \\( m = \\max{ 4, d }\\), we have \\( Pr( n \\geq t) \\leq K^{-d }\\) since we have \\( c_t \\leq \\Delta_{i,j}/2\\) when \\(t = \\left\\lceil m \\log \\left(T K^{2}\\right) / \\epsilon_{i, j}^{2}\\right\\rceil\\) and \\( m \u0026gt; 4\\).\nBtM leverages the concept of Borda score: $$ b(a_i) = \\frac{1}{K}\\sum_{j} p_{i, j} $$ and two facts:\nthe Condorcet winner cannot be a Borda loser, in the sense that the average Borda score must be greater than \\(0.5\\); the Condorcet winner stays if some other arms are \u0026ldquo;removed\u0026rdquo;. Therefore, as long as we keep eliminate Borda losers, nothing will be left except for the Condorcet winner.\nThe problem with IF is that the theoretical guarantee requires some sorts of Strong Stochastic Transitivity (SST) property: for any triple \\( (i, j, k)\\), \\( \\Delta_{i,k } \\geq \\max \\{ \\Delta_{1, j}, \\Delta_{1,k} \\}\\). BtM only requires a relaxed SST property: there exists \\(\\gamma \\geq 1\\) such that for all pairs \\( (j,k)\\) with \\( 1 \u0026lt; j \u0026lt; k\\), we have \\( \\gamma \\Delta_{1,k} \\geq \\max\\{ \\Delta_{1, j}, \\Delta_{1,k} \\}\\). \\(\\gamma\\) measures the hardness of the problem, as the smaller the gap becomes, the harder it is to identify the better arm while dueling.\nThe following regret bounds have been proven already, given \\(\\Delta_{\\min}\\) and \\( \\gamma\\), $$ \\begin{align*} \\mathbb{E}[ \\mathcal{R}_T^{IF} ] \u0026amp; \\leq \\mathcal{O}\\left( \\frac{K \\log T}{\\Delta_{\\min} }\\right), \\\\ \\mathcal{R}_T^{BtM} \u0026amp; \\leq \\mathcal{O}\\left( \\frac{\\gamma^7 K \\log T}{\\Delta_{\\min }}\\right) \\quad \\text{ with high probability}. \\end{align*} $$\nSensitivity Analysis of VAriables of Generic Exploration (SAVAGE) and Doubler SAVAGE works in a way similar to how BtM works: if we know there is a Condorcet winner, any arms that lose with high probability can be safely eliminated from further consideration. So we can compare arms in a round robin (all-to-all) fashion and drop the pairs of arms as long as it\u0026rsquo;s \u0026ldquo;safe\u0026rdquo; to do so. Its regret bound is of order \\( \\mathcal{O}( K^2 \\log T)\\), which is not tight, however, it empirically outperforms IF and BtM by a wide margin when the arm size is moderate.\nDoubler converts the dueling bandits into conventional multi-armed bandit problems, under the assumptions that the preferences are linear choice functions of underlying utilities associated with the arms. In other words, \\( \\Delta_{A,B} = (\\mu_A - \\mu_B)/2\\), \\(\\mu_A\\) is the mean utility of arm \\(A\\), for example. Doubler proceeds in epochs of exponentially increasing size, (called \u0026ldquo;doubling trick\u0026rdquo;). In each epoch, the left arm is sampled from a fixed distribution, the right arm is chosen from a ordinary bandit algorithm, minimizing the regret against the left arm.\nThere is also a bunch of algorithms based on UCB (Upper Confidence Bound) variants. The fundamental principle for UCB type of algorithms is one can always create a confidence interval for the interesetd statistic estimates depending on how the sampling procedure is going on. (People write tons of tons of paper about different procedures while essentially they are the variants for the same thing, and then there will be someone trying to unifying the different frameworks. Always interesting to keep this thread going on.)\nRMED and Sparring The Relative Minimum Empirical Divergence (RMED) algorithm has been proved to have optimal asymptotic regret. First of all, the authors from 3 constructed some nuanced lower bound example, (I browsed their paper, a little bit hard to comprehend\u0026hellip;) to claim that the lower bound for dueling bandits problem is characterized by $$ \\liminf_{T \\rightarrow \\infty} \\frac{\\mathbb{E}[R(T)]}{\\log T} \\geq \\sum_{i \\in[K] \\backslash{1}} \\min_{j \\in \\mathcal{O}_{i}} \\frac{\\Delta_{1, i}+\\Delta_{1, j}}{2 d\\left(\\mu_{i, j}, 1 / 2\\right)} $$ where the notation \\( d(\\mu_{i,j}, \\frac{1}{2})\\) is kind of characterzing how \u0026ldquo;easy\u0026rdquo; it is to tell which one is better, \\(i\\) or \\(j\\). The trick to match this lower bound is sort of like the plugin principle, you design an algorithm that directly uses the empirical divergence as a criteria to pick the Condorcet winner.\nThe Sparring series belongs to the symmetric algorithms, it is inspired by that the dueling bandits problem is just an example of a symmetric game. symmetric game is one where the payoffs for playing a particular strategy depend only on the other strategies employed, not on who is playing them. That is, if any two players were to switch their strategies with one another, their payoffs would switch as well, leaving the overall outcome of the game unchanged. This is intuitively understandable because you can always view the dueling recommendations as actions played by two players and the eventual goal is to come up with a \u0026ldquo;draw\u0026rdquo; that no one wants to deviate.\nTherefore, any no-regret dynamics would lead to the empirical convergence to the equilibrium of such a game, oftentimes the regret has the form of \\( \\mathcal{O}(\\sqrt{T}) \\), I\u0026rsquo;m going to write a post to discuss the adversarial bandit problems later to talk about my understandings. What was surprising is that empirical experiments show that adversarial algorithms seem to have a logaritmic regret rate despite the proven bound.\nIn general, given an algorithm \\( \\mathcal{A} \\) that solves the adversarial bandit problem, we can use it to sovle the dueling bandits problem by placing a row player and a column player, both sparring with each other by giving out his bet of best action, one gets reward \\(1\\) if he wins, otherwise \\(0\\). As the comparisons have been carried out, the player uses \\(\\mathcal{A}\\) to update their strategies. In this fashion, we are sort of obtaining a sampling estimate of the preference matrix, or at least getting some information of this preference matrix through sparring, it is just this sampling procedure is symmetric.\nA bit thoughts Dueling bandits, as a variant of the multi-armed bandit problem has its relevance in that the preference feedback nowadays are way more easier to obtain. It sort of gave rises to the concept of Reinforcement Learning with Human Feedback and I believe chatgpt has been significantly benefiting from that. I personally think the whole RLHF thing don\u0026rsquo;t really quite need a theoretical fundation unless it is really instructive. Entertainment-wise, I love the advancement of dueling bandits in that it gives a theoretical formalism for how we can deal with discrete type of data. We\u0026rsquo;ll see where it leads us to.\nYue, Y., Broder, J., Kleinberg, R., \u0026amp; Joachims, T. (2012). The k-armed dueling bandits problem. Journal of Computer and System Sciences, 78(5), 1538-1556.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nYue, Y., \u0026amp; Joachims, T. (2011). Beat the mean bandit. In Proceedings of the 28th international conference on machine learning (ICML-11) (pp. 241-248).\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nKomiyama, J., Honda, J., Kashima, H. and Nakagawa, H., 2015, June. Regret lower bound and optimal algorithm in dueling bandit problem. In Conference on learning theory (pp. 1141-1154). PMLR.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"2023-10-11T00:00:00Z","permalink":"https://yunianpan.com/post/2023/dueling_bandits/","section":"post","tags":null,"title":" Some Notes on Dueling Bandits"},{"categories":["Some random math"],"contents":"This post is dedicated to Erdos-Szekeres Theorem, which says:\nES Thm. (Monotone sequence) For any positive integer $n$, any sequence of $n^2 + 1$ distinct real numbers contains a monotone subsequence of length at least $n+1$.\nThis means that within any sufficiently long sequence, there is either an increasing subsequence or a decreasing subsequence of a certain minimum length.\nThere\u0026rsquo;s another version of the theorem statement coming from the 1935 paper by Paul Erdos and George Szekeres1, which concerns combinatorial geometry. It says:\nES Thm. (combinatorial geometry) For every positive integer $n$ among every $N=\\binom{2 n-4}{n-2}+1 \\sim 4^{n} / \\sqrt{n}$ points $p_1, \\ldots, p_N \\in \\mathbb{R}^2$, where $p_i = (x_i , y_i)$ and $x_1 \u0026lt; x_2 \u0026lt; \\ldots \u0026lt; x_N$, there is a convex configuration or a concave configuation of at least $n$ points.\nThis means that there are indices $i_1 \u0026lt; \\ldots \u0026lt; i_n$ such that the slopes of the segments $p_{i_j} p_{i_{j+1}}, j = 1, \\ldots, n-1$ are either all nondecreasing or all nonincreasing.\nThese two statements can be traced back to Ramsey\u0026rsquo;s theory.\nRamsey Thm. For every $k,r,n \\in \\mathbb{N}$ there exists a number $N \\in \\mathbb{N}$ such that for every coloring of the $k$-tuples of the $[N]$ by $r$ colors there is a subset $T \\subseteq [N]$ of size $n$ such that all $k$-tuples of elements of $T$ have the same color. The Ramsey number, $R^r_n (k)$ is the smallest such $N$.\nI might have lost you here already. The connections between these three theorems are pretty vague to me at first. But let me make an analogy to clarify that ES theorem actually makes a special case of Ramsey theorem.\nFirst, Ramsey\u0026rsquo;s theorem can be translated using graph theory language. Let\u0026rsquo;s focus on the case where we are coloring pairs of points, so $k=2$. The theorem then says that if you take a complete graph $K_N$ with a large enough number of vertices $N$, and you color every edge with one of $r$ colors, you are guaranteed to find a complete subgraph $K_n$ (a clique of size $n$) where all edges have the same color.\nLet\u0026rsquo;s see how this applies to the Monotone Sequence Theorem.\nImagine there is a graph that connects the elements of our sequence of $N = n^2+1$ distinct numbers, ${x_1, x_2, \\ldots, x_N}$.\nVertices: Let the vertices of a complete graph $K_N$ be the numbers $x_1, \\ldots, x_N$. Edges: The edges are all the pairs $(x_i, x_j)$ where $i \u0026lt; j$. Colors: We will use two colors ($r=2$), let\u0026rsquo;s call them \u0026ldquo;blue\u0026rdquo; and \u0026ldquo;red\u0026rdquo;. Color the edge $(x_i, x_j)$ blue if $x_i \u0026lt; x_j$ (the sequence is \u0026ldquo;increasing\u0026rdquo; at this step). Color the edge $(x_i, x_j)$ red if $x_i \u0026gt; x_j$ (the sequence is \u0026ldquo;decreasing\u0026rdquo; at this step). Since every pair of numbers is either increasing or decreasing, every edge in our complete graph $K_N$ is colored. Now, what does Ramsey\u0026rsquo;s Theorem promise us? It guarantees that there exists a monochromatic clique of a certain size. What is that in our context?\nA blue clique of size $n+1$ is a subset of vertices ${x_{i_1}, x_{i_2}, \\ldots, x_{i_{n+1}}}$ with $i_1 \u0026lt; i_2 \u0026lt; \\ldots \u0026lt; i_{n+1}$ such that every edge between them is blue. This means for any pair $j \u0026lt; k$, the edge $(x_{i_j}, x_{i_k})$ is blue, which implies $x_{i_j} \u0026lt; x_{i_k}$. This is precisely an increasing subsequence of length $n+1$.\nA red clique of size $n+1$ is a subset of vertices ${x_{i_1}, x_{i_2}, \\ldots, x_{i_{n+1}}}$ with $i_1 \u0026lt; i_2 \u0026lt; \\ldots \u0026lt; i_{n+1}$ such that every edge between them is red. This means for any pair $j \u0026lt; k$, the edge $(x_{i_j}, x_{i_k})$ is red, which implies $x_{i_j} \u0026gt; x_{i_k}$. This is precisely a decreasing subsequence of length $n+1$.\nRamsey\u0026rsquo;s Theorem for two colors, $R(n+1, n+1)$, gives us an upper bound on the sequence length $N$ needed to guarantee one of these structures. The Erdős-Szekeres Theorem provides a much tighter bound, $N = n^2 + 1$, but the underlying principle is the same: in a sufficiently large system, order must emerge.\nA Simple Proof While the connection to Ramsey Theory is fundamental, there is a wonderfully simple proof of the Monotone Sequence Theorem that feels almost like magic. It\u0026rsquo;s one of my all-time favorites.\nLet\u0026rsquo;s take our sequence $X = (x_1, x_2, \\ldots, x_{n^2+1})$. For each number $x_k$ in the sequence, let\u0026rsquo;s assign it a special label—a pair of integers $(a_k, b_k)$. We define them like this:\n$a_k$ = the length of the longest increasing subsequence that ends with $x_k$. $b_k$ = the length of the longest decreasing subsequence that ends with $x_k$. Now, here’s the crucial insight: every single one of these labels is unique.\nLet\u0026rsquo;s think about why this has to be true. Pick any two numbers from our sequence, say $x_i$ and $x_j$, where $x_i$ comes before $x_j$ (so $i \u0026lt; j$). Since all the numbers are distinct, there are only two possibilities:\nIf $x_i$ is smaller than $x_j$: We can take the longest increasing subsequence ending at $x_i$ (which we know has length $a_i$) and just tack $x_j$ onto the end. This gives us a new, longer increasing subsequence that ends at $x_j$. Its length is $a_i + 1$. This means the longest possible increasing subsequence ending at $x_j$ must be at least this long, so $a_j \\ge a_i + 1$. Right away, we know $a_i$ and $a_j$ can\u0026rsquo;t be the same.\nIf $x_i$ is greater than $x_j$: We can do the same trick with the decreasing subsequences. Take the longest decreasing subsequence ending at $x_i$ (length $b_i$) and add $x_j$ to it. This creates a decreasing subsequence ending at $x_j$ of length $b_i + 1$. So, we must have $b_j \\ge b_i + 1$, which means $b_i$ and $b_j$ can\u0026rsquo;t be the same.\nIn either scenario, the label $(a_i, b_i)$ is different from the label $(a_j, b_j)$. Since this applies to any pair of numbers in our sequence, it means all $n^2+1$ labels are unique.\nThis is where the argument comes together. Let\u0026rsquo;s assume, just for a moment, that the theorem is wrong and there is no monotone subsequence of length $n+1$. What would that mean for our labels?\nIt would mean that the length of any increasing subsequence is at most $n$, and the length of any decreasing subsequence is also at most $n$. This puts a very tight limit on the possible values in our labels: $$1 \\le a_k \\le n$$ $$1 \\le b_k \\le n$$\nSo, if our assumption is true, how many different labels could we possibly create? The first number, $a_k$, can be anything from 1 to $n$. The second number, $b_k$, can also be anything from 1 to $n$. This gives us a total of $n \\times n = n^2$ possible unique labels.\nBut wait. We have $n^2+1$ numbers in our sequence, and we just proved that every single one of them generates a completely unique label. This leads to a logical contradiction: we have $n^2+1$ unique items, but we only have $n^2$ available slots for them to fit into. That\u0026rsquo;s simply not possible.\nBy contradiction, there must be at least one monotone subsequence of length $n+1$ or greater, hence the proof.\nErdös, P. and Szekeres, G., 1935. A combinatorial problem in geometry. Compositio mathematica, 2, pp.463-470.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"2023-07-24T00:00:00Z","permalink":"https://yunianpan.com/post/2023/erdos-szekeres/","section":"post","tags":null,"title":"Erdos-Szekeres Theorem"},{"categories":["Economics is not science---Yuri Dvorkin"],"contents":"I remember having those unpleasant lines in our high school canteen, flooded by the starving students queeing for their lunch, I would always pick a window with fewer people waiting, compromising myself with awful food. Another similar thought that always stricked me was the odds that tourists always pick the same time to travel, there\u0026rsquo;s almost always traffic congestion everywhere during holiday seasons. Yeah, lives have been always so hard.\nOn the first level of thinking, when there\u0026rsquo;s a lack of resource you attempt to find alternatives, e.g., picking another time for traveling, another food window, etc.. The level two thinking is maybe \u0026lsquo;\u0026lsquo;since other people are avoiding holidays, what if I insist on going out on holidays\u0026rsquo;\u0026rsquo;? Or maybe there\u0026rsquo;s a twisted level three thinking, \u0026lsquo;\u0026lsquo;what if everybody thinks like the level two thinker, \u0026hellip;.\u0026rsquo;\u0026rsquo; It\u0026rsquo;s somewhat intimidating to follow this infinite hierachy, but definitely rewarding, as there are certainly moments when people are regretting their travel decisions, thinking to themselves \u0026lsquo;\u0026lsquo;I probably should not have gone the high way.\u0026rsquo;\u0026rsquo;\nThe aforementioned phenomenons significantly resembled the first idea proposed by Wardrop1 in 1952. Wardrop conducted some \u0026ldquo;before-and-after\u0026rdquo; analysis over some traffic data, and came into the two alternative criteria to determine the flow distribution on the routes,\nThe journey times on all the routes actually used are equal, and less than those which would be experienced by a single vehicle on any unused route; The average journey time is a minimum. It was not until Beckmann2, 1956 that the two simple yet powerful ideas were mathematically formulated, and was considered an example of Rosenthal games3. The first principle basically states that nobody should be happy to deviate from their own routes, which is essentially the behavior of Nash equilibrium.\nTo elaborate, we consider a transporation network represented by a directed, finite, and connected graph $ \\mathcal{G} = (\\mathcal{V}, \\mathcal{E})$, where \\(\\mathcal{V}\\) represent road junctions, the edges \\(\\mathcal{E}\\) represent road segments. The set of Origin-Destination (OD) pairs is \\(\\mathcal{W} \\subseteq \\mathcal{V} \\times \\mathcal{V}\\), indexed by \\(w\\), \\(|\\mathcal{W}| = :W\\). Let \\(\\mathcal{P}: = \\bigcup_{w\\in\\mathcal{W}}\\mathcal{P}_w\\) be the set of directed paths between OD pairs, each path set \\(\\mathcal{P}_w\\) is indexed by \\(w\\).\nThe individual vehicles traveling through \\(\\mathcal{G}\\) are infinitesimal players over \\(\\mathcal{G}\\), denoted by a measurable space \\((\\mathcal{X}, \\mathcal{M}, m)\\). The players are non-atomic, i.e., \\( m(x) = 0 \\ \\ \\forall x \\in \\mathcal{X}\\); they are split into distinct populations indexed by the OD pairs, i.e., \\(\\mathcal{X} = \\bigcup_{w\\in\\mathcal{W}} \\mathcal{X}_w\\) and \\(\\mathcal{X}_w \\bigcap\\) \\(\\mathcal{X}_w\u0026rsquo; = \\emptyset,\\ \\forall w, w^{\u0026rsquo;} \\in \\mathcal{W}\\). For each OD pair \\(w \\in \\mathcal{W}\\), let \\(m_w = m(\\mathcal{X}_w) \\) represent the traffic demand. For each player \\(x \\in \\mathcal{X}_w\\), we assume that their travel path \\(a \\in \\mathcal{P}_w\\) is fixed right after the path selection.\nThe action profile of all the players \\(\\mathcal{X}\\) induces an edge flow vector \\(q \\in \\mathbb{R}^{|\\mathcal{E}|}_+\\), where \\(q_e := \\int_{\\mathcal{X}} \\mathbb{I}_{{e \\in a}} m(dx), e \\in \\mathcal{E}\\), and a path flow vector \\(\\mu \\in \\Delta :=\\{(\\mu_p)_{p\\in \\cup_{w\\in \\mathcal{W}}\\mathcal{P}_w} | \\mu_p := {\\int_{\\mathcal{X}_w} \\mathbb{I}_{{a = p}} m(dx)}\\}\\). The edge-path incident matrix is \\(\\Lambda = [\\Lambda^1 \\vert , \\ldots, \\vert \\Lambda^{|\\mathcal{W}|}] \\in \\mathbb{R}^{|\\mathcal{E}| \\times |\\mathcal{P}|}\\), \\(\\Lambda^w_{e, p} = \\mathbb{I}_{{e\\in p}}, \\forall e\\in \\mathcal{E}, w \\in \\mathcal{W}, p \\in \\mathcal{P}_w\\). Easy to verify the compact form of edge-path flow relation is \\(q = \\Lambda \\mu\\).\nLet \\((\\Omega, \\mathcal{F}, \\mathbb{P})\\) be the probability space, \\(l_e: \\mathbb{R}_+ \\times \\Omega \\to \\mathbb{R}_+\\) be the cost/latency functions, measuring the travel delay of the edge \\(e \\in \\mathcal{E}\\) determined by its edge flow \\(q_e\\) and a state variable \\(\\omega \\in \\Omega\\) that is universal for the entire traffic network, e.g., \\(\\omega\\) can represent the weather condition, road incidents or anything that affects the congestion level. Let \\(l : \\mathbb{R}^{|\\mathcal{E}|}_{\\geq 0} \\times \\Omega \\mapsto \\mathbb{R}^{|\\mathcal{E}|}_+\\) denote the vector-valued latency function.For an instance \\(\\omega \\in \\Omega\\), the latency of path \\(p\\) is defined as \\(\\ell_p : = \\sum_{e \\in p} l_e (q_e, \\omega ) = \\Lambda^{\\top}_p l (\\Lambda \\mu, \\omega )\\), which can be seen as a function of \\(\\mu\\) and \\(\\omega\\), written as \\(\\ell_p = \\ell_p(\\mu, \\omega)\\). We write the vector-valued path latency function as \\(\\ell : \\Delta \\times \\Omega \\mapsto \\mathbb{R}^{|\\mathcal{P}|}_+\\). Each instance \\(\\omega\\) determines a congestion game, captured by the tuple \\(\\mathcal{G}_c^{\\omega} = ( \\mathcal{G}, \\mathcal{W}, \\mathcal{X} , \\mathcal{P}, \\mathcal{\\ell}(\\cdot, \\omega) )\\). Each path flow profile \\(\\mu \\in \\Delta\\) induces a probability measure associated with the positive random vector \\(\\ell(\\mu, \\cdot): \\Omega \\mapsto \\mathbb{R}^{|\\mathcal{P}|}_+\\). The following assumption gives some realistic properties for the latency.\nStanding Assumption For all \\(e \\in \\mathcal{E}\\), the latency functions \\(l_e\\) are \\(\\omega\\)-measurable, for all \\(\\omega \\in \\Omega\\), \\(l_e\\) are \\(L_0\\)-Lipschitz continuous and differentiable in \\(q_e\\) with \\(\\cfrac{ \\partial l_e (q_e, \\omega)}{\\partial q_e} \u0026gt; 0 \\) for all \\(q_e \\geq 0\\).\nNow let\u0026rsquo;s define and find the equilibria satisfying of a deterministic game, i.e., when \\(|\\Omega|\\) is a singleton. The first, as it turns out, coincides with the Nash Equilibrium.\nDefinition (Wardrop Equilibrium) A path flow \\(\\mu \\in \\Delta\\) is said to be a Wardrop Equilibrium (WE) if \\(\\forall w \\in \\mathcal{W}\\), \\( \\mu_p \u0026gt; 0\\) indicates \\( \\ell_p \\leq \\ell_{p^{\\prime}}\\) for all \\(p^{\\prime} \\in \\mathcal{P}_w\\).\nThe second, while not satisfying the incentive conditions, somehow concides with the Nash equilibrium in a \u0026ldquo;regularized\u0026rdquo; version of the game, where the utility for each individual player is a sum of the latency and a \u0026ldquo;toll price\u0026rdquo;.\nDefinition (System Equilibrium) A path flow \\(\\mu \\in \\Delta\\) is said to be a system optimum if the aggregated latency \\(S(\\mu) := \\sum_{e \\in \\mathcal{E}} q_e l_e \\) is minimized.\nWardrop equilibrium can be characterized by the minimization of an objective function called Beckmann potential, we end this post by proving it.\nTheorem A path flow \\(\\mu^* \\in \\Delta\\) is a WE if and only if it minimizes the Beckmann potential: $$ \\min_{\\mu \\in \\Delta} \\Phi (\\mu) := \\sum_{e\\in \\mathcal{E}}\\int_{0}^{(\\Lambda \\mu)_e} l_e (z) dz. $$\nProof We first write down the constraint \\(\\Delta\\) as a set of inequalities and equalities, $$ \\begin{align*} -\\mu \u0026amp; \\preceq 0 \\\\ M \\mu - \\bf{m} \u0026amp; = 0 \\end{align*} $$ where \\(M \\in \\mathbb{R}^{W \\times |\\mathcal{P}|}\\), \\(M_{w,p} = 1\\) if \\(p \\in \\mathcal{P}_w\\) otherwise \\(0\\), \\(\\boldsymbol{m} = (m_1, \\ldots, m_W)\\) is the measure vector.\nWriting down the Lagrangian, by defining multipliers \\(\\lambda \\in \\mathbb{R}^{|\\mathcal{P}|}, \\nu \\in \\mathbb{R}^{W}\\), let \\(t_e = (0, \\ldots, \\underbrace{1}_{e^{th} edge}, \\ldots, 0)\\) be the basis vecotrs, $$ \\mathcal{L} (\\mu , \\lambda, \\nu ) = \\sum_{e\\in \\mathcal{E}} \\int_{0}^{(t_e \\Lambda) \\mu} l_e (z) dz - \\lambda^{\\top} \\mu - \\nu^{\\top} (M\\mu - \\boldsymbol{m}). $$\nThe KKT condition says, $$ \\begin{align*} \\nabla_{\\mu} \\mathcal{L} \u0026amp; = \\sum_{e \\in \\mathcal{E}} \\Lambda^{\\top} t_e^{\\top} l_e(q_e ) - \\lambda - M^{\\top}\\nu \\\\ \u0026amp; = \\ell - \\lambda - M^{\\top}\\nu = 0 \\\\ \\lambda^{\\top} \\mu \u0026amp; = 0 \\\\ \\lambda \u0026amp; \\succeq 0 \\end{align*} $$ we get that \\(\\ell \\succeq M^{\\top} \\nu\\), \\(\\ell^{\\top }\\mu = \\nu^{\\top} M \\mu\\), what this essentially means is that whenever \\(\\mu_p \u0026gt; 0\\), \\(\\ell_p\\) are identical in that path set \\(\\mathcal{P}_w\\). To see this, note that \\(\\ell_p \\geq (M^{\\top} \\nu)_p = \\sum_{w \\in \\mathcal{W}} \\nu_w \\mathbb{I}_{ p \\in \\mathcal{P}_w }\\), which is a constant lower bound for \\(p \\in \\mathcal{P}_w\\), fixing a \\(w\\); and \\(\\sum_{w \\in \\mathcal{W}}\\sum_{p \\in \\mathcal{P}_w}\\ell_p \\mu_p = \\sum_{w \\in \\mathcal{W}}\\sum_{p \\in \\mathcal{P}_w} (M^{\\top} \\nu)_p \\mu_p \\), this implies that for any \\(w \\in \\mathcal{W}\\), if \\(\\mu_p \u0026gt; 0\\) for some of the \\(p \\in \\mathcal{P}_w\\), the only possibility is that \\(\\ell_p = (M^{\\top}\\nu)_p\\). The uniqueness of the edge flow solution \\(q = \\Lambda \\mu\\) can be established through calculating the Hessian of \\(\\Phi\\), however, \\(\\mu\\) is generally non-unique, as it should be in the solution set to a linear equation.\nWardrop, J.G., 1952. Road paper. some theoretical aspects of road traffic research. Proceedings of the institution of civil engineers, 1(3), pp.325-362.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nBeckmann, M., McGuire, C.B. and Winsten, C.B., 1956. Studies in the Economics of Transportation (No. 226 pp).\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nRosenthal, R.W., 1973. A class of games possessing pure-strategy Nash equilibria. International Journal of Game Theory, 2(1), pp.65-67.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"2023-01-30T00:00:00Z","permalink":"https://yunianpan.com/post/2023/wardrop_equilibirum/","section":"post","tags":null,"title":"Wardrop Equilibrium"},{"categories":["It might be useful idk"],"contents":"There are many service centers in our city, such as MTA subway station, Vaccination sites, Wifi hot-spots, Blue Bicycles, hospitals, parking lots etc.. Meanwhile, there are so many people in need of these services who are distributed maybe according to some point processes. The question of how to efficiently make assignments between the demands and the service centers gives rise to a special type of problems called semi-discrete optimal transport.\nThis post will explain Monge\u0026rsquo;s partition and how it can be achieved using some geospatial data.\nPreliminary Setting Let us denote the entire space as $\\mathcal{X}$, (in a lot of cases this $\\mathcal{X}$ is just some hull in a 2D plane.) The dirac-delta measure of service centers is $\\nu = \\sum_{i=1}^S \\nu_s \\delta_{x_s}$, where $x_s \\in \\mathcal{X} \\ \\ s = 1, \\ldots, S$ are $S$ resource centers. The demand processes is a Poisson point process $\\eta: \\mathcal{X} \\to \\mathbf{N}$ ($\\mathbf{N}$ is some $\\sigma$-finite measure) with intensity measure $\\lambda: \\mathcal{X} \\to \\mathbb{R}_+$.\nThe public utility of assgining a demand point $x \\in \\mathcal{X}$ to service center $x_s$ is $u_s(\\cdot): \\mathcal{X} \\to \\mathbb{R}$. We assume this utility is $u_s(x) = C - |x - x_s|^2$, where $C$ is some constant.\nThe Matching Problem First look at a natural possible solution where we partition the entire map so that each site is assigned some \u0026lsquo;\u0026lsquo;governed area\u0026rsquo;\u0026rsquo;, the allocation takes a form of an open subpartition of $\\mathcal{X}$.\nDefinition The set of such allocation rule is: $$ \\mathcal{A} = \\left\\{ \\mathbf{A} = (A_1, \\ldots, A_S), A_s \\text{ is an open subset of }\\mathcal{X}, A_s \\bigcap A_k = \\emptyset \\ \\ \\forall s \\neq k \\right\\} $$ and the unallocated set is $A_0 : = \\mathcal{X} - \\bigcup_{s=1}^S A_s$.\nNow let the regional social welfare be $u(\\mathbf{A})$, (i.e., the sum of utility,) by Campbell\u0026rsquo;s formula $$ \\begin{aligned} u(\\mathbf{A}) = \u0026amp; \\sum_{s=1}^S \\mathbb{E}\\left[ \\int_{A_s} u_s(x) \\eta(dx) \\right] \\ = \u0026amp;\\sum_{s=1}^S \\int_{A_s} u_s(x)\\lambda(dx) \\end{aligned} $$ The Problem\nThe incentive of a public planner is to maximize the regional social welfare, under the constraint such that every service center is fully loaded, (under the assumption that total demand is saturated.)\n\\begin{align} \\max_{\\mathbf{A}\\in \\mathcal{A}}\u0026amp;\\quad u(\\mathbf{A}) \\ \\text{s.t. }\u0026amp;\\quad \\nu_s \\leq \\lambda(A_s) \\ \\ \\ s = 1,\\ldots, S . \\end{align}\nA New York City Instance Now construct the two densities needed for our problem.\nYou can sort of downloaded the data from NYC Open Data, which provides the locations of seasonal flu vaccinatiion sites. Let\u0026rsquo;s take a look at the locations by visualize it through $\\texttt{seaborn}$.\nimport geopandas as gpd import numpy as np import pandas import pysal import seaborn import contextily import matplotlib.pyplot as plt from sklearn.cluster import DBSCAN #from sklearn.neighbors import KernelDensity v_sites = pandas.read_csv( \u0026#39;data/New_York_City_Locations_Providing_Seasonal_Flu_Vaccinations.csv\u0026#39;) g_sites = gpd.GeoDataFrame(v_sites, geometry=gpd.points_from_xy(v_sites.Longitude, v_sites.Latitude)) g_sites.head() joint_axes = seaborn.jointplot( x=\u0026#39;Longitude\u0026#39;, y=\u0026#39;Latitude\u0026#39;, data=v_sites, s=0.8, height=15 ) contextily.add_basemap( joint_axes.ax_joint, crs=\u0026#34;EPSG:4326\u0026#34;, source=contextily.providers.CartoDB.PositronNoLabels ) # For simplicity we assume that the capacity is index-irrelevant, assgining uniform mass to every site. site_pos = np.vstack([v_sites.Longitude, v_sites.Latitude]) site_measure = {\u0026#34;pos\u0026#34;: site_pos.T, \u0026#34;cap\u0026#34;: (1/len(site_pos[0])) * np.ones(len(site_pos[0]))} Kernel Density and Hexbin Plot as Two Types of Density Estimation A hexbin plot is simply a hexagonal grid that counts how many points fall within each grid cell, so it is actually a spatial or $2$-dimensional histogram.\nKernel density estimator uses a non-parametric smooth kernel bump $\\frac{1}{h}K(\\frac{x - y}{h})$ such that $\\int_{-\\infty}^{\\infty} \\frac{1}{h}K(\\frac{x - y}{h})dx = 1$ and $\\int_{-\\infty}^{\\infty} \\frac{x}{h}K(\\frac{x - y}{h})dx = 0 $ to describe the grid bins. The approximated density in $2$-dimensional case is: $$ f(x, y) = \\frac{1}{n} \\sum_{i=1}^n \\frac{1}{h_1 h_2} K(\\frac{x - X_i}{h_1}) K(\\frac{y - Y_i}{h_2}) $$ Often times we simply choose $K(\\frac{x - y}{h})$ to be Gaussian kernel: $$ K(x, y) \\propto \\exp(- \\frac{|x - y|^2}{2h^2}) $$\nObviously for the discrete measure we do not need this type of modeling, so it is just for illustration purpose. Later when we deal with diffusing measure we might need these density estimators.\nf, ax = plt.subplots(1, 2, figsize=(25, 18)) ax[0].hexbin( v_sites[\u0026#39;Longitude\u0026#39;], v_sites[\u0026#39;Latitude\u0026#39;], gridsize=50, linewidths=0, alpha=0.5, cmap=\u0026#39;viridis_r\u0026#39; ) contextily.add_basemap( ax[0], crs=\u0026#34;EPSG:4326\u0026#34;, source=contextily.providers.CartoDB.Positron ) ax[1] = seaborn.kdeplot( v_sites[\u0026#39;Longitude\u0026#39;], v_sites[\u0026#39;Latitude\u0026#39;], n_levels=50, shade=True, alpha=0.55, cmap=\u0026#39;viridis_r\u0026#39; ) contextily.add_basemap( ax[1], crs=\u0026#34;EPSG:4326\u0026#34;, source=contextily.providers.CartoDB.Positron ) What about the real density? Now we have obtained the point locations, and we assigned uniform point masses to each of them representing the \u0026lsquo;\u0026lsquo;capacity\u0026rsquo;\u0026rsquo;, so we are done constructing the first discrete measure.\nNow what we need is the populational density of demand. Unfortunately even the Poisson type of point data is unavailable, as it has to be the dataset of spatial-tagged disease cases. So, to get the demand density distribution, we approximate it from a NYC census data, downloaded from Kaggle Mapping New York City Census Data. There are two files we can consider. The first one being the census block, which contains the exact locations of the blocks being censored, the block codes, county and state names. The second one contains the census tracts, the county, borough names and exact populations calculated for each block.\nLet\u0026rsquo;s first merge the two pandas data frames to obtain the point data, each point contains all the information carried by the census tract statistics.\nblocks = pandas.read_csv(\u0026#39;data/census_block_loc.csv\u0026#39;) census = pandas.read_csv(\u0026#39;data/nyc_census_tracts.csv\u0026#39;, index_col=0) blocks = blocks[blocks.County.isin( [\u0026#39;Bronx\u0026#39;, \u0026#39;Kings\u0026#39;, \u0026#39;New York\u0026#39;, \u0026#39;Queens\u0026#39;, \u0026#39;Richmond\u0026#39;])] blocks[\u0026#39;Tract\u0026#39;] = blocks.BlockCode // 10000 blocks = blocks.merge(census, how=\u0026#39;left\u0026#39;, right_index=True, left_on=\u0026#39;Tract\u0026#39;) blocks.head() Start From Populational Density We choose the total population as a number that labels the specific points with their associated populational density, so still they form a discrete point measure. But it is at least helpful to visualize the populational density through some colormap.\nlatmin = 40.48 lonmin = -74.28 latmax = 40.93 lonmax = -73.65 lat_vals = np.mgrid[latmin:latmax:200j] lon_vals = np.mgrid[lonmin:lonmax:200j] mp_vals = np.zeros([200, 200]) d_lat = lat_vals[1] - lat_vals[0] d_lon = lon_vals[1] - lon_vals[0] for lat, lon, val in zip(blocks.Latitude, blocks.Longitude, blocks.TotalPop): lat_idx = int(np.rint((lat - latmin) / d_lat)) lon_idx = int(np.rint((lon - lonmin) / d_lon)) if not np.isnan(val): mp_vals[lon_idx, lat_idx] = val lon_mts = np.min(lon_vals), np.max(lon_vals) lat_mts = np.min(lat_vals), np.max(lat_vals) fig, ax = plt.subplots(figsize=(15,15)) ax.set(xlim=lon_mts, ylim=lat_mts) contextily.add_basemap(ax, crs=\u0026#34;EPSG:4326\u0026#34;, source=contextily.providers.CartoDB.Positron) im = ax.imshow(mp_vals.T, origin=\u0026#39;lower\u0026#39;, cmap=\u0026#39;Greens\u0026#39;, extent=(np.min(lon_vals), np.max(lon_vals), np.min(lat_vals), np.max(lat_vals))) ax.set_xlabel(\u0026#39;Longitude\u0026#39;) ax.set_ylabel(\u0026#39;Latitude\u0026#39;) ax.set_title(\u0026#39;The population distributiion\u0026#39;) plt.colorbar(im, fraction=0.035, pad=0.04) plt.show() entire_pop = int(np.sum(blocks.TotalPop)) num_tracts = len(blocks.Longitude) print(\u0026#34;The entire population in NYC is {}, censored in {} tracts \\n \\ The city area is in the bounded box of longitudes [{} {}] and latitudes [{} {}]\u0026#34;. format(entire_pop, num_tracts, lon_mts[0], lon_mts[1], lat_mts[0], lat_mts[1])) The entire population in NYC is 52551009, censored in 18053 tracts The city area is in the bounded box of longitudes [-74.28 -73.65] and latitudes [40.48 40.93] Recall that the kernel density estimation is basically interpolating between the sampled points to predict the intensity of unsampled region, so the early view is that maybe we can also use the trick by fine-griding the entire 2D plane and apply the \u0026lsquo;\u0026lsquo;scaled\u0026rsquo;\u0026rsquo; version of density estimator, say $$ \\hat{\\lambda}(x, y) = \\frac{1}{\\sum_i pop_i h_x h_y} \\sum_{i=1}^N pop_i K(\\frac{x - x_i}{h_x}) K(\\frac{y - y_i}{h_y}), $$ where $\\hat{\\lambda}(x, y)$ is the estimated density at longitude $y$ and latitude $x$, $pop_i$ is the censored total population at point $(x_i, y_i)$\nTo examine the correctness of this estimation, integrate the density over latitude and longitude $$ \\begin{aligned} \u0026amp; \\qquad \\int_{- \\infty}^{\\infty} \\int_{-\\infty}^{\\infty} \\frac{1}{\\sum_i pop_i h_x h_y} \\sum_{i=1}^N pop_i K(\\frac{x - x_i}{h_x}) K(\\frac{y - y_i}{h_y}) dx dy \\\\ \u0026amp; = \\frac{1}{\\sum_i pop_i} \\sum_{i=1}^N pop_i \\int_{- \\infty}^{\\infty} \\frac{1}{h_x} K(\\frac{x - x_i}{h_x}) dx \\int_{-\\infty}^{\\infty} \\frac{1}{h_y} K(\\frac{y - y_i}{h_y}) dy \\\\ \u0026amp; = \\frac{1}{\\sum_i pop_i} \\sum_i pop_i = 1 \\end{aligned} $$ Note that this demand estimation has some issues: first, it does not really make sense to integrate over an infinite 2-D plane, however for our application it\u0026rsquo;s sufficient as the tail will \u0026lsquo;\u0026lsquo;fade\u0026rsquo;\u0026rsquo; near the boundary; second, we have to consider the plausibility to use census data as a source of demand sampling. But since now I don\u0026rsquo;t have many choices :(, I will briefly use this for illustration purposes. If there are spatial-tagged discease data poping up, maybe then we can perform some more accurate geospatial analysis.\nI am going to use the libary scipy Gaussian Kernel density estimation module to obtain the population density since it has the weighted implementation.\nfrom scipy import stats points = np.stack([np.array(blocks.Longitude), np.array(blocks.Latitude)]) pops = np.array(blocks.TotalPop) pops[np.isnan(pops)] = 0 gkernel = stats.gaussian_kde(dataset=points, bw_method=None, weights=pops) positions = np.stack([lat_vals.ravel(), lon_vals.ravel()]) Z = np.reshape(gkernel(positions).T, lon_vals.shape) print(\u0026#39;The kernel integration over the entire boundary is {}\u0026#39;.format(gkernel.integrate_box(low_bounds=[np.min(lon_vals), np.min(lat_vals)], high_bounds=[np.max(lon_vals), np.max(lat_vals)]))) The kernel integration over the entire boundary is 0.9917503652964066 Just to verify the estimated density is close to the data by plotting the heatmap of kernel integration. It turns out the 2-loop integration is quite slow, (about 20 mins). What we finally will get is a [200, 200] matrix representing the populational density on the map.\nker_vals = np.zeros((200, 200)) for lat_val in lat_vals: for lon_val in lon_vals: lat_idx = int(np.rint((lat_val - latmin) / d_lat)) lon_idx = int(np.rint((lon_val - lonmin) / d_lon)) #print(\u0026#34;iter: lon {} lat {}\u0026#34;.format(lat_idx, lon_idx)) ker_vals[lon_idx, lat_idx] = gkernel.integrate_box(low_bounds=[lon_val, lat_val], high_bounds=[lon_val+d_lon, lat_val+d_lat]) fig, ax = plt.subplots(figsize=(15,15)) ax.set(xlim=lon_mts, ylim=lat_mts) #contextily.add_basemap(ax, crs=\u0026#34;EPSG:4326\u0026#34;, source=contextily.providers.CartoDB.Positron) im = ax.imshow(ker_vals.T, origin=\u0026#39;lower\u0026#39;, cmap=\u0026#39;Purples\u0026#39;, extent=(np.min(lon_vals), np.max(lon_vals), np.min(lat_vals), np.max(lat_vals))) ax.set_xlabel(\u0026#39;Longitude\u0026#39;) ax.set_ylabel(\u0026#39;Latitude\u0026#39;) ax.set_title(\u0026#39;The estimated populational density\u0026#39;) plt.colorbar(im, fraction=0.035, pad=0.04) plt.show() The Kantorovich Relaxation and Dual It turns out the infinite dimensional problem can be solved by looking at its dual.\nRecall what we previously did is representing the transference plan $\\mathrm{T}_{\\#}$ as $\\mathbf{A}$, where $\\mathrm{T}_{\\#}(x) = \\sum_{x_s} x_s \\mathbb{I}_{{ x \\in A_s}}$. What this transference plan means is that for every individual in location $x$, he or she is going to be sent site $x_s$ if $x$ is in the partition governed by $x_s$, i.e., $x \\in A_s$. However, we can define a more general transference plan that allows the splitting of population in the location $x$.\nDefinition A weak relaxation is a set of non-negative measure over $\\mathcal{X}$: $$ \\boldsymbol{\\lambda} := (\\lambda_1, \\lambda_2, \\ldots, \\lambda_S) \\quad \\sum_{i=1}^S \\lambda_i \\leq \\lambda \\quad \\lambda_0 = \\lambda - \\sum_{i=1}^S \\lambda_i $$\nConsider a more general partition which assign each site a positive measure $\\lambda_s$, such that $\\int_{\\mathcal{X}}\\lambda_{s}=\\nu_{s}$ and $\\sum_{s \\in \\mathcal{S}} \\lambda_{s} \\leq \\lambda $, in this form of assignment the demand of the entire area is divided by $\\boldsymbol{\\lambda}$ and assigned to corresponding sites.\nThe problem can be restated as finding a coupling $\\pi \\in \\bar{\\Pi}(\\lambda, \\nu)$ to maximize the total utility of assignment:\n$$ \\begin{aligned} U \u0026amp;:= \\max_{\\pi \\in \\bar{\\Pi}(\\lambda, \\nu)} \\int_{\\mathcal{X} \\times \\mathcal{X}} u(x, y)\\pi(dx, dy) \\ \u0026amp;= \\max_{\\pi \\in \\bar{\\Pi}(\\lambda, \\nu)} \\int_{\\mathcal{X}}\\sum_{s \\in \\mathcal{X}_{\\mathcal{S}}} u(x, s) \\pi(dx, s), \\end{aligned} $$\nwhere $\\bar{\\Pi}(\\lambda, \\nu)$ is defined as: $$ \\bar{\\Pi} (\\lambda, \\nu) = \\left\\{\\sum_{s \\in \\mathcal{S}} \\delta_{x_{s}}(d y) \\otimes \\lambda_{s}(dx), \\text { where } \\int_{\\mathcal{X}} \\lambda_{s}=\\nu_{s} \\text{ and } \\sum_{s \\in \\mathcal{S}} \\lambda_{s} \\leq \\lambda .\\right\\} $$\nThis set of coupling can be thought of as a sum of naive independent couplings between site measure $\\nu$ and splitted population measures $\\lambda_s$, the splitted population measure sum up to at most $\\lambda$.\nIn analogue to the Monge-Kantorovich theory, define the set of dual profit-price pairs: $$ \\overline{\\mathcal{J}}_{u}:={(\\xi, p) \\in C(\\mathcal{X}) \\times \\mathbb{R}^{|S|} ; \\quad \\xi(x)+p_s \\geq u(x, x_s) \\ \\ \\forall(x, x_s) \\in \\mathcal{X} \\times \\mathcal{X}_{\\mathcal{S}} }. $$ This dual variable set has an interesting interpretation. Imagine you are a broker who helps the government to make vaccination recomendation for the customers by pricing the sites. For each customer at location $x$, you help the customer get $\\xi(x)$ as a vaccination welfare, and if the customer goes to site $x_s$, and the government gets paid with price $p_s$. So the total utility made with your help is $ \\xi(x) + p_s$ for customer $x$ going to site $x_s$, it has to satisfy that your interposition actually gives no shrinkage of the social welfare, i.e., $\\xi(x)+p_s \\geq u(x, x_s) $.\nThe classical Kantorovich duality theorem says:\nTheorem[Strong duality] If $\\lambda(\\mathcal{X}) = \\nu(\\mathcal{X})$, $$ U = \\inf_{(\\xi, p ) \\in \\overline{J}_u}\\int_{\\mathcal{X}}\\xi d\\lambda + \\sum_{s \\in \\mathcal{S}} p_s \\nu_s. $$\nThe Dual Program Now we define a transformation that is related to the optimal pricing.\nDefinition [$u$-transform] For profit function $\\xi \\in \\mathcal{C}(\\mathcal{X})$, its $u$-transform is $$ \\xi^u_s = \\sup_{x \\in \\mathcal{X}} u(x, x_s) - \\xi(x) $$ for price vector $p \\in \\R^S$, its $u$-transform is $$ p^u(x) = \\sup_{s \\in \\mathcal{S}}u(x, x_s) - p_s $$\nDefinition [$u$-convexity] $\\xi$ is $u$-convex if it satisfies that $\\xi = p^u$ for some $p \\in \\mathbb{R}^S$, similarly, $p$ is $u$-convex if it satisfies that $p = \\xi^u$ for some $\\xi \\in C(\\mathcal{X})$. Denote the set of $u$-convex functions as $\\mathcal{U}_{\\xi}$ and $\\mathcal{U}_p$.\nOne important property is that $\\xi$ is $u$-convex if.f $\\xi^{uu} = \\xi$, and the same holds for $p$. By this property it can be shown that $\\mathbb{R}^S \\subset \\mathcal{U}_p $ ($\\forall p \\in \\mathbb{R}^S$, $p^{uu} = p$), thus $\\mathcal{U}_p = \\mathbb{R}^S$. Without such simplification, the significance is that by villani optimal transport, when the source and target measure is balanced one can narrow the searching scope in the set of $u$-convex functions $\\mathcal{U}_{\\ xi }$ or $\\mathcal{U}_{p}$.\nStrong Duality Restate Theorem[Strong duality (unbalanced)] The primal problem can be reduced to its dual form: if $\\lambda(\\mathcal{X}) \u0026gt; \\nu(\\mathcal{X})$, then $$ U = \\inf_{ p \\in \\mathcal{U}_p; p \\ge 0} \\int_{\\mathcal{X}} [p^u]_{+} d \\lambda + \\sum_{i=1}^S p_i \\nu_i, $$ where $[p^u]_+$ gives the optimal partition.\nThe advantage of such dual transform is that instead of directly looking for a infinite-dimensional partition, one optimize $p$ over finite dimensional space $\\R^S_+$, computing $u$-transform of $p$ is equivalent to finding $p$-weighted partition, denoted as $\\mathbf{A}^p = (A_1^p, \\ldots, A^p_S)$, where $$ A^p_s = { x \\in \\mathcal{X} \\ \\ [u(x, x_s) - p_s]_+ \\geq [u(x, x_j) - p_j]_+ \\ \\ \\forall j \\neq s} $$ Note that if $u(x,x_s)$ is the negative Euclidean distance plus a large constant, the partition coincides with $p$-weighted Voronoi diagram.\nSolution to the Dual Let the dual objective function be $\\Phi(\\cdot): \\mathbb{R}^S \\to \\mathbb{R}$, $$ \\Phi(p) = \\int_{\\mathcal{X}} \\sup_{s\\in \\mathcal{S}}[u(x, x_s) - p_s]_+ d\\lambda(x) + \\sum_{s\\in\\mathcal{S}} p_s \\nu_s $$\nLemma[Characterization of $\\Phi$] $\\Phi$ is convex and continuously differentiable with $$ \\frac{ \\partial \\Phi (p) }{ \\partial p_s } = \\nu_s - \\lambda A^p_s $$\nThus we can leverage gradient descent $ p \\leftarrow p - \\alpha_t \\nabla_p \\Phi$ to solve the problem. This gradient has a quite straightforward interpretation which is the demand-capacity differences. Intuitively if the demand is higher than the capacity then the price goes higher such that the assigned partition will be smaller, and vice versa.\nNow we are at least ready to approach to the partition problem, with sites measure $\\nu$, estimated density $\\hat{\\lambda}$, and shifted quadratic utility $u_s$. The first task is to calculate and plot $A_p$, it turns out that given a price $p$, the partition is just a set of adjacent polygons.\nWe can plot that under no whatsoever assumptions about the demand density, i.e., population uniformly distributed, what the Voronoi-diagram looks like. But we have to develop some machinaries to calculate and display the polygons induced by the diagrams. The code from (author: sunayana) provides an implementation to obtain the weighted voronoi cells.\nI did not found particularly interesting things implementing the pricing algorithm but I blame my advisor for that, :)\n","date":"2022-12-31T00:00:00Z","permalink":"https://yunianpan.com/post/2022/monge/","section":"post","tags":null,"title":"Monge Partition"},{"categories":["Crazy brain melting math"],"contents":"I did a presentation in a group meeting to briefly review the lower complexity bound of first-order convex optimization; and how Nesterov proceed to match the lower bound using the estimation sequence, the slides are here.\nConsider an unconstrained optimization problem: $$ \\min_{x \\in \\mathbb{R}^n} f(x) . $$ Here, \\(f \\in \\mathcal{C}^1\\) is a convex, $L$-Lipschitz smooth function. Obviously we can solve this problem by using first-order methods, using iterations:\n$$ x_{k} \\in x_{0} + \\operatorname{Span} \\left \\{f^{\\prime} \\left (x_{0} \\right ), \\ldots, f^{\\prime} \\left (x_{k-1} \\right )\\right \\}. $$\nThe question is what their fundamental limit is, and how to achieve it.\nFor fundamental limits, Nesterov constructed a quadratic function whose one-step gradients give very little geometry information about the other dimensions, so that once the dimensionality of the problem becomes large, it gets really hard to solve it. (As the error is lower bounded by $\\mathcal{O} (1/k^2)$, with $k$ being the dimension.)\nHe then came up with an algorithm to match the lower bound. Despite the complexity of the algorithm itself, the idea is not that (actually it is) complicated:\nWe use a series of weighted quadratic functions $\\{\\lambda_k, \\phi_k \\}$ to estimate the function itself at each point $x_k$, which has two properties\nThey provide a lower bound to the true function at each step. $$ \\phi_{k}(x) \\leq (1-\\lambda_{k} ) f(x)+\\lambda_{k} \\phi_{0}(x) $$ They converge to the true function as the algorithm progresses. $$ \\lambda_k \\to 0 $$ So intead of minimizing the function itself, we minimize $\\{\\phi_k \\}$ at each step, which is easier but also matches the lower bound because it is quadratic!\nThere are of course a bunch of alternative interpretations about this method, one of which I found quite intriguing was the dual perspective1, which views every accelerated gradient step as an optimal coupling of the primal and dual step.\nAnother that I know of is Bubbeck\u0026rsquo;s paper. You will have to see the geometric interpretation for yourself, it basically says that two balls shrink is faster than one ball shrinks. Personally I find the youtube \u0026lsquo;\u0026lsquo;momentum videos\u0026rsquo;\u0026rsquo; not super helpful\u0026hellip;\nAllen-Zhu, Z. and Orecchia, L., 2014. Linear coupling: An ultimate unification of gradient and mirror descent. arXiv preprint arXiv:1407.1537.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"2022-12-31T00:00:00Z","permalink":"https://yunianpan.com/post/2022/nesterov/","section":"post","tags":null,"title":"Nesterov"},{"categories":["It is too trivial"],"contents":"This is pretty much scribed from Tor Lattimore\u0026rsquo;s Bandit Algorithms book,Bandit Algorithms. This book has been the most informational item for me in a while. There is a chapter about the foundations of information theory, which is simply entropy, but the story telling reminds me of deception. Entropy, in plain words, is the measure of uncertainty for certain information; to communicate is to eliminate such uncertainty, to deceive, however, is to obscure certain information, hence to enforce such uncertainty. The duality has never been formalized as far as I know, because if you simply name the Shannon limit as the capacity of deception, you are doing anything innovative. But, if you think from Shannon\u0026rsquo;s perspective, wasn\u0026rsquo;t he also just doing the same thing? (Sorry for quoting my advisor here.) Anyway here goes the story:\nSo Alice wants to tell with Bob about a sequence of \\(n\\) independent random outcomes sampled from a known distribution \\(Q\\). To keep things concise, they\u0026rsquo;ve agreed on a secret binary language. Now, we know that the entropy of \\(Q\\) can be interpreted as the expected number of bits necessary per random variable using the optimal code as \\(n\\) goes to \\(\\infty\\). The relative entropy between distributions \\(P\\) and \\(Q\\), we can think of it as the extra bits Alice and Bob have to lug around if they mistakenly believe the random variables are sampled from \\(P\\) instead of \\(Q\\).\nLet $P$ be a measure on $[N]$ with $\\sigma$-algebra $2^{[N]}$ and $X: [N] \\to [N]$ be the identity random variable, i.e., $X(\\omega) =X$. Since binary code is used to convey the message, they might code each $X$ as a binary code function $c: [N] \\to \\{0,1\\}^* $ where $\\{0,1\\}^{*} $ is the set of finite sequence of zeros and ones. $c$ must be injective (so it won\u0026rsquo;t cause amibiguity between different random variables), and prefix free (so it won\u0026rsquo;t cause ambiguity between any two codes). This is simply because Bob needs to know where one symbol starts and ends for multiple samples.\nWe know that the easiest choice is to use $\\lceil \\log (N) \\rceil$ bits no matter what value of $X$ is, but if $X$ is far from uniform, let\u0026rsquo;s say $P(X = 1) = 0.99$, and then no matter what the rest of them look like, it\u0026rsquo;s preferreable to use shorter code for $X = 1$ than $\\lceil \\log (N) \\rceil$. A natural objective formulated is\n$$ c^* = \\arg\\min_c \\mathbb{E}_{i \\sim P } [ length(c(i)) ]. $$\nIt is well known that this optimization problem can be solved by Huffman Coding, thus the optimal value satisfies:\n$$ H_2(P) \\leq \\sum_{i=1}^N p_i length(c^*(i)) \\leq H_2(P) + 1, $$\nwhere $H_2(P)$ is the entropy of $P$\n\\[ H_2(P) = \\sum_{i=1, p_i \u0026gt; 0}^N - p_i \\log(p_i) . \\]\nThe naive idea of using a code of uniform length is only recovered when $P$ is uniformly distributed. Why $p_i \u0026gt; 0$? Think about $\\lim_{x \\to 0^+} x\\log(x) = 0$, or think about $H_2(P)$ as kind of an expectation, which should not change under the perturbation of measure $0$ set.\nThe entropy $H_2(P)$ is a fundamental quantity. It\u0026rsquo;s based on $\\log_2$ since we are talking about binary code, sometimes it\u0026rsquo;s more convenient to scale it with natural logarithm. Shannon Source Coding Theorem tells us (informal) that any $P$ compressed to fewer than $N H_2(P)$ will inevitably result in information lost, so any coding that results average bits cost $H_2(P)$ is unimprovable.\nRelative Entropy Now, imagine Alice and Bob in a parallel universe where they use a code that is optimal for $X$ sampled from distribution $Q$, but actually $X$ is sampled from $P$. Here comes the terminology of related entropy between $P$ and $Q$, it measures how much longer the messages are expected to be using the optimal code for $Q$ than what is obtained from using optimal code for $P$. Let $p_i = P(X= i)$ and $q_i = Q(X= i)$, assuming Shannon\u0026rsquo;s coding, the definition of relative entropy can be written as\n$$ D(P,Q) = \\sum_{i \\in [N]: p_i \u0026gt; 0} - p_i \\log(q_i) - (\\sum_{i \\in [N]: p_i \u0026gt; 0} - p_i \\log(p_i)) = \\sum_{i \\in [N]: p_i \u0026gt; 0} p_i \\log(\\frac{p_i}{q_i}) $$\nFrom Jensen\u0026rsquo;s inequality ($\\log$ is concave so using the fact that $\\mathbb{E}_p \\{\\log(\\frac{q_i}{p_i})\\} \\leq \\log( \\mathbb{E}_p ( \\frac{q_i}{p_i})))$ or the optimality of coding, $D(P,Q) \\geq 0$. Actually this is also called KL divergence just in some other contexts. Question remains that what if $p_i $ and $ q_i = 0$ for some $i \\in [N]$? Well, it means that $i$ is not neccessary for consideration since by both $P$ and $Q$, $i$ is in a measure zero set. Also, the sufficient and necessary condition for $D(P,Q)$ to be finite is that whenver $q_i = 0$, $p_i = 0$. Using measure-theoretic language, this condition means that $P$ is absolutely continuous with respect to $Q$.\nNow we jump out of the story and consider arbitrary measurable space $(\\Omega, \\mathcal{F})$. Support of $P$ might not be finite, or even countable. Defining entropy through the same path is pretty hard as the symbols needed have to be infinite. This fundamental difficulty is automatically resolved if we directly consider relative entropy.\nFormally, if we do a discretization over the sample space $\\Omega$, i.e., find a measurable map $X : \\Omega \\to [N]$. Then, the relative entropy can be defined as $$ D(P,Q) = \\sup_{N \\in \\mathbb{N}^+} \\sup_{X} D(P_X, Q_X), $$ $P_X$ and $Q_X$ are pushforwards, i.e., they measure, say, $\\forall \\mathcal{I} \\in 2^{[N]}$, by $P( X^{-1}(\\mathcal{I}))$. In other words, the relative entropy here actually measures the capacity of Bob distinguishing between $P$ and $Q$ by receiving the \u0026lsquo;\u0026lsquo;codes\u0026rsquo;\u0026rsquo; $\\mathcal{I}$, however the encrpytion is done by Alice. This measurement has profound meanings in a ton of applications.\nTheorem 1 Let $(\\Omega, \\mathcal{F})$ be a measurable space, also let $P$ and $Q$ be measures on this space. Then\n$$ D(P,Q) = \\begin{cases} \\int \\log(\\frac{dP}{dQ}(\\omega)) dP(\\omega) ,\\ \\ \u0026amp;\\text{if} P \\ll Q; \\\\\\ \\infty, \\quad \u0026amp;\\text{otherwise} \\end{cases} \\tag{1} $$\nWhen calculating the relative entropy the densities are always used. If $\\lambda$ is a $\\sigma$-finite measure dominating both $P$ and $Q$, let $p = \\frac{dP}{d\\lambda}$ and $q = \\frac{dQ}{d\\lambda}$, if $P \\ll Q$, by chain rule we write\n$$ D(P,Q) = \\int p \\log(\\frac{p}{q}) d\\lambda \\tag{2} $$\nSuch a $\\lambda$ can always be found, for example $\\lambda = P+Q$ always dominate $P$ and $Q$.\nNote that relative entropy measures the distance from $P$ to $Q$ but it can never be treated as a metric since there are some properties unsatisfied such as triangular inequality and commutability. However, it serves the same purpose.\nExamples Consider two Gaussian variables with means $\\mu_1$ and $\\mu_2$ and variance $\\sigma^2$:\n$$ D(\\mathcal{N}(\\mu_1 , \\sigma^2), \\mathcal{N}(\\mu_2 , \\sigma^2)) = \\frac{(\\mu_1 - \\mu_2)^2}{2\\sigma^2}. $$\nThe quadratic term matches our intuition about such a \u0026lsquo;\u0026lsquo;distance\u0026rsquo;\u0026rsquo;.\nConsider two Bernouli random variables with means $p, q \\in [0,1]$, then:\n$$ D(\\mathcal{B}(p), \\mathcal{B}(q)) = p\\log(\\frac{p}{q}) + (1-p)\\log(\\frac{1-p}{1-q}), $$\nand we have to let $0 \\log(\\cdot) = 0$.\nTo Wrap It Up Now we come to the inequality dictating the capacity of deception, an inequality that Connects the relative entropy to the hardness of hypothesis testing in the following theorem\nTheorem 2 (Bretagnolle-Huber Inequality) Let $P$ and $Q$ be probability measures on the same measurable spcae $(\\Omega, \\mathcal{F})$, and let $A \\in \\mathcal{F}$ be an arbitrary event. Then\n$$ P(A) + Q(A^c) \\geq \\frac{1}{2} \\exp (-D(P,Q)) $$\nwhere $A^c = \\Omega \\backslash A$ is the complement of A.\nProof For reals $a, b$, abbreviate $a \\vee b: = \\max\\{ a, b\\}$, and $a \\wedge b := \\min\\{a,b\\}$. If $D(P,Q) = \\infty$, then the inequality holds trivially true. If it\u0026rsquo;s not, then $P \\ll Q$ by Theorem $1$. Let $\\nu = P+Q$, and the Radon-Nikodym derivatives $p =\\frac{ dP }{d\\nu}$, $q =\\frac{ dQ }{d\\nu}$. By $(2)$, the relative entropy\n$$ D(P,Q) = \\int p \\log(\\frac{p}{q}) d\\nu $$\nSometimes we drop $\\nu$ for brevity, writtin it as $\\int p \\log(\\frac{p}{q})$. It turns out a stronger result is sufficient:\n$$ \\int p\\wedge q \\geq \\frac{1}{2} \\exp(-D(P,Q)). $$\nWhy? Because $\\int p \\wedge q = \\int_A p\\wedge q + \\int_{A^c} p \\wedge q \\leq \\int_A p + \\int_{A^c} q = P(A) + Q(A^c)$. We firstly have to utilize the Cauchy-Schwarz inequality and identity $pq = (p\\wedge q) (p\\vee q)$,\n$$ \\left( \\int \\sqrt{pq}\\right)^2 = \\left( \\sqrt{(p\\wedge q) (p\\wedge q)} \\right) \\leq \\left( \\int p \\wedge q\\right) \\left( \\int p \\vee q\\right). $$\nAlso, using identity $p\\wedge q + p \\vee q = p + q$, we have $\\int p\\wedge q = 2 - \\int p \\vee q \\leq 2$, so for both $p \\vee q$ and $p \\wedge q$ we have them lower bounded by $\\left(\\int \\sqrt{pq}\\right)^2$. Now, using Jensen\u0026rsquo;s inequality we arrive at some elementry manipulation:\n$$\\begin{align} \\left(\\int \\sqrt{pq}\\right)^2 \u0026amp; = \\exp(2 \\log \\int \\sqrt{pq}) = \\exp(2 \\log\\int_{p\u0026gt; 0} p \\sqrt{\\frac{q}{p}}) . \\nonumber \\\\\\ \u0026amp; \\geq \\exp(2\\int_{p \u0026gt; 0} p \\frac{1}{2} \\log(\\frac{q}{p})) = \\exp(-\\int_{pq \u0026gt; 0} p \\log(\\frac{p}{q})) \\nonumber \\\\\\ \u0026amp; = \\exp(-\\int p \\log (\\frac{p}{q})) = \\exp(-D(P,Q)). \\end{align}$$\nSince $P\\ll Q$, $q = 0$ implies $p = 0$, so $p\u0026gt;0$ implies $q \u0026gt; 0$, therefore $pq \u0026gt; 0$. Divide both sides by $2$ concludes the proof.\nThere\u0026rsquo;s a little bit intuition. If $P$ and $Q$ are close, we expect $P(A) + Q(A^c)$ to be large to be close enough to $1$, and how large it is is just quantified by this theorem. Also the result is symmetric and we can always replace $D(P,Q)$ by $D(Q,P)$, yet $D(P,Q)$ is not symmetric, therefore sometimes stronger inequality is obtained.\n","date":"2022-11-16T18:58:11+08:00","permalink":"https://yunianpan.com/post/2022/the_capacity_of_deception/","section":"post","tags":null,"title":"Capacity of Deception"},{"categories":null,"contents":" Phone +1 (646)-404-1857\nEmail pyn950@.gmail.com\n","date":"0001-01-01T00:00:00Z","permalink":"https://yunianpan.com/contact/","section":"","tags":null,"title":""},{"categories":null,"contents":"I am a P.h.D. candidate at NYU Tandon. My dissertation focuses on the transient, macroscopic, and microscopic behaviors of multi-agent learning and interactions, which emerge from the complex systems accross various disciplines. My work primarily centers on the intersection of statistical reinforcement learning and applied game theory, where I try to quantify the uncertainties arise in decision-making processes and understand their efficiency. An application is the design of decentralized multi-radar communication algorithm that mitigates the mutual interference during the target detection.\nEducation Ph.D., Electrical Engineering (Applied Game Theory) — New York University, 2021–2026\nDissertation focus: Non-equilibrium design in multi-agent learning systems.\nM.Sc., Electrical Engineering (Reinforcement Learning) — New York University, 2018–2020\nProjects:\n• Reproducing TRPO \u0026amp; PPO (code)\n• Urban vaccination-site covering via semi-discrete Optimal Transport (code)\nB.Eng., Communication Engineering (NLP) — Beijing University of Posts and Telecommunications, 2014–2018\nWorking Graduate Assistant — LARX Lab, 2020–2021\nModel-Agnostic Meta-Reinforcement Learning for LQR\n• Repo: github.com/UnionPan/mamllqr\nResearch Intern - NXP Semiconductors, 2025 Developed frequency-time-domain multi-agent interference avoidance strategy through game-theoretic modeling;\nTeaching Teaching Assistant — NYU ECE Department\n• Probability and Stochastic Processes (2020)\n• System Optimization Methods (2019–2024) • Game Theory (2021–2025)\nPublications Journal Articles Y. Pan, T. Li, Q. Zhu. “Model-agnostic meta-policy optimization via zeroth-order estimation: A linear quadratic regulator perspective,” arXiv:2503.00385, 2025.\nH. Li, T. Li, Y. Pan, T. Xu, Q. Zhu, Z. Zheng. “Towards universal robust federated learning via meta Stackelberg game,” 2024. OpenReview.\nConference Proceedings Y. Pan, J. Li, L. Xu, S. Sun, Q. Zhu. “A game-theoretic approach for high-resolution automotive FMCW radar interference avoidance,” 2025 (arXiv–2503).\nY. Pan, Q. Zhu. “Extending no-regret hopping in FMCW radar interference avoidance,” 2025.\nY.-T. Yang, Y. Pan, Q. Zhu. “Preference-centric route recommendation: Equilibrium, learning, and provable efficiency,” 2025.\nY. Pan, T. Li, Q. Zhu. “On the variational interpretation of mirror play in monotone games,” 2024. arXiv:2403.15636.\nY. Pan, T. Li, H. Li, T. Xu, Q. Zhu, Z. Zheng. “A first-order meta Stackelberg method for robust federated learning,” New Frontiers in Adversarial ML Workshop, 2023. OpenReview.\nY. Pan, T. Li, Q. Zhu. “Is stochastic mirror descent vulnerable to adversarial delay attacks? A traffic assignment resilience study,” CDC 2023, pp. 8328–8333. DOI.\nY. Pan, T. Li, Q. Zhu. “On the resilience of traffic networks under non-equilibrium learning,” ACC 2023, pp. 3484–3489.\nY. Pan, Q. Zhu. “On poisoned Wardrop equilibrium in congestion games,” GameSec 2022, pp. 191–211.\nY. Pan, Q. Zhu. “Efficient episodic learning of nonstationary and unknown zero-sum games using expert game ensembles,” CDC 2021, pp. 1669–1676.\nY. Pan, G. Peng, J. Chen, Q. Zhu. “MASAGE: Model-agnostic sequential and adaptive game estimation,” GameSec 2020, pp. 365–384.\nBook Chapters \u0026amp; Technical Reports H. Li, T. Xu, T. Li, Y. Pan, Q. Zhu, Z. Zheng. A First-Order Meta Stackelberg Method for Robust Federated Learning (Technical Report), 2023. arXiv:2306.13273.\nT. Li, Y. Pan, Q. Zhu. Decision-Dominant Strategic Defense Against Lateral Movement for 5G Zero-Trust Multi-Domain Networks, 2023. arXiv:2310.01675.\nSkills Math Foundations: Algorithmic Game Theory • Statistical Learning • Convex Optimization • Stochastic Calculus • Probability • Control Reinforcement Learning • Radar Signal Processing\nTools \u0026amp; Platforms: PyTorch • LangChain • Gym • Pettingzoo • SUMO • R • MATLAB • SQL • LoRA • vLLM • VectorBT • QuantLib • Backtrader\nCompetencies: Academic Research • Presentations • Linux • Git • LaTeX • HPC\nAwards 2023 — Dante Youla Award for Graduate Research Excellence in Electrical Engineering\n2022 — Best Paper Award (GameSec 2022): On Poisoned Wardrop Equilibrium in Congestion Games\n2020 — Merit Award (NYU ECE)\nInvited Sessions INFORMS 2021 \u0026amp; 2025\nEfficient Episodic Learning of Nonstationary and Unknown Zero-Sum Games Using Expert Game Ensembles\nEfficient Learning in Congestion Games with Dueling Feedback\nACC 2023 — On the Resilience of Traffic Networks Under Non-Equilibrium Learning\nTechnical Reviewer (2022-2025): Reviewed research papers for top venues including Anual Reviews in Control, IEEE ACC/CDC/RADAR, and Transporation Research.\n","date":"0001-01-01T00:00:00Z","permalink":"https://yunianpan.com/statement/","section":"","tags":null,"title":""}]