[{"categories":["Statistics"],"contents":"The dueling bandit problem natrually fits the description of a variety of recommendation systems that require \u0026lsquo;\u0026rsquo;learning on the fly\u0026rsquo;\u0026rsquo;, yet have no explicit access to a \u0026lsquo;\u0026lsquo;reward\u0026rsquo;\u0026rsquo; model. Instead, the \u0026lsquo;\u0026lsquo;human\u0026rsquo;\u0026rsquo; feedback part takes the form of \u0026lsquo;\u0026lsquo;choices\u0026rsquo;\u0026rsquo;, \u0026lsquo;\u0026lsquo;votes\u0026rsquo;\u0026rsquo;, or some discrete forms. Hence, oftentimes a learning protocol proceeds at time steps \\(t = 1, \\ldots, T\\):\nThe algorithm chooses a pair of arms \\( a_i, a_j \\) from \\(K\\) available ones; The oracle/human feedback/nature reveals the winner arm \\( a_i \\), with probability \\( P(a_i \\succ a_j)\\), \\( P(a_i \\prec a_j) = 1 - P(a_i \\succ a_j) \\) So the feedback is either \\(a_i\\) or \\( a_j \\), the preferences forms a matrix \\(P \\in \\mathbb{R}^{K \\times K}\\) such that \\( P + P^{\\top} = I\\) which defines the hidden information of the dueling bandit problem. The cumulative regret in the stochastic dueling bandit setting is: $$ \\mathcal{R}_T = \\sum_{t=1}^T P( a^* \\succ a^t_i ) + P( a^* \\succ a^t_j ) $$ where \\(a^* \\) is usually the Condorcet winner, i.e., \\( P( a^* \\succ a_j) \u0026gt; \\frac{1}{2} \\ \\ \\forall j \\in [K], a_j \\neq a^*\\). Condorcet winner is a pretty straightforward idea, which might not exist in general cases. To see that, suppose there are three candidates: A, B, and C, and three voters with the following preferences:\nVoter 1: A \u0026gt; B \u0026gt; C Voter 2: B \u0026gt; C \u0026gt; A Voter 3: C \u0026gt; A \u0026gt; B Voter 1 Voter 2 Voter 3 A \u0026gt; B X X B \u0026gt; C X X C \u0026gt; A X X In this example, let\u0026rsquo;s check the pairwise comparisons:\nA vs. B: B is preferred by Voter 2, but A is preferred by Voter 1 and 3. B vs. C: B is preferred by Voter 1 and 2, but C is preferred by Voter 3. C vs. A: C is preferred by Voter 2 and 3, but A is preferred by Voter 1. Since no candidate consistently beats all others in pairwise comparisons, there is no Condorcet winner in this example. Now we transform this example to fit in the dueling bandits context, we can have, for instance, a cyclic relation \\(A \\succ B \\succ C \\succ A \\).\nA B C A 0.5 0.7 0.2 B 0.3 0.5 0.6 C 0.8 0.4 0.5 Apparently the existence of a Condorcet winner requires a row that is greater than \\(0\\). While in a lot of cases such a winner/solution concept might not be suitable, let\u0026rsquo;s first dive into the algorithmic design of trying to find it when it exists. There are basically two styles of algorithms, asymmetric or symmetric:\nThe asymmetric style conceptually separates two choices into choosing a reference arm and an exploration arm. The reference arm acts as a summary of historical pulls. Typical algorithms of this type includes IF, BtM, SAVAGE, Doubler, RUCB, MergeRUCB, RCS, and DTS. The exploration strategy is to maximize the efficiency of identifying the best arm.\nInterleaved Filter (IF) and Beat the Mean (BtM) The very first two methods proposed are Interleaved Filter (IF) 1 and Beat the Mean (BtM) 2. They all assume there is a total ordering of the arms, i.e., we can relabel the arms as \\(a_1, \\ldots, a_K\\), such that \\( p_{i,j} \u0026gt; 0.5 \\) for all \\( i \u0026lt; j\\). Under this assumption the Condorcet winner is \\(a_1\\).\nIF method basically, is a type of \u0026ldquo;hill climbing\u0026rdquo; method that iteratively updates the reference arm \\(\\hat{a}\\) by comparing it with other arms until it becomes the most possible Condorcet winner. The term \u0026ldquo;interleaved\u0026rdquo; is kind of originating from a kind of Netflix ranking acceleration technique, which uses a blend of ranker to recommend videos to the same group of users, then compare the share of viewing hours coming from different rankers. IF picks a reference arm \\(\\hat{a}\\) randomly and preserves a set of \u0026ldquo;Condorcet candidates\u0026rdquo; to compare with the reference arm, as well as a set of confidence intervals \\( \\hat{C}_t := [ P_{\\hat{a}, a} - c_t, P_{\\hat{a}, a} + c_t] \\), where \\(c_{t}=\\sqrt{\\log (1 / \\delta) / t}\\), then it iteratively compares all of them and gradually filters out dominated arms which are out of the confidence intervals, i.e., \\( P_{\\hat{a}, a} - c_t \u0026gt; \\frac{1}{2} \\) and also updates reference arms, i.e., \\(P_{\\hat{a}, a} + c_t \u0026lt; \\frac{1}{2}\\). The corner stone idea is to prove after logarithmic comparisons, the winner between any two pairs is identified as the winner \u0026ldquo;correctly\u0026rdquo; with probability \\(1 - \\delta \\). To see this, let\u0026rsquo;s say \\(n\\) is the number of comparisons, for \\(t \\in \\mathbb{N}_+\\), the event \\( \\mathcal{E}_t\\) is when \\( \\hat{P} - c_t \u0026lt; \\frac{1}{2} \\), which is the condition for the match to continue after \\(t\\) comparisons, therefore $$ Pr( n \\geq t) \\leq P( \\mathcal{E}_t ), $$\nthe confidence interval boundaries \\( p_{i,j} \\notin \\hat{C}_t \\) $$ \\begin{align*} Pr(\\mathcal{E}_t) \u0026amp; = Pr( \\hat{P}_t - p_{i,j} \\leq c_t - \\Delta_{i,j} ) \\\\ \u0026amp; = Pr( \\mathbb{E}[\\hat{P}_t] - \\hat{P}_t \u0026gt; \\Delta_{i,j} - c_t) \\\\ \u0026amp; \\leq Pr( |\\mathbb{E}[\\hat{P}_t] - \\hat{P}_t| \u0026gt; \\Delta_{i,j}/2 ) \\\\ \u0026amp;\\leq 2 \\exp \\left(-t \\epsilon_{i, j}^{2} / 2\\right) \\\\ \u0026amp;\\leq 2 \\exp \\left(-m \\log \\left(T K^{2}\\right)\\right) \\\\ \u0026amp; =2 /\\left(T K^{2}\\right)^{m} , \\end{align*} $$ taking \\( m = \\max{ 4, d }\\), we have \\( Pr( n \\geq t) \\leq K^{-d }\\) since we have \\( c_t \\leq \\Delta_{i,j}/2\\) when \\(t = \\left\\lceil m \\log \\left(T K^{2}\\right) / \\epsilon_{i, j}^{2}\\right\\rceil\\) and \\( m \u0026gt; 4\\).\nBtM leverages the concept of Borda score: $$ b(a_i) = \\frac{1}{K}\\sum_{j} p_{i, j} $$ and two facts:\nthe Condorcet winner cannot be a Borda loser, in the sense that the average Borda score must be greater than \\(0.5\\); the Condorcet winner stays if some other arms are \u0026ldquo;removed\u0026rdquo;. Therefore, as long as we keep eliminate Borda losers, nothing will be left except for the Condorcet winner.\nThe problem with IF is that the theoretical guarantee requires some sorts of Strong Stochastic Transitivity (SST) property: for any triple \\( (i, j, k)\\), \\( \\Delta_{i,k } \\geq \\max \\{ \\Delta_{1, j}, \\Delta_{1,k} \\}\\). BtM only requires a relaxed SST property: there exists \\(\\gamma \\geq 1\\) such that for all pairs \\( (j,k)\\) with \\( 1 \u0026lt; j \u0026lt; k\\), we have \\( \\gamma \\Delta_{1,k} \\geq \\max\\{ \\Delta_{1, j}, \\Delta_{1,k} \\}\\). \\(\\gamma\\) measures the hardness of the problem, as the smaller the gap becomes, the harder it is to identify the better arm while dueling.\nThe following regret bounds have been proven already, given \\(\\Delta_{\\min}\\) and \\( \\gamma\\), $$ \\begin{align*} \\mathbb{E}[ \\mathcal{R}_T^{IF} ] \u0026amp; \\leq \\mathcal{O}\\left( \\frac{K \\log T}{\\Delta_{\\min} }\\right), \\\\ \\mathcal{R}_T^{BtM} \u0026amp; \\leq \\mathcal{O}\\left( \\frac{\\gamma^7 K \\log T}{\\Delta_{\\min }}\\right) \\quad \\text{ with high probability}. \\end{align*} $$\nSensitivity Analysis of VAriables of Generic Exploration (SAVAGE) and Doubler SAVAGE works in a way similar to how BtM works: if we know there is a Condorcet winner, any arms that lose with high probability can be safely eliminated from further consideration. So we can compare arms in a round robin (all-to-all) fashion and drop the pairs of arms as long as it\u0026rsquo;s \u0026ldquo;safe\u0026rdquo; to do so. Its regret bound is of order \\( \\mathcal{O}( K^2 \\log T)\\), which is not tight, however, it empirically outperforms IF and BtM by a wide margin when the arm size is moderate.\nDoubler converts the dueling bandits into conventional multi-armed bandit problems, under the assumptions that the preferences are linear choice functions of underlying utilities associated with the arms. In other words, \\( \\Delta_{A,B} = (\\mu_A - \\mu_B)/2\\), \\(\\mu_A\\) is the mean utility of arm \\(A\\), for example. Doubler proceeds in epochs of exponentially increasing size, (called \u0026ldquo;doubling trick\u0026rdquo;). In each epoch, the left arm is sampled from a fixed distribution, the right arm is chosen from a ordinary bandit algorithm, minimizing the regret against the left arm.\nThere is also a bunch of algorithms based on UCB (Upper Confidence Bound) variants. The fundamental principle for UCB type of algorithms is one can always create a confidence interval for the interesetd statistic estimates depending on how the sampling procedure is going on. (People write tons of tons of paper about different procedures while essentially they are the variants for the same thing, and then there will be someone trying to unifying the different frameworks. Always interesting to keep this thread going on.)\nRMED and Sparring The Relative Minimum Empirical Divergence (RMED) algorithm has been proved to have optimal asymptotic regret. First of all, the authors from 3 constructed some nuanced lower bound example, (I browsed their paper, a little bit hard to comprehend\u0026hellip;) to claim that the lower bound for dueling bandits problem is characterized by $$ \\liminf_{T \\rightarrow \\infty} \\frac{\\mathbb{E}[R(T)]}{\\log T} \\geq \\sum_{i \\in[K] \\backslash{1}} \\min_{j \\in \\mathcal{O}_{i}} \\frac{\\Delta_{1, i}+\\Delta_{1, j}}{2 d\\left(\\mu_{i, j}, 1 / 2\\right)} $$ where the notation \\( d(\\mu_{i,j}, \\frac{1}{2})\\) is kind of characterzing how \u0026ldquo;easy\u0026rdquo; it is to tell which one is better, \\(i\\) or \\(j\\). The trick to match this lower bound is sort of like the plugin principle, you design an algorithm that directly uses the empirical divergence as a criteria to pick the Condorcet winner.\nThe Sparring series belongs to the symmetric algorithms, it is inspired by that the dueling bandits problem is just an example of a symmetric game. symmetric game is one where the payoffs for playing a particular strategy depend only on the other strategies employed, not on who is playing them. That is, if any two players were to switch their strategies with one another, their payoffs would switch as well, leaving the overall outcome of the game unchanged. This is intuitively understandable because you can always view the dueling recommendations as actions played by two players and the eventual goal is to come up with a \u0026ldquo;draw\u0026rdquo; that no one wants to deviate.\nTherefore, any no-regret dynamics would lead to the empirical convergence to the equilibrium of such a game, oftentimes the regret has the form of \\( \\mathcal{O}(\\sqrt{T}) \\), I\u0026rsquo;m going to write a post to discuss the adversarial bandit problems later to talk about my understandings. What was surprising is that empirical experiments show that adversarial algorithms seem to have a logaritmic regret rate despite the proven bound.\nIn general, given an algorithm \\( \\mathcal{A} \\) that solves the adversarial bandit problem, we can use it to sovle the dueling bandits problem by placing a row player and a column player, both sparring with each other by giving out his bet of best action, one gets reward \\(1\\) if he wins, otherwise \\(0\\). As the comparisons have been carried out, the player uses \\(\\mathcal{A}\\) to update their strategies. In this fashion, we are sort of obtaining a sampling estimate of the preference matrix, or at least getting some information of this preference matrix through sparring, it is just this sampling procedure is symmetric.\nA bit thoughts Dueling bandits, as a variant of the multi-armed bandit problem has its relevance in that the preference feedback nowadays are way more easier to obtain. It sort of gave rises to the concept of Reinforcement Learning with Human Feedback and I believe chatgpt has been significantly benefiting from that. I personally think the whole RLHF thing don\u0026rsquo;t really quite need a theoretical fundation unless it is really instructive. Entertainment-wise, I love the advancement of dueling bandits in that it gives a theoretical formalism for how we can deal with discrete type of data. We\u0026rsquo;ll see where it leads us to.\nYue, Y., Broder, J., Kleinberg, R., \u0026amp; Joachims, T. (2012). The k-armed dueling bandits problem. Journal of Computer and System Sciences, 78(5), 1538-1556.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nYue, Y., \u0026amp; Joachims, T. (2011). Beat the mean bandit. In Proceedings of the 28th international conference on machine learning (ICML-11) (pp. 241-248).\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nKomiyama, J., Honda, J., Kashima, H. and Nakagawa, H., 2015, June. Regret lower bound and optimal algorithm in dueling bandit problem. In Conference on learning theory (pp. 1141-1154). PMLR.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"2023-10-11T00:00:00Z","permalink":"https://unionpan.github.io/post/dueling_bandits/","section":"post","tags":null,"title":" Some Notes on Dueling Bandits"},{"categories":["Game Theory"],"contents":"I remember having those unpleasant lines in our high school canteen, flooded by the starving students queeing for their lunch, I would always pick a window with fewer people waiting, compromising myself with awful food. Another similar thought that always stricked me was the odds that tourists always pick the same time to travel, there\u0026rsquo;s almost always traffic congestion everywhere during holiday seasons. Yeah, lives have been always so hard.\nOn the first level of thinking, when there\u0026rsquo;s a lack of resource you attempt to find alternatives, e.g., picking another time for traveling, another food window, etc.. The level two thinking is maybe \u0026lsquo;\u0026lsquo;since other people are avoiding holidays, what if I insist on going out on holidays\u0026rsquo;\u0026rsquo;? Or maybe there\u0026rsquo;s a twisted level three thinking, \u0026lsquo;\u0026lsquo;what if everybody thinks like the level two thinker, \u0026hellip;.\u0026rsquo;\u0026rsquo; It\u0026rsquo;s somewhat intimidating to follow this infinite hierachy, but definitely rewarding, as there are certainly moments when people are regretting their travel decisions, thinking to themselves \u0026lsquo;\u0026lsquo;I probably should not have gone the high way.\u0026rsquo;\u0026rsquo;\nThe aforementioned phenomenons significantly resembled the first idea proposed by Wardrop1 in 1952. Wardrop conducted some \u0026ldquo;before-and-after\u0026rdquo; analysis over some traffic data, and came into the two alternative criteria to determine the flow distribution on the routes,\nThe journey times on all the routes actually used are equal, and less than those which would be experienced by a single vehicle on any unused route; The average journey time is a minimum. It was not until Beckmann2, 1956 that the two simple yet powerful ideas were mathematically formulated, and was considered an example of Rosenthal games3. The first principle basically states that nobody should be happy to deviate from their own routes, which is essentially the behavior of Nash equilibrium.\nTo elaborate, we consider a transporation network represented by a directed, finite, and connected graph $ \\mathcal{G} = (\\mathcal{V}, \\mathcal{E})$, where \\(\\mathcal{V}\\) represent road junctions, the edges \\(\\mathcal{E}\\) represent road segments. The set of Origin-Destination (OD) pairs is \\(\\mathcal{W} \\subseteq \\mathcal{V} \\times \\mathcal{V}\\), indexed by \\(w\\), \\(|\\mathcal{W}| = :W\\). Let \\(\\mathcal{P}: = \\bigcup_{w\\in\\mathcal{W}}\\mathcal{P}_w\\) be the set of directed paths between OD pairs, each path set \\(\\mathcal{P}_w\\) is indexed by \\(w\\).\nThe individual vehicles traveling through \\(\\mathcal{G}\\) are infinitesimal players over \\(\\mathcal{G}\\), denoted by a measurable space \\((\\mathcal{X}, \\mathcal{M}, m)\\). The players are non-atomic, i.e., \\( m(x) = 0 \\ \\ \\forall x \\in \\mathcal{X}\\); they are split into distinct populations indexed by the OD pairs, i.e., \\(\\mathcal{X} = \\bigcup_{w\\in\\mathcal{W}} \\mathcal{X}_w\\) and \\(\\mathcal{X}_w \\bigcap\\) \\(\\mathcal{X}_w\u0026rsquo; = \\emptyset,\\ \\forall w, w^{\u0026rsquo;} \\in \\mathcal{W}\\). For each OD pair \\(w \\in \\mathcal{W}\\), let \\(m_w = m(\\mathcal{X}_w) \\) represent the traffic demand. For each player \\(x \\in \\mathcal{X}_w\\), we assume that their travel path \\(a \\in \\mathcal{P}_w\\) is fixed right after the path selection.\nThe action profile of all the players \\(\\mathcal{X}\\) induces an edge flow vector \\(q \\in \\mathbb{R}^{|\\mathcal{E}|}_+\\), where \\(q_e := \\int_{\\mathcal{X}} \\mathbb{I}_{{e \\in a}} m(dx), e \\in \\mathcal{E}\\), and a path flow vector \\(\\mu \\in \\Delta :=\\{(\\mu_p)_{p\\in \\cup_{w\\in \\mathcal{W}}\\mathcal{P}_w} | \\mu_p := {\\int_{\\mathcal{X}_w} \\mathbb{I}_{{a = p}} m(dx)}\\}\\). The edge-path incident matrix is \\(\\Lambda = [\\Lambda^1 \\vert , \\ldots, \\vert \\Lambda^{|\\mathcal{W}|}] \\in \\mathbb{R}^{|\\mathcal{E}| \\times |\\mathcal{P}|}\\), \\(\\Lambda^w_{e, p} = \\mathbb{I}_{{e\\in p}}, \\forall e\\in \\mathcal{E}, w \\in \\mathcal{W}, p \\in \\mathcal{P}_w\\). Easy to verify the compact form of edge-path flow relation is \\(q = \\Lambda \\mu\\).\nLet \\((\\Omega, \\mathcal{F}, \\mathbb{P})\\) be the probability space, \\(l_e: \\mathbb{R}_+ \\times \\Omega \\to \\mathbb{R}_+\\) be the cost/latency functions, measuring the travel delay of the edge \\(e \\in \\mathcal{E}\\) determined by its edge flow \\(q_e\\) and a state variable \\(\\omega \\in \\Omega\\) that is universal for the entire traffic network, e.g., \\(\\omega\\) can represent the weather condition, road incidents or anything that affects the congestion level. Let \\(l : \\mathbb{R}^{|\\mathcal{E}|}_{\\geq 0} \\times \\Omega \\mapsto \\mathbb{R}^{|\\mathcal{E}|}_+\\) denote the vector-valued latency function.For an instance \\(\\omega \\in \\Omega\\), the latency of path \\(p\\) is defined as \\(\\ell_p : = \\sum_{e \\in p} l_e (q_e, \\omega ) = \\Lambda^{\\top}_p l (\\Lambda \\mu, \\omega )\\), which can be seen as a function of \\(\\mu\\) and \\(\\omega\\), written as \\(\\ell_p = \\ell_p(\\mu, \\omega)\\). We write the vector-valued path latency function as \\(\\ell : \\Delta \\times \\Omega \\mapsto \\mathbb{R}^{|\\mathcal{P}|}_+\\). Each instance \\(\\omega\\) determines a congestion game, captured by the tuple \\(\\mathcal{G}_c^{\\omega} = ( \\mathcal{G}, \\mathcal{W}, \\mathcal{X} , \\mathcal{P}, \\mathcal{\\ell}(\\cdot, \\omega) )\\). Each path flow profile \\(\\mu \\in \\Delta\\) induces a probability measure associated with the positive random vector \\(\\ell(\\mu, \\cdot): \\Omega \\mapsto \\mathbb{R}^{|\\mathcal{P}|}_+\\). The following assumption gives some realistic properties for the latency.\nStanding Assumption For all \\(e \\in \\mathcal{E}\\), the latency functions \\(l_e\\) are \\(\\omega\\)-measurable, for all \\(\\omega \\in \\Omega\\), \\(l_e\\) are \\(L_0\\)-Lipschitz continuous and differentiable in \\(q_e\\) with \\(\\cfrac{ \\partial l_e (q_e, \\omega)}{\\partial q_e} \u0026gt; 0 \\) for all \\(q_e \\geq 0\\).\nNow let\u0026rsquo;s define and find the equilibria satisfying of a deterministic game, i.e., when \\(|\\Omega|\\) is a singleton. The first, as it turns out, coincides with the Nash Equilibrium.\nDefinition (Wardrop Equilibrium) A path flow \\(\\mu \\in \\Delta\\) is said to be a Wardrop Equilibrium (WE) if \\(\\forall w \\in \\mathcal{W}\\), \\( \\mu_p \u0026gt; 0\\) indicates \\( \\ell_p \\leq \\ell_{p^{\\prime}}\\) for all \\(p^{\\prime} \\in \\mathcal{P}_w\\).\nThe second, while not satisfying the incentive conditions, somehow concides with the Nash equilibrium in a \u0026ldquo;regularized\u0026rdquo; version of the game, where the utility for each individual player is a sum of the latency and a \u0026ldquo;toll price\u0026rdquo;.\nDefinition (System Equilibrium) A path flow \\(\\mu \\in \\Delta\\) is said to be a system optimum if the aggregated latency \\(S(\\mu) := \\sum_{e \\in \\mathcal{E}} q_e l_e \\) is minimized.\nWardrop equilibrium can be characterized by the minimization of an objective function called Beckmann potential, we end this post by proving it.\nTheorem A path flow \\(\\mu^* \\in \\Delta\\) is a WE if and only if it minimizes the Beckmann potential: $$ \\min_{\\mu \\in \\Delta} \\Phi (\\mu) := \\sum_{e\\in \\mathcal{E}}\\int_{0}^{(\\Lambda \\mu)_e} l_e (z) dz. $$\nProof We first write down the constraint \\(\\Delta\\) as a set of inequalities and equalities, $$ \\begin{align*} -\\mu \u0026amp; \\preceq 0 \\\\ M \\mu - \\bf{m} \u0026amp; = 0 \\end{align*} $$ where \\(M \\in \\mathbb{R}^{W \\times |\\mathcal{P}|}\\), \\(M_{w,p} = 1\\) if \\(p \\in \\mathcal{P}_w\\) otherwise \\(0\\), \\(\\boldsymbol{m} = (m_1, \\ldots, m_W)\\) is the measure vector.\nWriting down the Lagrangian, by defining multipliers \\(\\lambda \\in \\mathbb{R}^{|\\mathcal{P}|}, \\nu \\in \\mathbb{R}^{W}\\), let \\(t_e = (0, \\ldots, \\underbrace{1}_{e^{th} edge}, \\ldots, 0)\\) be the basis vecotrs, $$ \\mathcal{L} (\\mu , \\lambda, \\nu ) = \\sum_{e\\in \\mathcal{E}} \\int_{0}^{(t_e \\Lambda) \\mu} l_e (z) dz - \\lambda^{\\top} \\mu - \\nu^{\\top} (M\\mu - \\boldsymbol{m}). $$\nThe KKT condition says, $$ \\begin{align*} \\nabla_{\\mu} \\mathcal{L} \u0026amp; = \\sum_{e \\in \\mathcal{E}} \\Lambda^{\\top} t_e^{\\top} l_e(q_e ) - \\lambda - M^{\\top}\\nu \\\\ \u0026amp; = \\ell - \\lambda - M^{\\top}\\nu = 0 \\\\ \\lambda^{\\top} \\mu \u0026amp; = 0 \\\\ \\lambda \u0026amp; \\succeq 0 \\end{align*} $$ we get that \\(\\ell \\succeq M^{\\top} \\nu\\), \\(\\ell^{\\top }\\mu = \\nu^{\\top} M \\mu\\), what this essentially means is that whenever \\(\\mu_p \u0026gt; 0\\), \\(\\ell_p\\) are identical in that path set \\(\\mathcal{P}_w\\). To see this, note that \\(\\ell_p \\geq (M^{\\top} \\nu)_p = \\sum_{w \\in \\mathcal{W}} \\nu_w \\mathbb{I}_{ p \\in \\mathcal{P}_w }\\), which is a constant lower bound for \\(p \\in \\mathcal{P}_w\\), fixing a \\(w\\); and \\(\\sum_{w \\in \\mathcal{W}}\\sum_{p \\in \\mathcal{P}_w}\\ell_p \\mu_p = \\sum_{w \\in \\mathcal{W}}\\sum_{p \\in \\mathcal{P}_w} (M^{\\top} \\nu)_p \\mu_p \\), this implies that for any \\(w \\in \\mathcal{W}\\), if \\(\\mu_p \u0026gt; 0\\) for some of the \\(p \\in \\mathcal{P}_w\\), the only possibility is that \\(\\ell_p = (M^{\\top}\\nu)_p\\). The uniqueness of the edge flow solution \\(q = \\Lambda \\mu\\) can be established through calculating the Hessian of \\(\\Phi\\), however, \\(\\mu\\) is generally non-unique, as it should be in the solution set to a linear equation.\nWardrop, J.G., 1952. Road paper. some theoretical aspects of road traffic research. Proceedings of the institution of civil engineers, 1(3), pp.325-362.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nBeckmann, M., McGuire, C.B. and Winsten, C.B., 1956. Studies in the Economics of Transportation (No. 226 pp).\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nRosenthal, R.W., 1973. A class of games possessing pure-strategy Nash equilibria. International Journal of Game Theory, 2(1), pp.65-67.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"2023-01-30T00:00:00Z","permalink":"https://unionpan.github.io/post/wardrop_equilibirum/","section":"post","tags":null,"title":"Wardrop Equilibrium"},{"categories":["Statistics"],"contents":"The Quest for Optimal Communication Let there be Alice and Bob, whose mission this time is to navigate the labyrinth of random outcomes and communicate in the most efficient way possible.\nAlice wants to communicate with Bob about a sequence of \\(n\\) independent random outcomes sampled from a known distribution \\(Q\\). To keep things concise, they\u0026rsquo;ve agreed on a secret binary language. Now, the plot thickens with the concept of entropy. The entropy of \\(Q\\) is the expected number of bits necessary per random variable using the optimal code as \\(n\\) goes to \\(\\infty\\). There is also the relative entropy between distributions \\(P\\) and \\(Q\\), we can think of it as the extra baggage in message length Alice and Bob have to lug around if they mistakenly believe the random variables are sampled from \\(P\\) instead of \\(Q\\).\nLet $P$ be a measure on $[N]$ with $\\sigma$-algebra $2^{[N]}$ and $X: [N] \\to [N]$ be the identity random variable, i.e., $X(\\omega) =X$. Since binary code is used to convey the message, they might code each $X$ as a binary code function $c: [N] \\to \\{0,1\\}^* $ where $\\{0,1\\}^{*} $ is the set of finite sequence of zeros and ones. $c$ must be injective (so it won\u0026rsquo;t cause amibiguity between different random variables), and prefix free (so it won\u0026rsquo;t cause ambiguity between any two codes). This is simply because Bob needs to know where one symbol starts and ends for multiple samples.\nWe know that the easiest choice is to use $\\lceil \\log (N) \\rceil$ bits no matter what value of $X$ is, but if $X$ is far from uniform, let\u0026rsquo;s say $P(X = 1) = 0.99$, and then no matter what the rest of them look like, it\u0026rsquo;s preferreable to use shorter code for $X = 1$ than $\\lceil \\log (N) \\rceil$. A natural objective formulated is\n$$ c^* = \\arg\\min_c \\mathbb{E}_{i \\sim P } [ length(c(i)) ]. $$\nIt is well known that this optimization problem can be solved by Huffman Coding, thus the optimal value satisfies:\n$$ H_2(P) \\leq \\sum_{i=1}^N p_i length(c^*(i)) \\leq H_2(P) + 1, $$\nwhere $H_2(P)$ is the entropy of $P$\n\\[ H_2(P) = \\sum_{i=1, p_i \u0026gt; 0}^N - p_i \\log(p_i) . \\]\nThe naive idea of using a code of uniform length is only recovered when $P$ is uniformly distributed. Why $p_i \u0026gt; 0$? Think about $\\lim_{x \\to 0^+} x\\log(x) = 0$, or think about $H_2(P)$ as kind of an expectation, which should not change under the perturbation of measure $0$ set.\nThe entropy $H_2(P)$ is a fundamental quantity. It\u0026rsquo;s based on $\\log_2$ since we are talking about binary code, sometimes it\u0026rsquo;s more convenient to scale it with natural logarithm. Shannon Source Coding Theorem tells us (informal) that any $P$ compressed to fewer than $N H_2(P)$ will inevitably result in information lost, so any coding that results average bits cost $H_2(P)$ is unimprovable.\nRelative Entropy Now, imagine Alice and Bob in a parallel universe where they use a code that is optimal for $X$ sampled from distribution $Q$, but actually $X$ is sampled from $P$. Here comes the terminology of related entropy between $P$ and $Q$, it measures how much longer the messages are expected to be using the optimal code for $Q$ than what is obtained from using optimal code for $P$. Let $p_i = P(X= i)$ and $q_i = Q(X= i)$, assuming Shannon\u0026rsquo;s coding, the definition of relative entropy can be written as\n$$ D(P,Q) = \\sum_{i \\in [N]: p_i \u0026gt; 0} - p_i \\log(q_i) - (\\sum_{i \\in [N]: p_i \u0026gt; 0} - p_i \\log(p_i)) = \\sum_{i \\in [N]: p_i \u0026gt; 0} p_i \\log(\\frac{p_i}{q_i}) $$\nFrom Jensen\u0026rsquo;s inequality ($\\log$ is concave so using the fact that $\\mathbb{E}_p \\{\\log(\\frac{q_i}{p_i})\\} \\leq \\log( \\mathbb{E}_p ( \\frac{q_i}{p_i})))$ or the optimality of coding, $D(P,Q) \\geq 0$. Actually this is also called KL divergence just in some other contexts. Question remains that what if $p_i $ and $ q_i = 0$ for some $i \\in [N]$? Well, it means that $i$ is not neccessary for consideration since by both $P$ and $Q$, $i$ is in a measure zero set. Also, the sufficient and necessary condition for $D(P,Q)$ to be finite is that whenver $q_i = 0$, $p_i = 0$. Using measure-theoretic language, this condition means that $P$ is absolutely continuous with respect to $Q$.\nNow we jump out of the story and consider arbitrary measurable space $(\\Omega, \\mathcal{F})$. Support of $P$ might not be finite, or even countable. Defining entropy through the same path is pretty hard as the symbols needed have to be infinite. This fundamental difficulty is automatically resolved if we directly consider relative entropy.\nFormally, if we do a discretization over the sample space $\\Omega$, i.e., find a measurable map $X : \\Omega \\to [N]$. Then, the relative entropy can be defined as $$ D(P,Q) = \\sup_{N \\in \\mathbb{N}^+} \\sup_{X} D(P_X, Q_X), $$ $P_X$ and $Q_X$ are pushforwards, i.e., they measure, say, $\\forall \\mathcal{I} \\in 2^{[N]}$, by $P( X^{-1}(\\mathcal{I}))$. In other words, the relative entropy here actually measures the capacity of Bob distinguishing between $P$ and $Q$ by receiving the \u0026lsquo;\u0026lsquo;codes\u0026rsquo;\u0026rsquo; $\\mathcal{I}$, however the encrpytion is done by Alice. This measurement has profound meanings in a ton of applications.\nTheorem 1 Let $(\\Omega, \\mathcal{F})$ be a measurable space, also let $P$ and $Q$ be measures on this space. Then\n$$ D(P,Q) = \\begin{cases} \\int \\log(\\frac{dP}{dQ}(\\omega)) dP(\\omega) ,\\ \\ \u0026amp;\\text{if} P \\ll Q; \\\\\\ \\infty, \\quad \u0026amp;\\text{otherwise} \\end{cases} \\label{thm1} \\tag{1}$$\nWhen calculating the relative entropy the densities are always used. If $\\lambda$ is a $\\sigma$-finite measure dominating both $P$ and $Q$, let $p = \\frac{dP}{d\\lambda}$ and $q = \\frac{dQ}{d\\lambda}$, if $P \\ll Q$, by chain rule we write\n$$ D(P,Q) = \\int p \\log(\\frac{p}{q}) d\\lambda \\label{eq2} \\tag{2} $$\nSuch a $\\lambda$ can always be found, for example $\\lambda = P+Q$ always dominate $P$ and $Q$.\nNote that relative entropy measures the distance from $P$ to $Q$ but it can never be treated as a metric since there are some properties unsatisfied such as triangular inequality and commutability. However, it serves the same purpose.\nExamples Consider two Gaussian variables with means $\\mu_1$ and $\\mu_2$ and variance $\\sigma^2$:\n$$ D(\\mathcal{N}(\\mu_1 , \\sigma^2), \\mathcal{N}(\\mu_2 , \\sigma^2)) = \\frac{(\\mu_1 - \\mu_2)^2}{2\\sigma^2}. $$\nThe quadratic term matches our intuition about such a \u0026lsquo;\u0026lsquo;distance\u0026rsquo;\u0026rsquo;.\nConsider two Bernouli random variables with means $p, q \\in [0,1]$, then:\n$$ D(\\mathcal{B}(p), \\mathcal{B}(q)) = p\\log(\\frac{p}{q}) + (1-p)\\log(\\frac{1-p}{1-q}), $$\nand we have to let $0 \\log(\\cdot) = 0$.\nTo Wrap It Up Now we come to the inequality dictating the capacity of deception, an inequality that Connects the relative entropy to the hardness of hypothesis testing in the following theorem\nTheorem 2 (Bretagnolle-Huber Inequality) Let $P$ and $Q$ be probability measures on the same measurable spcae $(\\Omega, \\mathcal{F})$, and let $A \\in \\mathcal{F}$ be an arbitrary event. Then\n$$ P(A) + Q(A^c) \\geq \\frac{1}{2} \\exp (-D(P,Q)) $$\nwhere $A^c = \\Omega \\backslash A$ is the complement of A.\nProof For reals $a, b$, abbreviate $a \\vee b: = \\max\\{ a, b\\}$, and $a \\wedge b := \\min\\{a,b\\}$. If $D(P,Q) = \\infty$, then the inequality holds trivially true. If it\u0026rsquo;s not, then $P \\ll Q$ by Theorem $\\eqref{thm1}$. Let $\\nu = P+Q$, and the Radon-Nikodym derivatives $p =\\frac{ dP }{d\\nu}$, $q =\\frac{ dQ }{d\\nu}$. By $\\eqref{eq2}$, the relative entropy\n$$ D(P,Q) = \\int p \\log(\\frac{p}{q}) d\\nu $$\nSometimes we drop $\\nu$ for brevity, writtin it as $\\int p \\log(\\frac{p}{q})$. It turns out a stronger result is sufficient:\n$$ \\int p\\wedge q \\geq \\frac{1}{2} \\exp(-D(P,Q)). $$\nWhy? Because $\\int p \\wedge q = \\int_A p\\wedge q + \\int_{A^c} p \\wedge q \\leq \\int_A p + \\int_{A^c} q = P(A) + Q(A^c)$. We firstly have to utilize the Cauchy-Schwarz inequality and identity $pq = (p\\wedge q) (p\\vee q)$,\n$$ \\left( \\int \\sqrt{pq}\\right)^2 = \\left( \\sqrt{(p\\wedge q) (p\\wedge q)} \\right) \\leq \\left( \\int p \\wedge q\\right) \\left( \\int p \\vee q\\right). $$\nAlso, using identity $p\\wedge q + p \\vee q = p + q$, we have $\\int p\\wedge q = 2 - \\int p \\vee q \\leq 2$, so for both $p \\vee q$ and $p \\wedge q$ we have them lower bounded by $\\left(\\int \\sqrt{pq}\\right)^2$. Now, using Jensen\u0026rsquo;s inequality we arrive at some elementry manipulation:\n$$\\begin{align} \\left(\\int \\sqrt{pq}\\right)^2 \u0026amp; = \\exp(2 \\log \\int \\sqrt{pq}) = \\exp(2 \\log\\int_{p\u0026gt; 0} p \\sqrt{\\frac{q}{p}}) . \\nonumber \\\\\\ \u0026amp; \\geq \\exp(2\\int_{p \u0026gt; 0} p \\frac{1}{2} \\log(\\frac{q}{p})) = \\exp(-\\int_{pq \u0026gt; 0} p \\log(\\frac{p}{q})) \\nonumber \\\\\\ \u0026amp; = \\exp(-\\int p \\log (\\frac{p}{q})) = \\exp(-D(P,Q)). \\end{align}$$\nSince $P\\ll Q$, $q = 0$ implies $p = 0$, so $p\u0026gt;0$ implies $q \u0026gt; 0$, therefore $pq \u0026gt; 0$. Divide both sides by $2$ concludes the proof.\nThere\u0026rsquo;s a little bit intuition. If $P$ and $Q$ are close, we expect $P(A) + Q(A^c)$ to be large to be close enough to $1$, and how large it is is just quantified by this theorem. Also the result is symmetric and we can always replace $D(P,Q)$ by $D(Q,P)$, yet $D(P,Q)$ is not symmetric, therefore sometimes stronger inequality is obtained.\n","date":"2022-11-16T18:58:11+08:00","permalink":"https://unionpan.github.io/post/the_capacity_of_deception/","section":"post","tags":null,"title":"A Cryptographic Conundrum"},{"categories":["Optimization"],"contents":"We briefly reviewed the lower complexity bound of first-order convex optimization and how Nesterov proceed to match the lower bound using the estimation sequence, the slides are here.\nConsider an unconstrained optimization problem: \\[ \\min_{x \\in \\mathbb{R}^n} f(x) . \\] Here, \\(f \\in \\mathcal{C}^1\\) is a convex, $L$-Lipschitz smooth function. Obviously we can solve this problem by using first-order methods, the question is how fast they are.\n","date":"2022-11-15T00:00:00Z","permalink":"https://unionpan.github.io/post/nesterov/","section":"post","tags":null,"title":"Nesterov"},{"categories":null,"contents":"I am a PhD candidate at NYU Tandon. My primary interest lies in statistical/reinforcement learning and applied game theory. Recently I\u0026rsquo;ve been working on macroscopic traffic assignment problems and Non-equilibrium learning in games.\nI love basketball and boxing, I play mildly competitive games of basketball and do some sparring occasionally, you can also find me doing other outdoorsy stuff (like slacklining :))\nI\u0026rsquo;ve recently just finished East of Eden, now I\u0026rsquo;m on my way reading Aumulet, Woes of the True Policeman, (still revisiting the key to 2666) and revisiting Gatsby.\n","date":"2020-04-26T20:18:54+03:00","permalink":"https://unionpan.github.io/about/","section":"","tags":null,"title":"About Me"}]