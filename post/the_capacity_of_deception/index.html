
<!DOCTYPE html>
<html lang="en-us">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, viewport-fit=cover">
    <title>A Cryptographic Conundrum | union&#39;s blog</title>
    <meta name="description"
        content="The Quest for Optimal Communication Let there be Alice and Bob, whose mission this time is to navigate the labyrinth of random outcomes and communicate in the most efficient way possible.
Alice wants to communicate with Bob about a sequence of \(n\) independent random outcomes sampled from a known distribution \(Q\). To keep things concise, they&rsquo;ve agreed on a secret binary language. Now, the plot thickens with the concept of entropy.">
    <link rel="canonical" href="https://unionpan.github.io/post/the_capacity_of_deception/" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/bulma/0.7.4/css/bulma.min.css">
    
    <link rel="stylesheet" href="https://unionpan.github.io/scss/style.min.fd03f00c6cceb36e9051ed7a5f050822f69766d1ebc4b0a9eaf818a01fbcb591.css">
    <meta property="og:title" content="A Cryptographic Conundrum" />
<meta property="og:description" content="The Quest for Optimal Communication Let there be Alice and Bob, whose mission this time is to navigate the labyrinth of random outcomes and communicate in the most efficient way possible.
Alice wants to communicate with Bob about a sequence of \(n\) independent random outcomes sampled from a known distribution \(Q\). To keep things concise, they&rsquo;ve agreed on a secret binary language. Now, the plot thickens with the concept of entropy." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://unionpan.github.io/post/the_capacity_of_deception/" /><meta property="article:section" content="post" />
<meta property="article:published_time" content="2022-11-16T18:58:11+08:00" />
<meta property="article:modified_time" content="2022-11-16T18:58:11+08:00" /><meta property="og:site_name" content="union&#39;s blog" />

    <meta name="twitter:card" content="summary"/><meta name="twitter:title" content="A Cryptographic Conundrum"/>
<meta name="twitter:description" content="The Quest for Optimal Communication Let there be Alice and Bob, whose mission this time is to navigate the labyrinth of random outcomes and communicate in the most efficient way possible.
Alice wants to communicate with Bob about a sequence of \(n\) independent random outcomes sampled from a known distribution \(Q\). To keep things concise, they&rsquo;ve agreed on a secret binary language. Now, the plot thickens with the concept of entropy."/>

    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"
    integrity="sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasHpSy3SV" crossorigin="anonymous">


<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"
    integrity="sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUb0ZY0l8"
    crossorigin="anonymous"></script>


<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05" crossorigin="anonymous"
    onload="renderMathInElement(document.body);"></script>
    
    
    
    <script>
        document.addEventListener("DOMContentLoaded", function () {
            renderMathInElement(document.body, {
                delimiters: [
                    { left: "$$", right: "$$", display: true },
                    { left: "$", right: "$", display: false }
                ]
            });
        });
    </script>
    
</head><body><nav class="navbar is-light" role="navigation">
    <div class="container">
        <div class="navbar-brand">
            <a href="/" title="home" class="navbar-item">
                <span class="logo">
                    <h1>union&#39;s blog</h1>
                </span>
            </a>

            
            <a id="theme-toggle" class="theme-toggle" href="#">
                <img src="https://unionpan.github.io/svg/sun.svg" alt="sun icon" class="theme-icon" />
            </a>

            <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
                <span aria-hidden="true"></span>
                <span aria-hidden="true"></span>
                <span aria-hidden="true"></span>
            </a>
        </div>

        <div class="navbar-menu">
            <div class="navbar-start">
                
                <a href="/about" class="navbar-item">About</a>
                
                <a href="/post" class="navbar-item">Blog</a>
                
                <a href="/categories" class="navbar-item">Categories</a>
                
            </div>

        </div>
        <div class="search">
            <div id="fastSearch">
                <input id="searchInput" tabindex="0" placeholder="Search..">
                <ul id="searchResults">

                </ul>
            </div>
            <a id="search-btn" style="display: inline-block;" href="# ">
                <div class="icon-search"><svg class="search-svg" xmlns="http://www.w3.org/2000/svg" width="28" height="28" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="11" cy="11" r="8"></circle><line x1="21" y1="21" x2="16.65" y2="16.65"></line></svg></div>
            </a>
        </div>

        <script src="/js/fuse.min.js"></script> 
        <script src="/js/fastsearch.js"></script>

    </div>
</nav>

<script>
    
    document.addEventListener('DOMContentLoaded', function() {
        var burger = document.querySelector('.navbar-burger');
        burger.addEventListener('click', function() {
            burger.classList.toggle('is-active');
            document.querySelector('.navbar-menu').classList.toggle('is-active');
        });
    });

    
    function setTheme(theme) {
        let body = document.body;
        let themeIcon = document.querySelector(".theme-icon");
        if (theme === "dark") {
            body.classList.add("dark-mode");
            themeIcon.src = "https:\/\/unionpan.github.io\/svg/moon.svg";
            themeIcon.alt = "moon icon";
        } else {
            body.classList.remove("dark-mode");
            themeIcon.src = "https:\/\/unionpan.github.io\/svg/sun.svg";
            themeIcon.alt = "sun icon";
        }
        
        localStorage.setItem("theme", theme);
    }

    
    let theme = localStorage.getItem("theme") || "light";
    const isDarkMode = window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches;
    if (isDarkMode) {
        
        setTheme('dark');

    } else {
        
        setTheme('light');
    }
    setTheme(theme);

    
    document.getElementById("theme-toggle").addEventListener("click", function() {
        if (theme === "light") {
            theme = "dark";
        } else {
            theme = "light";
        }
        setTheme(theme);
    });



</script>



</header><main>
<div class="single-container">
    <div class="archive">
        <h1 class="title is-1">A Cryptographic Conundrum</h1>
        <div class="title subtitle heading is-6">
            <div class="author-info columns is-vcentered">
                <div class="column">
                    <div class="columns is-vcentered is-mobile">
                        
                        <div class="column is-narrow">
                            <img src="/images/me/avatar.jpeg" class="author-image">
                        </div>
                        
                        <div class="column">
                            <p>union</p>
                            <p><time>November 16, 2022</time>
                            </p>
                        </div>
                    </div>
                </div>
                <div class="small-categories-container">
                    <a href="/categories/statistics">Statistics</a>
                </div>
            </div>
        </div>
        <div class="content article-content">
            <div class="toc-container">
                
    <div class="post-toc">
        
            <aside>
                <button id="tocButton" ><h4 id="contents" style="margin-left: 1vw;color:rgb(96, 134, 180);margin-bottom: 0;">CONTENTS</h4></button>
                <div id="hide"><nav id="TableOfContents">
  <ul>
    <li><a href="#the-quest-for-optimal-communication">The Quest for Optimal Communication</a></li>
    <li><a href="#relative-entropy">Relative Entropy</a>
      <ul>
        <li></li>
        <li><a href="#examples">Examples</a></li>
      </ul>
    </li>
    <li><a href="#to-wrap-it-up">To Wrap It Up</a>
      <ul>
        <li></li>
      </ul>
    </li>
  </ul>
</nav></div>
            </aside>
        
    </div><script>
    
        let button = document.getElementById('tocButton');
        let hide = document.getElementById("hide");
        let contents=document.getElementById("contents");
        button.addEventListener("click", function() {
        if (hide.style.display!='block') {
            hide.style.display='block'
        } else {
            hide.style.display='none'
            contents.style.color='rgb(96, 134, 180)'
        }
        });
    




</script>
            </div>
            <h2 id="the-quest-for-optimal-communication">The Quest for Optimal Communication</h2>
<p>Let there be Alice and Bob, whose mission this time is to navigate the labyrinth of random outcomes and communicate in the most efficient way possible.</p>
<p>Alice wants to communicate with Bob about a sequence of \(n\) independent random outcomes sampled from a known distribution \(Q\).
To keep things concise, they&rsquo;ve agreed on a secret binary language. Now, the plot thickens with the concept of <strong>entropy</strong>. The entropy of \(Q\) is the expected number of bits necessary per random variable using the optimal code as \(n\) goes to \(\infty\). There is also the <strong>relative entropy</strong> between distributions \(P\) and \(Q\), we can think of it as the extra baggage in message length Alice and Bob have to lug around if they mistakenly believe the random variables are sampled from \(P\) instead of \(Q\).</p>
<p>Let $P$ be a measure on $[N]$ with $\sigma$-algebra $2^{[N]}$ and $X: [N] \to [N]$ be the identity random variable, i.e., $X(\omega) =X$. Since binary code is used to convey the message, they might code each $X$ as a <strong>binary code</strong> function $c: [N] \to \{0,1\}^* $ where $\{0,1\}^{*} $ is the set of finite sequence of zeros and ones. $c$ must be <strong>injective</strong> (so it won&rsquo;t cause amibiguity between different random variables), and <strong>prefix free</strong> (so it won&rsquo;t cause ambiguity between any two codes). This is simply because Bob needs to know where one symbol starts and ends for multiple samples.</p>
<p>We know that the easiest choice is to use $\lceil \log (N) \rceil$ bits no matter what value of $X$ is, but if $X$ is far from uniform, let&rsquo;s say $P(X = 1) = 0.99$, and then no matter what the rest of them look like, it&rsquo;s preferreable to use shorter code for $X = 1$ than $\lceil \log (N) \rceil$. A natural objective formulated is</p>
<p>$$
c^* = \arg\min_c \mathbb{E}_{i \sim P } [ length(c(i)) ].
$$</p>
<p>It is well known that this optimization problem can be solved by <strong>Huffman Coding</strong>, thus the optimal value satisfies:</p>
<p>$$
H_2(P) \leq \sum_{i=1}^N p_i length(c^*(i)) \leq H_2(P) + 1,
$$</p>
<p>where $H_2(P)$ is the entropy of $P$</p>
<p>\[
H_2(P) = \sum_{i=1, p_i &gt; 0}^N - p_i \log(p_i) .
\]</p>
<p>The naive idea of using a code of uniform length is only recovered when $P$ is uniformly distributed. Why $p_i &gt; 0$? Think about $\lim_{x \to 0^+} x\log(x) = 0$, or think about $H_2(P)$ as kind of an expectation, which should not change under the perturbation of measure $0$ set.</p>
<p>The entropy $H_2(P)$ is a fundamental quantity. It&rsquo;s based on $\log_2$ since we are talking about binary code, sometimes it&rsquo;s more convenient to scale it with natural logarithm. <strong>Shannon Source Coding Theorem</strong> tells us (informal) that any $P$ compressed to fewer than $N H_2(P)$ will inevitably result in information lost, so any coding that results average bits cost $H_2(P)$ is unimprovable.</p>
<h2 id="relative-entropy">Relative Entropy</h2>
<p>Now, imagine Alice and Bob in a parallel universe where they use a code that is optimal for $X$ sampled from distribution $Q$, but actually $X$ is sampled from $P$. Here comes the terminology of related entropy between $P$ and $Q$, it measures how much longer the messages are expected to be using the optimal code for $Q$ than what is obtained from using optimal code for $P$. Let $p_i = P(X= i)$ and $q_i = Q(X= i)$, assuming Shannon&rsquo;s coding, the definition of relative entropy can be written as</p>
<p>$$
D(P,Q) = \sum_{i \in [N]: p_i &gt; 0} - p_i \log(q_i) - (\sum_{i \in [N]: p_i &gt; 0} - p_i \log(p_i)) = \sum_{i \in [N]: p_i &gt; 0} p_i \log(\frac{p_i}{q_i})
$$</p>
<p>From Jensen&rsquo;s inequality ($\log$ is concave so using the fact that $\mathbb{E}_p \{\log(\frac{q_i}{p_i})\} \leq \log( \mathbb{E}_p ( \frac{q_i}{p_i})))$ or the optimality of coding, $D(P,Q) \geq 0$. Actually this is also called <strong>KL divergence</strong> just in some other contexts. Question remains that what if $p_i $ and $ q_i = 0$ for some $i \in [N]$? Well, it means that $i$ is not neccessary for consideration since by both $P$ and $Q$, $i$ is in a measure zero set. Also, the sufficient and necessary condition for $D(P,Q)$ to be finite is that whenver $q_i = 0$, $p_i = 0$. Using measure-theoretic language, this condition means that $P$ is absolutely continuous with respect to $Q$.</p>
<p>Now we jump out of the story and consider arbitrary measurable space $(\Omega, \mathcal{F})$. Support of $P$ might not be finite, or even countable. Defining entropy through the same path is pretty hard as the symbols needed have to be infinite. This fundamental difficulty is automatically resolved if we directly consider relative entropy.</p>
<p>Formally, if we do a discretization over the sample space $\Omega$, i.e., find a measurable map $X : \Omega \to [N]$. Then, the relative entropy can be defined as
$$
D(P,Q) = \sup_{N \in \mathbb{N}^+} \sup_{X} D(P_X, Q_X),
$$
$P_X$ and $Q_X$ are pushforwards, i.e., they measure, say, $\forall \mathcal{I} \in 2^{[N]}$, by $P( X^{-1}(\mathcal{I}))$. In other words, the relative entropy here actually measures the capacity of Bob distinguishing between $P$ and $Q$ by receiving the &lsquo;&lsquo;codes&rsquo;&rsquo; $\mathcal{I}$, however the encrpytion is done by Alice. This measurement has profound meanings in a ton of applications.</p>
<h4 id="theorem-1">Theorem 1</h4>
<blockquote>
<p>Let $(\Omega, \mathcal{F})$ be a measurable space, also let $P$ and $Q$ be measures on this space. Then</p>
<p>$$ D(P,Q) =  \begin{cases} \int \log(\frac{dP}{dQ}(\omega)) dP(\omega) ,\ \ &amp;\text{if} P \ll Q; \\\ \infty, \quad &amp;\text{otherwise}  \end{cases} \label{thm1} \tag{1}$$</p>
</blockquote>
<p>When calculating the relative entropy the densities are always used. If $\lambda$ is a $\sigma$-finite measure dominating both $P$ and $Q$, let $p = \frac{dP}{d\lambda}$ and $q = \frac{dQ}{d\lambda}$, if $P \ll Q$, by chain rule we write</p>
<p>$$
D(P,Q) = \int p \log(\frac{p}{q}) d\lambda  \label{eq2} \tag{2}
$$</p>
<p>Such a $\lambda$ can always be found, for example $\lambda = P+Q$ always dominate $P$ and $Q$.</p>
<p>Note that relative entropy measures the distance from $P$ to $Q$ but it can never be treated as a metric since there are some properties unsatisfied such as triangular inequality and commutability. However, it serves the same purpose.</p>
<h3 id="examples">Examples</h3>
<p>Consider two Gaussian variables with means $\mu_1$ and $\mu_2$ and variance $\sigma^2$:</p>
<p>$$
D(\mathcal{N}(\mu_1 , \sigma^2), \mathcal{N}(\mu_2 , \sigma^2)) = \frac{(\mu_1 - \mu_2)^2}{2\sigma^2}.
$$</p>
<p>The quadratic term matches our intuition about such a &lsquo;&lsquo;distance&rsquo;&rsquo;.</p>
<p>Consider two Bernouli random variables with means $p, q \in [0,1]$, then:</p>
<p>$$
D(\mathcal{B}(p), \mathcal{B}(q)) = p\log(\frac{p}{q}) + (1-p)\log(\frac{1-p}{1-q}),
$$</p>
<p>and we have to let $0 \log(\cdot) = 0$.</p>
<h2 id="to-wrap-it-up">To Wrap It Up</h2>
<p>Now we come to the inequality dictating the capacity of deception, an inequality that Connects the relative entropy to the hardness of hypothesis testing in the following theorem</p>
<h4 id="theorem-2-bretagnolle-huber-inequality">Theorem 2 (Bretagnolle-Huber Inequality)</h4>
<blockquote>
<p>Let $P$ and $Q$ be probability measures on the same measurable spcae $(\Omega, \mathcal{F})$, and let $A \in \mathcal{F}$ be an arbitrary event. Then</p>
<p>$$ P(A) + Q(A^c) \geq \frac{1}{2} \exp (-D(P,Q)) $$</p>
<p>where $A^c = \Omega \backslash A$ is the complement of A.</p>
</blockquote>
<h4 id="proof">Proof</h4>
<p>For reals $a, b$, abbreviate $a \vee b: = \max\{ a, b\}$, and $a \wedge b := \min\{a,b\}$.
If $D(P,Q) = \infty$, then the inequality holds trivially true. If it&rsquo;s not, then $P \ll Q$ by Theorem $\eqref{thm1}$. Let $\nu = P+Q$, and the Radon-Nikodym derivatives $p =\frac{ dP }{d\nu}$, $q =\frac{ dQ }{d\nu}$. By $\eqref{eq2}$, the relative entropy</p>
<p>$$
D(P,Q) = \int p \log(\frac{p}{q}) d\nu
$$</p>
<p>Sometimes we drop $\nu$ for brevity, writtin it as  $\int p \log(\frac{p}{q})$. It turns out a stronger result is sufficient:</p>
<p>$$
\int p\wedge q \geq \frac{1}{2} \exp(-D(P,Q)).
$$</p>
<p>Why? Because $\int p \wedge q = \int_A  p\wedge q + \int_{A^c} p \wedge q \leq \int_A p + \int_{A^c} q = P(A) + Q(A^c)$.
We firstly have to utilize the Cauchy-Schwarz inequality and identity $pq = (p\wedge q) (p\vee q)$,</p>
<p>$$
\left( \int \sqrt{pq}\right)^2  = \left( \sqrt{(p\wedge q) (p\wedge q)} \right) \leq \left( \int p \wedge q\right) \left( \int p \vee q\right).
$$</p>
<p>Also, using identity $p\wedge q + p \vee q = p + q$, we have $\int p\wedge q = 2 - \int p \vee q \leq 2$, so for both $p \vee q$ and $p \wedge q$ we have them lower bounded by $\left(\int \sqrt{pq}\right)^2$.
Now, using Jensen&rsquo;s inequality we arrive at some elementry manipulation:</p>
<p>$$\begin{align} \left(\int \sqrt{pq}\right)^2 &amp; =  \exp(2 \log \int \sqrt{pq}) = \exp(2 \log\int_{p&gt; 0} p \sqrt{\frac{q}{p}}) .  \nonumber \\\
&amp; \geq \exp(2\int_{p &gt; 0} p \frac{1}{2} \log(\frac{q}{p})) = \exp(-\int_{pq &gt; 0} p \log(\frac{p}{q}))  \nonumber \\\
&amp; = \exp(-\int p \log (\frac{p}{q})) = \exp(-D(P,Q)). \end{align}$$</p>
<p>Since $P\ll Q$, $q = 0$ implies $p = 0$, so $p&gt;0$ implies $q &gt; 0$, therefore $pq &gt; 0$. Divide both sides by $2$ concludes the proof.</p>
<p>There&rsquo;s a little bit intuition. If $P$ and $Q$ are close, we expect $P(A) + Q(A^c)$ to be large to be close enough to $1$, and how large it is is just quantified by this theorem. Also the result is symmetric and we can always replace $D(P,Q)$ by $D(Q,P)$, yet $D(P,Q)$ is not symmetric, therefore sometimes stronger inequality is obtained.</p>

        </div>
    </div>
    <a href="#" id="scrollToTopButton">
        <svg t="1686753152588" class="icon" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg"
            p-id="3988" width="48" height="48">
            <path
                d="M518.5 360.3c-3.2-4.4-9.7-4.4-12.9 0l-178 246c-3.8 5.3 0 12.7 6.5 12.7H381c10.2 0 19.9-4.9 25.9-13.2L512 460.4l105.2 145.4c6 8.3 15.6 13.2 25.9 13.2H690c6.5 0 10.3-7.4 6.5-12.7l-178-246z"
                p-id="3989" fill="#363636"></path>
            <path
                d="M512 64C264.6 64 64 264.6 64 512s200.6 448 448 448 448-200.6 448-448S759.4 64 512 64z m0 820c-205.4 0-372-166.6-372-372s166.6-372 372-372 372 166.6 372 372-166.6 372-372 372z"
                p-id="3990" fill="#363636"></path>
        </svg>
    </a>

<div class="pp-container">
        <section class="pre-and-post">
            <div class="has-text-left">
                
                <p>Previous post</p>
                <a href="https://unionpan.github.io/post/nesterov/">Nesterov</a>
                
            </div>
            <div class="has-text-right">
                
                <p>Next post</p>
                <a href="https://unionpan.github.io/post/wardrop_equilibirum/">Wardrop Equilibrium</a>
                
            </div>
        </section>
    </div>

</div>

        </main><footer class="footer">
    <div class="content has-text-centered">
    <span>&copy; 2024 </span>
    <span>
        only in chaos are we conceivable.
    </span>
    </div>


  </footer>

  </body>
</html>

