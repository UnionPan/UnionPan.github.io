
<!DOCTYPE html>
<html lang="en-us">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, viewport-fit=cover">
    <title>Capacity of Deception | union&#39;s blog</title>
    <meta name="description"
        content="This is pretty much scribed from Tor Lattimore&rsquo;s Bandit Algorithms book,Bandit Algorithms. This book has been the most informational item for me in a while. There is a chapter about the foundations of information theory, which is simply entropy, but the story telling reminds me of deception.
Entropy, in plain words, is the measure of uncertainty for certain information; to communicate is to eliminate such uncertainty, to deceive, however, is to obscure certain information, hence to enforce such uncertainty. The duality has never been formalized as far as I know, because if you simply name the Shannon limit as the capacity of deception, you are doing anything innovative.
But, if you think from Shannon&rsquo;s perspective, wasn&rsquo;t he also just doing the same thing? (Sorry for quoting my advisor here.) Anyway here goes the story:">
    <meta name="google-site-verification" content="leqKZ6ZyQGOq6f_hMMWU-dgFPt8qgZeyJj-ZfCjyz3I" />
    <link rel="canonical" href="https://unionpan.github.io/post/2022/the_capacity_of_deception/" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/bulma/0.7.4/css/bulma.min.css">
    
    <link rel="stylesheet" href="https://unionpan.github.io/scss/style.min.bdfc41214cc1a8cd1b66e75ea61094e5fc36501e8aa0c63d2662d1a318cc3668.css">
    <meta property="og:url" content="https://unionpan.github.io/post/2022/the_capacity_of_deception/">
  <meta property="og:site_name" content="union&#39;s blog">
  <meta property="og:title" content="Capacity of Deception">
  <meta property="og:description" content="This is pretty much scribed from Tor Lattimore’s Bandit Algorithms book,Bandit Algorithms. This book has been the most informational item for me in a while. There is a chapter about the foundations of information theory, which is simply entropy, but the story telling reminds me of deception. Entropy, in plain words, is the measure of uncertainty for certain information; to communicate is to eliminate such uncertainty, to deceive, however, is to obscure certain information, hence to enforce such uncertainty. The duality has never been formalized as far as I know, because if you simply name the Shannon limit as the capacity of deception, you are doing anything innovative. But, if you think from Shannon’s perspective, wasn’t he also just doing the same thing? (Sorry for quoting my advisor here.) Anyway here goes the story:">
  <meta property="og:locale" content="en_us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="post">
    <meta property="article:published_time" content="2022-11-16T18:58:11+08:00">
    <meta property="article:modified_time" content="2022-11-16T18:58:11+08:00">
    <meta property="og:image" content="https://unionpan.github.io/post/2022/the_capacity_of_deception/thumbnail.png">

    
  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:image" content="https://unionpan.github.io/post/2022/the_capacity_of_deception/thumbnail.png">
  <meta name="twitter:title" content="Capacity of Deception">
  <meta name="twitter:description" content="This is pretty much scribed from Tor Lattimore’s Bandit Algorithms book,Bandit Algorithms. This book has been the most informational item for me in a while. There is a chapter about the foundations of information theory, which is simply entropy, but the story telling reminds me of deception. Entropy, in plain words, is the measure of uncertainty for certain information; to communicate is to eliminate such uncertainty, to deceive, however, is to obscure certain information, hence to enforce such uncertainty. The duality has never been formalized as far as I know, because if you simply name the Shannon limit as the capacity of deception, you are doing anything innovative. But, if you think from Shannon’s perspective, wasn’t he also just doing the same thing? (Sorry for quoting my advisor here.) Anyway here goes the story:">

    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"
    integrity="sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasHpSy3SV" crossorigin="anonymous">


<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"
    integrity="sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUb0ZY0l8"
    crossorigin="anonymous"></script>


<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05" crossorigin="anonymous"
    onload="renderMathInElement(document.body);"></script>
    
    
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-P8CZD2GL8F"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag() { dataLayer.push(arguments); }
        gtag('js', new Date());
        gtag('config', 'G-P8CZD2GL8F');
    </script>
    
    <script>
        document.addEventListener("DOMContentLoaded", function () {
            renderMathInElement(document.body, {
                delimiters: [
                    { left: "$$", right: "$$", display: true },
                    { left: "$", right: "$", display: false }
                ]
            });
        });
    </script>
    
</head><body>
        <div class="theme-toggle-container">
            <a id="theme-toggle" class="theme-toggle" href="#">
                <img src="https://unionpan.github.io/svg/sun.svg" alt="sun icon" class="theme-icon" />
            </a>
        </div>
        
        
        <button class="mobile-menu-toggle" id="mobile-menu-toggle" aria-label="Toggle menu">
            <span class="hamburger-line"></span>
            <span class="hamburger-line"></span>
            <span class="hamburger-line"></span>
        </button>

        <div class="layout-container">
            <aside class="sidebar" id="sidebar">
                <div class="sidebar-content">
                    <div class="sidebar-header">
                        <h1 class="site-title">
                            <a href="/" class="site-title-link">YUNIAN PAN</a>
                        </h1>
                    </div>
                    <nav class="sidebar-nav">
                        <div class="nav-item-container">
                            <a href="#" class="nav-link blog-toggle active" onclick="toggleBlogMenu(event)">
                                Blog <span class="dropdown-arrow">▼</span>
                            </a>
                            <div class="submenu" id="blog-submenu">
                                <a href="/post/about_me" class="submenu-link">About Me</a>
                                <a href="/post/2022" class="submenu-link">2022</a>
                                <a href="/post/2023" class="submenu-link">2023</a>
                                <a href="/post/2024" class="submenu-link">2024</a>
                                <a href="/post/2025" class="submenu-link">2025</a>
                            </div>
                        </div>
                        <a href="/statement" class="nav-link ">Statement/CV</a>
                        <a href="/contact" class="nav-link ">Contact</a>
                        <a href="/photos" class="nav-link ">Photos</a>
                    </nav>
                    <div class="sidebar-footer">
                        <div class="copyright">
                            © 2022-2025 all rights reserved
                        </div>
                        <div class="social-icons">
                            
                            <a href="https://www.linkedin.com/in/unionpan/" title="Linkedin" class="social-link">
                                <svg height="32px" width="32px" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"
    stroke-linecap="round" stroke-linejoin="round">
    <path d="M16 8a6 6 0 0 1 6 6v7h-4v-7a2 2 0 0 0-2-2 2 2 0 0 0-2 2v7h-4v-7a6 6 0 0 1 6-6z"></path>
    <rect x="2" y="9" width="4" height="12"></rect>
    <circle cx="4" cy="4" r="2"></circle>
</svg>
                            </a>
                            
                            <a href="https://github.com/UnionPan" title="Github" class="social-link">
                                <svg height="32px" width="32px" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"
    stroke-linecap="round" stroke-linejoin="round">
    <path
        d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37 0 0 0-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44 0 0 0 20 4.77 5.07 5.07 0 0 0 19.91 1S18.73.65 16 2.48a13.38 13.38 0 0 0-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07 0 0 0 5 4.77a5.44 5.44 0 0 0-1.5 3.78c0 5.42 3.3 6.61 6.44 7A3.37 3.37 0 0 0 9 18.13V22">
    </path>
</svg>
                            </a>
                            
                            <a href="https://twitter.com/Union54572322" title="twitter" class="social-link">
                                <svg height="32px" width="32px" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"
    stroke-linecap="round" stroke-linejoin="round">
    <path
        d="M23 3a10.9 10.9 0 0 1-3.14 1.53 4.48 4.48 0 0 0-7.86 3v1A10.66 10.66 0 0 1 3 4s-4 9 5 13a11.64 11.64 0 0 1-7 2c9 5 20 0 20-11.5a4.5 4.5 0 0 0-.08-.83A7.72 7.72 0 0 0 23 3z">
    </path>
</svg>
                            </a>
                            
                            <a href="https://www.facebook.com/yunian.pan/" title="Facebook" class="social-link">
                                <svg height="32px" width="32px" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"
    stroke-linecap="round" stroke-linejoin="round">
    <path d="M18 2h-3a5 5 0 0 0-5 5v3H7v4h3v8h4v-8h3l1-4h-4V7a1 1 0 0 1 1-1h3z"></path>
</svg>
                            </a>
                            
                            <a href="https://www.instagram.com/pyn_rodcutter/?hl=en" title="instagram" class="social-link">
                                <svg height="32px" width="32px" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"
    stroke-linecap="round" stroke-linejoin="round">
    <rect x="2" y="2" width="20" height="20" rx="5" ry="5"></rect>
    <path d="M16 11.37A4 4 0 1 1 12.63 8 4 4 0 0 1 16 11.37z"></path>
    <line x1="17.5" y1="6.5" x2="17.5" y2="6.5"></line>
</svg>
                            </a>
                            
                        </div>
                    </div>
                </div>
            </aside>
            <main class="main-content">
<div class="single-container">
    <div class="archive">
        
        <div class="post-thumbnail-container">
            <img src="https://unionpan.github.io//images/post/2022/The_Capacity_of_Deception/thumbnail.png" alt="Capacity of Deception" class="post-thumbnail-image">
        </div>
        
        <h1 class="title is-1">Capacity of Deception</h1>
        <hr class="title-content-separator">
        
        <div class="title subtitle heading is-6">
            <div class="small-categories-container">
                
                        <span class="category-text">It is too trivial</span>
                    
            </div>
        </div>
        
        <div class="content article-content">
            <div class="toc-container">
                
    <div class="post-toc">
        
            <aside>
                <button id="tocButton" ><h4 id="contents" style="margin-left: 1vw;color:rgb(96, 134, 180);margin-bottom: 0;">CONTENTS</h4></button>
                <div id="hide"><nav id="TableOfContents">
  <ul>
    <li><a href="#relative-entropy">Relative Entropy</a>
      <ul>
        <li></li>
        <li><a href="#examples">Examples</a></li>
      </ul>
    </li>
    <li><a href="#to-wrap-it-up">To Wrap It Up</a>
      <ul>
        <li></li>
      </ul>
    </li>
  </ul>
</nav></div>
            </aside>
        
    </div><script>
    
        let button = document.getElementById('tocButton');
        let hide = document.getElementById("hide");
        let contents=document.getElementById("contents");
        button.addEventListener("click", function() {
        if (hide.style.display!='block') {
            hide.style.display='block'
        } else {
            hide.style.display='none'
            contents.style.color='rgb(96, 134, 180)'
        }
        });
    




</script>
            </div>
            <p>This is pretty much scribed from Tor Lattimore&rsquo;s Bandit Algorithms book,<a href="https://tor-lattimore.com/downloads/book/book.pdf">Bandit Algorithms</a>. This book has been the most informational item for me in a while. There is a chapter about the foundations of information theory, which is simply entropy, but the story telling reminds me of deception.
Entropy, in plain words, is the measure of uncertainty for certain information; to communicate is to eliminate such uncertainty, to deceive, however, is to obscure certain information, hence to enforce such uncertainty. The duality has never been formalized as far as I know, because if you simply name the Shannon limit as the capacity of deception, you are doing anything innovative.
But, if you think from Shannon&rsquo;s perspective, wasn&rsquo;t he also just doing the same thing? (Sorry for quoting my advisor here.) Anyway here goes the story:</p>
<p>So Alice wants to tell with Bob about a sequence of \(n\) independent random outcomes sampled from a known distribution \(Q\).
To keep things concise, they&rsquo;ve agreed on a secret binary language.
Now, we know that the <strong>entropy</strong> of \(Q\) can be interpreted as the expected number of bits necessary per random variable using the optimal code as \(n\) goes to \(\infty\).
The <strong>relative entropy</strong> between distributions \(P\) and \(Q\), we can think of it as the extra bits Alice and Bob have to lug around if they mistakenly believe the random variables are sampled from \(P\) instead of \(Q\).</p>
<p>Let $P$ be a measure on $[N]$ with $\sigma$-algebra $2^{[N]}$ and $X: [N] \to [N]$ be the identity random variable, i.e., $X(\omega) =X$. Since binary code is used to convey the message, they might code each $X$ as a <strong>binary code</strong> function $c: [N] \to \{0,1\}^* $ where $\{0,1\}^{*} $ is the set of finite sequence of zeros and ones. $c$ must be <strong>injective</strong> (so it won&rsquo;t cause amibiguity between different random variables), and <strong>prefix free</strong> (so it won&rsquo;t cause ambiguity between any two codes). This is simply because Bob needs to know where one symbol starts and ends for multiple samples.</p>
<p>We know that the easiest choice is to use $\lceil \log (N) \rceil$ bits no matter what value of $X$ is, but if $X$ is far from uniform, let&rsquo;s say $P(X = 1) = 0.99$, and then no matter what the rest of them look like, it&rsquo;s preferreable to use shorter code for $X = 1$ than $\lceil \log (N) \rceil$. A natural objective formulated is</p>
<p>$$
c^* = \arg\min_c \mathbb{E}_{i \sim P } [ length(c(i)) ].
$$</p>
<p>It is well known that this optimization problem can be solved by <strong>Huffman Coding</strong>, thus the optimal value satisfies:</p>
<p>$$
H_2(P) \leq \sum_{i=1}^N p_i length(c^*(i)) \leq H_2(P) + 1,
$$</p>
<p>where $H_2(P)$ is the entropy of $P$</p>
<p>\[
H_2(P) = \sum_{i=1, p_i &gt; 0}^N - p_i \log(p_i) .
\]</p>
<p>The naive idea of using a code of uniform length is only recovered when $P$ is uniformly distributed. Why $p_i &gt; 0$? Think about $\lim_{x \to 0^+} x\log(x) = 0$, or think about $H_2(P)$ as kind of an expectation, which should not change under the perturbation of measure $0$ set.</p>
<p>The entropy $H_2(P)$ is a fundamental quantity. It&rsquo;s based on $\log_2$ since we are talking about binary code, sometimes it&rsquo;s more convenient to scale it with natural logarithm. <strong>Shannon Source Coding Theorem</strong> tells us (informal) that any $P$ compressed to fewer than $N H_2(P)$ will inevitably result in information lost, so any coding that results average bits cost $H_2(P)$ is unimprovable.</p>
<h2 id="relative-entropy">Relative Entropy</h2>
<p>Now, imagine Alice and Bob in a parallel universe where they use a code that is optimal for $X$ sampled from distribution $Q$, but actually $X$ is sampled from $P$. Here comes the terminology of related entropy between $P$ and $Q$, it measures how much longer the messages are expected to be using the optimal code for $Q$ than what is obtained from using optimal code for $P$. Let $p_i = P(X= i)$ and $q_i = Q(X= i)$, assuming Shannon&rsquo;s coding, the definition of relative entropy can be written as</p>
<p>$$
D(P,Q) = \sum_{i \in [N]: p_i &gt; 0} - p_i \log(q_i) - (\sum_{i \in [N]: p_i &gt; 0} - p_i \log(p_i)) = \sum_{i \in [N]: p_i &gt; 0} p_i \log(\frac{p_i}{q_i})
$$</p>
<p>From Jensen&rsquo;s inequality ($\log$ is concave so using the fact that $\mathbb{E}_p \{\log(\frac{q_i}{p_i})\} \leq \log( \mathbb{E}_p ( \frac{q_i}{p_i})))$ or the optimality of coding, $D(P,Q) \geq 0$. Actually this is also called <strong>KL divergence</strong> just in some other contexts. Question remains that what if $p_i $ and $ q_i = 0$ for some $i \in [N]$? Well, it means that $i$ is not neccessary for consideration since by both $P$ and $Q$, $i$ is in a measure zero set. Also, the sufficient and necessary condition for $D(P,Q)$ to be finite is that whenver $q_i = 0$, $p_i = 0$. Using measure-theoretic language, this condition means that $P$ is absolutely continuous with respect to $Q$.</p>
<p>Now we jump out of the story and consider arbitrary measurable space $(\Omega, \mathcal{F})$. Support of $P$ might not be finite, or even countable. Defining entropy through the same path is pretty hard as the symbols needed have to be infinite. This fundamental difficulty is automatically resolved if we directly consider relative entropy.</p>
<p>Formally, if we do a discretization over the sample space $\Omega$, i.e., find a measurable map $X : \Omega \to [N]$. Then, the relative entropy can be defined as
$$
D(P,Q) = \sup_{N \in \mathbb{N}^+} \sup_{X} D(P_X, Q_X),
$$
$P_X$ and $Q_X$ are pushforwards, i.e., they measure, say, $\forall \mathcal{I} \in 2^{[N]}$, by $P( X^{-1}(\mathcal{I}))$. In other words, the relative entropy here actually measures the capacity of Bob distinguishing between $P$ and $Q$ by receiving the &lsquo;&lsquo;codes&rsquo;&rsquo; $\mathcal{I}$, however the encrpytion is done by Alice. This measurement has profound meanings in a ton of applications.</p>
<h4 id="theorem-1">Theorem 1</h4>
<blockquote>
<p>Let $(\Omega, \mathcal{F})$ be a measurable space, also let $P$ and $Q$ be measures on this space. Then</p>
<p>$$ D(P,Q) =  \begin{cases} \int \log(\frac{dP}{dQ}(\omega)) dP(\omega) ,\ \ &amp;\text{if} P \ll Q; \\\ \infty, \quad &amp;\text{otherwise}  \end{cases} \tag{1} $$</p></blockquote>
<p>When calculating the relative entropy the densities are always used. If $\lambda$ is a $\sigma$-finite measure dominating both $P$ and $Q$, let $p = \frac{dP}{d\lambda}$ and $q = \frac{dQ}{d\lambda}$, if $P \ll Q$, by chain rule we write</p>
<p>$$
D(P,Q) = \int p \log(\frac{p}{q}) d\lambda  \tag{2}
$$</p>
<p>Such a $\lambda$ can always be found, for example $\lambda = P+Q$ always dominate $P$ and $Q$.</p>
<p>Note that relative entropy measures the distance from $P$ to $Q$ but it can never be treated as a metric since there are some properties unsatisfied such as triangular inequality and commutability. However, it serves the same purpose.</p>
<h3 id="examples">Examples</h3>
<p>Consider two Gaussian variables with means $\mu_1$ and $\mu_2$ and variance $\sigma^2$:</p>
<p>$$
D(\mathcal{N}(\mu_1 , \sigma^2), \mathcal{N}(\mu_2 , \sigma^2)) = \frac{(\mu_1 - \mu_2)^2}{2\sigma^2}.
$$</p>
<p>The quadratic term matches our intuition about such a &lsquo;&lsquo;distance&rsquo;&rsquo;.</p>
<p>Consider two Bernouli random variables with means $p, q \in [0,1]$, then:</p>
<p>$$
D(\mathcal{B}(p), \mathcal{B}(q)) = p\log(\frac{p}{q}) + (1-p)\log(\frac{1-p}{1-q}),
$$</p>
<p>and we have to let $0 \log(\cdot) = 0$.</p>
<h2 id="to-wrap-it-up">To Wrap It Up</h2>
<p>Now we come to the inequality dictating the capacity of deception, an inequality that Connects the relative entropy to the hardness of hypothesis testing in the following theorem</p>
<h4 id="theorem-2-bretagnolle-huber-inequality">Theorem 2 (Bretagnolle-Huber Inequality)</h4>
<blockquote>
<p>Let $P$ and $Q$ be probability measures on the same measurable spcae $(\Omega, \mathcal{F})$, and let $A \in \mathcal{F}$ be an arbitrary event. Then</p>
<p>$$ P(A) + Q(A^c) \geq \frac{1}{2} \exp (-D(P,Q)) $$</p>
<p>where $A^c = \Omega \backslash A$ is the complement of A.</p></blockquote>
<h4 id="proof">Proof</h4>
<p>For reals $a, b$, abbreviate $a \vee b: = \max\{ a, b\}$, and $a \wedge b := \min\{a,b\}$.
If $D(P,Q) = \infty$, then the inequality holds trivially true. If it&rsquo;s not, then $P \ll Q$ by Theorem $1$. Let $\nu = P+Q$, and the Radon-Nikodym derivatives $p =\frac{ dP }{d\nu}$, $q =\frac{ dQ }{d\nu}$. By $(2)$, the relative entropy</p>
<p>$$
D(P,Q) = \int p \log(\frac{p}{q}) d\nu
$$</p>
<p>Sometimes we drop $\nu$ for brevity, writtin it as  $\int p \log(\frac{p}{q})$. It turns out a stronger result is sufficient:</p>
<p>$$
\int p\wedge q \geq \frac{1}{2} \exp(-D(P,Q)).
$$</p>
<p>Why? Because $\int p \wedge q = \int_A  p\wedge q + \int_{A^c} p \wedge q \leq \int_A p + \int_{A^c} q = P(A) + Q(A^c)$.
We firstly have to utilize the Cauchy-Schwarz inequality and identity $pq = (p\wedge q) (p\vee q)$,</p>
<p>$$
\left( \int \sqrt{pq}\right)^2  = \left( \sqrt{(p\wedge q) (p\wedge q)} \right) \leq \left( \int p \wedge q\right) \left( \int p \vee q\right).
$$</p>
<p>Also, using identity $p\wedge q + p \vee q = p + q$, we have $\int p\wedge q = 2 - \int p \vee q \leq 2$, so for both $p \vee q$ and $p \wedge q$ we have them lower bounded by $\left(\int \sqrt{pq}\right)^2$.
Now, using Jensen&rsquo;s inequality we arrive at some elementry manipulation:</p>
<p>$$\begin{align} \left(\int \sqrt{pq}\right)^2 &amp; =  \exp(2 \log \int \sqrt{pq}) = \exp(2 \log\int_{p&gt; 0} p \sqrt{\frac{q}{p}}) .  \nonumber \\\
&amp; \geq \exp(2\int_{p &gt; 0} p \frac{1}{2} \log(\frac{q}{p})) = \exp(-\int_{pq &gt; 0} p \log(\frac{p}{q}))  \nonumber \\\
&amp; = \exp(-\int p \log (\frac{p}{q})) = \exp(-D(P,Q)). \end{align}$$</p>
<p>Since $P\ll Q$, $q = 0$ implies $p = 0$, so $p&gt;0$ implies $q &gt; 0$, therefore $pq &gt; 0$. Divide both sides by $2$ concludes the proof.</p>
<p>There&rsquo;s a little bit intuition. If $P$ and $Q$ are close, we expect $P(A) + Q(A^c)$ to be large to be close enough to $1$, and how large it is is just quantified by this theorem. Also the result is symmetric and we can always replace $D(P,Q)$ by $D(Q,P)$, yet $D(P,Q)$ is not symmetric, therefore sometimes stronger inequality is obtained.</p>

        </div>
    </div>
    <a href="#" id="scrollToTopButton">
        <svg t="1686753152588" class="icon" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg"
            p-id="3988" width="48" height="48">
            <path
                d="M518.5 360.3c-3.2-4.4-9.7-4.4-12.9 0l-178 246c-3.8 5.3 0 12.7 6.5 12.7H381c10.2 0 19.9-4.9 25.9-13.2L512 460.4l105.2 145.4c6 8.3 15.6 13.2 25.9 13.2H690c6.5 0 10.3-7.4 6.5-12.7l-178-246z"
                p-id="3989" fill="#363636"></path>
            <path
                d="M512 64C264.6 64 64 264.6 64 512s200.6 448 448 448 448-200.6 448-448S759.4 64 512 64z m0 820c-205.4 0-372-166.6-372-372s166.6-372 372-372 372 166.6 372 372-166.6 372-372 372z"
                p-id="3990" fill="#363636"></path>
        </svg>
    </a><hr style="border-top: 1px solid #EEEEEE;">
<div id="comment"></div>
<script>
    const getStoredTheme = () => localStorage.getItem("theme") === "dark" ? "dark" : "light";

    const setGiscusTheme = () => {
        const sendMessage = (message) => {
            const iframe = document.querySelector('iframe.giscus-frame');
            if (iframe) {
                iframe.contentWindow.postMessage({giscus: message}, 'https://giscus.app');
            }
        }
        sendMessage({setConfig: {theme: getStoredTheme()}})
    }

    document.addEventListener("DOMContentLoaded", () => {
        const giscusAttributes = {
            "src": "https://giscus.app/client.js",
            "data-repo": "UnionPan\/UnionPan.github.io",
            "data-repo-id": "R_kgDOIZ-Mxw",
            "data-category": "General",
            "data-category-id": "DIC_kwDOIZ-Mx84CuT-c",
            "data-mapping": "pathname",
            "data-reactions-enabled": "1",
            "data-emit-metadata": "0",
            "data-input-position": "bottom",
            "data-theme": getStoredTheme(),
            "data-lang": "en",
            "data-loading": "lazy",
            "crossorigin": "anonymous",
        };

        
        const giscusScript = document.createElement("script");
        Object.entries(giscusAttributes).forEach(
            ([key, value]) => giscusScript.setAttribute(key, value));
        document.getElementById("comment").appendChild(giscusScript);

        
        const themeToggle = document.querySelector(".theme-toggle");
        if (themeToggle) {
            themeToggle.addEventListener("click", setGiscusTheme);
        }
    });

</script>


<div class="pp-container">
        <section class="pre-and-post">
            <div class="has-text-left">
                
            </div>
            <div class="has-text-right">
                
                <p>Next post</p>
                <a href="https://unionpan.github.io/post/2022/nesterov/">Nesterov</a>
                
            </div>
        </section>
    </div>

</div>

                <footer class="footer">
</footer>
            </main>
        </div>

        <script>
            
            function setTheme(theme) {
                let body = document.body;
                let themeIcon = document.querySelector(".theme-icon");
                if (theme === "dark") {
                    body.classList.add("dark-mode");
                    themeIcon.src = "https:\/\/unionpan.github.io\/svg/moon.svg";
                    themeIcon.alt = "moon icon";
                } else {
                    body.classList.remove("dark-mode");
                    themeIcon.src = "https:\/\/unionpan.github.io\/svg/sun.svg";
                    themeIcon.alt = "sun icon";
                }
                localStorage.setItem("theme", theme);
            }

            
            document.addEventListener('DOMContentLoaded', function() {
                
                let theme = localStorage.getItem("theme");
                
                
                if (!theme) {
                    theme = 'dark';
                }
                
                setTheme(theme);

                
                document.getElementById("theme-toggle").addEventListener("click", function(e) {
                    e.preventDefault();
                    let currentTheme = localStorage.getItem("theme") || "light";
                    let newTheme = currentTheme === "light" ? "dark" : "light";
                    setTheme(newTheme);
                });
            });
            
            
            document.getElementById('mobile-menu-toggle').addEventListener('click', function() {
                const sidebar = document.getElementById('sidebar');
                const button = this;
                
                sidebar.classList.toggle('open');
                button.classList.toggle('active');
            });

            
            function toggleBlogMenu(event) {
                event.preventDefault();
                event.stopPropagation(); 
                const submenu = document.getElementById('blog-submenu');
                const arrow = document.querySelector('.dropdown-arrow');
                
                if (submenu.classList.contains('open')) {
                    submenu.classList.remove('open');
                    arrow.style.transform = 'rotate(0deg)';
                } else {
                    submenu.classList.add('open');
                    arrow.style.transform = 'rotate(180deg)';
                }
            }
            
            
            document.addEventListener('click', function(event) {
                const navContainer = document.querySelector('.nav-item-container');
                const submenu = document.getElementById('blog-submenu');
                const arrow = document.querySelector('.dropdown-arrow');
                
                if (!navContainer.contains(event.target) && submenu.classList.contains('open')) {
                    submenu.classList.remove('open');
                    arrow.style.transform = 'rotate(0deg)';
                }
            });

            
            document.addEventListener('click', function(event) {
                const sidebar = document.getElementById('sidebar');
                const toggleButton = document.getElementById('mobile-menu-toggle');
                
                if (sidebar.classList.contains('open') && 
                    !sidebar.contains(event.target) && 
                    !toggleButton.contains(event.target)) {
                    sidebar.classList.remove('open');
                    toggleButton.classList.remove('active');
                }
            });

            
            document.querySelectorAll('.sidebar .nav-link, .sidebar .submenu-link').forEach(link => {
                if (!link.classList.contains('blog-toggle')) {
                    link.addEventListener('click', function() {
                        const sidebar = document.getElementById('sidebar');
                        const toggleButton = document.getElementById('mobile-menu-toggle');
                        sidebar.classList.remove('open');
                        toggleButton.classList.remove('active');
                    });
                }
            });

            
            function addWheelScrolling(selector) {
                const carousel = document.querySelector(selector);
                if (!carousel) return;

                
                const isMobile = /Android|webOS|iPhone|iPad|iPod|BlackBerry|IEMobile|Opera Mini/i.test(navigator.userAgent);
                const isMac = /Mac|iPhone|iPod|iPad/i.test(navigator.userAgent);
                
                carousel.addEventListener('wheel', function(e) {
                    
                    if (isMobile) {
                        return;
                    }
                    
                    
                    
                    const isTouchpad = isMac && (Math.abs(e.deltaX) > 0 || Math.abs(e.deltaY) < 50);
                    
                    
                    if (isTouchpad && Math.abs(e.deltaX) > Math.abs(e.deltaY)) {
                        
                        return;
                    }
                    
                    
                    if (!isTouchpad && Math.abs(e.deltaY) > Math.abs(e.deltaX)) {
                        e.preventDefault();
                        carousel.scrollLeft += e.deltaY * 2;
                    }
                });
            }
            
            
            addWheelScrolling('.posts-carousel');
            addWheelScrolling('.photos-carousel');
        </script>
    </body>
</html>

