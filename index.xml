<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>union&#39;s blog</title>
    <link>https://unionpan.github.io/</link>
    <description>Recent content on union&#39;s blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 11 Oct 2023 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://unionpan.github.io/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title> Some Notes on Dueling Bandits</title>
      <link>https://unionpan.github.io/post/dueling_bandits/</link>
      <pubDate>Wed, 11 Oct 2023 00:00:00 +0000</pubDate>
      <guid>https://unionpan.github.io/post/dueling_bandits/</guid>
      <description>The dueling bandit problem natrually fits the description of a variety of recommendation systems that require &amp;lsquo;&amp;rsquo;learning on the fly&amp;rsquo;&amp;rsquo;, yet have no explicit access to a &amp;lsquo;&amp;lsquo;reward&amp;rsquo;&amp;rsquo; model. Instead, the &amp;lsquo;&amp;lsquo;human&amp;rsquo;&amp;rsquo; feedback part takes the form of &amp;lsquo;&amp;lsquo;choices&amp;rsquo;&amp;rsquo;, &amp;lsquo;&amp;lsquo;votes&amp;rsquo;&amp;rsquo;, or some discrete forms. Hence, oftentimes a learning protocol proceeds at time steps \(t = 1, \ldots, T\):&#xA;The algorithm chooses a pair of arms \( a_i, a_j \) from \(K\) available ones; The oracle/human feedback/nature reveals the winner arm \( a_i \), with probability \( P(a_i \succ a_j)\), \( P(a_i \prec a_j) = 1 - P(a_i \succ a_j) \) So the feedback is either \(a_i\) or \( a_j \), the preferences forms a matrix \(P \in \mathbb{R}^{K \times K}\) such that \( P + P^{\top} = I\) which defines the hidden information of the dueling bandit problem.</description>
    </item>
    <item>
      <title>Wardrop Equilibrium</title>
      <link>https://unionpan.github.io/post/wardrop_equilibirum/</link>
      <pubDate>Mon, 30 Jan 2023 00:00:00 +0000</pubDate>
      <guid>https://unionpan.github.io/post/wardrop_equilibirum/</guid>
      <description>I remember having those unpleasant lines in our high school canteen, flooded by the starving students queeing for their lunch, I would always pick a window with fewer people waiting, compromising myself with awful food. Another similar thought that always stricked me was the odds that tourists always pick the same time to travel, there&amp;rsquo;s almost always traffic congestion everywhere during holiday seasons. Yeah, lives have been always so hard.</description>
    </item>
    <item>
      <title>Nesterov</title>
      <link>https://unionpan.github.io/post/nesterov/</link>
      <pubDate>Tue, 15 Nov 2022 00:00:00 +0000</pubDate>
      <guid>https://unionpan.github.io/post/nesterov/</guid>
      <description>We briefly reviewed the lower complexity bound of first-order convex optimization and how Nesterov proceed to match the lower bound using the estimation sequence, the slides are here.&#xA;Consider an unconstrained optimization problem: \[ \min_{x \in \mathbb{R}^n} f(x) . \] Here, \(f \in \mathcal{C}^1\) is a convex, $L$-Lipschitz smooth function. Obviously we can solve this problem by using first-order methods, the question is how fast they are.</description>
    </item>
    <item>
      <title>The Capacity of Deception</title>
      <link>https://unionpan.github.io/post/capacity-deception/</link>
      <pubDate>Thu, 10 Nov 2022 00:00:00 +0000</pubDate>
      <guid>https://unionpan.github.io/post/capacity-deception/</guid>
      <description>Entropy and Optimal Coding Alice wants to communicate with Bob about a sequence of \(n\) independent random outcomes sampled from a known distribution \(Q\). They use binary code agreed in advance to limit the message length. The entropy of \(Q\) is the expected number of bits necessary per random variable using the optimal code as \(n\) goes to \(\infty\). The relative entropy between distributions $P$ and $Q$ can be interpreted as the price in terms of expected message length that Alice and Bob have to pay if they believe the random variables are sampled from $P$ when in fact they are sampled from $Q$.</description>
    </item>
    <item>
      <title>About Me</title>
      <link>https://unionpan.github.io/about/</link>
      <pubDate>Sun, 26 Apr 2020 20:18:54 +0300</pubDate>
      <guid>https://unionpan.github.io/about/</guid>
      <description>I am a PhD candidate at NYU Tandon. My primary interest lies in statistical/reinforcement learning and applied game theory. Recently I&amp;rsquo;ve been working on macroscopic traffic assignment problems and Non-equilibrium learning in games.&#xA;I love basketball and boxing, I play mildly competitive games of basketball and do some sparring occasionally, you can also find me doing other outdoorsy stuff (like slacklining :))&#xA;I&amp;rsquo;ve recently just finished East of Eden, now I&amp;rsquo;m on my way reading Aumulet, Woes of the True Policeman, (still revisiting the key to 2666) and revisiting Gatsby.</description>
    </item>
  </channel>
</rss>
